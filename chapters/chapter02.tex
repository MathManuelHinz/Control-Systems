\chapter{ODE methods for algorithm design}
\beginlecture{07}{06.05.2025}

\section{ODE methods for algorithm design} % TODO: Fix

Four steps:
\begin{itemize}
    \item Formulate the algorithmic goal as the root finding problem 
        \[\bar{f}(\theta^\star)=0\]
    \item if necessary, refine the design of \(\bar{f}\) to ensure that 
          the associated ODE is \highlight{globally asymptotically stable} \marginnote{\(\theta\) for descrete settings, \(\vartheta\) for continuous settings. Both do the same job}
          \[\frac{d}{dt}\vartheta=\bar{f}(\vartheta)\] 
    \item  Is an \highlight{Euler-approximation} appropriate? \marginnote{\(\theta_{n+1}\) is the next iterate, not the next time step!}
          \begin{equation}\label{eq:euler}\theta_{n+1}=\theta_n+\alpha_{n+1}\bar{f}(\theta_n)\end{equation}
    \item Design an algorithm to approximate (\ref{eq:euler}) based on whatever 
          observation is available.

\end{itemize}

\begin{remark}
    The idea is to transfer the global stability from the ODE to the algorithm.
\end{remark}

\dhighlight{Goal:} Construct a vector field \(f\) such that \(\vartheta_t\)
converges to the \highlight{target} \(\theta^\star\in \R^d\), where 
\(\theta^\star\) is an equilibrium \[f(\theta^\star)=0.\]

In ODE theory one uses so called \dhighlight{Picard-Iteration}
\begin{equation}\label{eq:picard-iterations}
    \vartheta_t^{n+1}1=\theta_0+\int_0^t f(\vartheta_\tau^n)d\tau,\ 0\leq t\leq T
\end{equation}
based on 
\begin{equation}\label{eq:fundamental_theorem}
    \vartheta_0+\int_0^t f(\vartheta_\tau)d\tau,\ 0 \leq t\leq T.
\end{equation}
\begin{proposition}\label{prop:25} % TODO: rename
    Suppose that the function \(f\) is globally Lipschitz continuous:
    \[\exists L >0: \forall x,y\in \R^d:\ \Vert f(x)-f(y)\Vert \leq L\Vert x -y \Vert\]    
    Then for each \(\theta_0\) there exists a unique solution to (\ref{eq:fundamental_theorem}).
    in the finite time horizon.
    Moreover, successive approximation is uniformly convergent:
    \[\lim_{n\to\infty}\max_{0\leq t\leq T}\Vert \vartheta_t^n-\vartheta_t=0\]
\end{proposition}

\begin{proposition}[Grönwall-Bellman-inequality]\label{prop:26}
    Let \(\alpha,\beta\) and \(z\) be non-negative functions defined on 
    \([0,T],\ T>0\). Assume that \(\beta,z\) are continuous and that 
    \[z_t\leq \alpha_t+\int_0^t\beta_s z_sds,\ 0\leq t\leq T\]
    Then it holds
    \begin{enumerate}
        \item[(i)] \(z_t\leq \alpha_t+\int_0^t \alpha_s \beta_s \exp\left(\int_s^t B_r dr\right)ds\)
        \item[(ii)] if in addition the function \(\alpha\) is non-decreasing, then \[z_t\leq \alpha_t\exp\left(\int_0^t B_sds\right),\ 0\leq t\leq T\] 
    \end{enumerate} 
\end{proposition}

\begin{proof}
    Both proofs can be found in any textbook on ODEs. 
    The second one is also found in \cite{Meyn_2022}.
\end{proof}

\begin{proposition}\label{prop:27}\marginnote{Not that nice, but at least a bound \dots}
    Consider \(\frac{d}{dt}\vartheta=f(\vartheta),\ \vartheta_0=\theta_0\) with \(f\) globally Lipschitz. Then 
    \begin{enumerate}
        \item[(i)] There is a constant \(B_f\) depending only on \(f\) such that, with \(t\geq 0\)
                   \begin{eqnarray}\Vert \vartheta_t\Vert \leq \left(B_f+\Vert \vartheta_\Vert\right)e^{Lt}-B_f\\
                    \Vert \vartheta_t-\vartheta_0\Vert \vert  B_f+L\Vert \vartheta_0\Vert \vert t e^{lt}
                   \end{eqnarray}
        \item[(ii)] If there is an equilibrium \(\theta^\star\), then for each initial condition:
                   \begin{equation}
                        \Vert \vartheta_t-\theta^\star\Vert \leq \Vert \vartheta_0-\theta^\star\Vert e^{Lt}
                   \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \highlight{(ii):} use \ref{eq:fundamental_theorem} to get 
    \[\vartheta_t-\theta^\star=\vartheta_0-\theta^\star+\int_0^tf(\vartheta_\tau)d\tau\]
    Since \(f(\theta_\star)=0\), we see 
    \begin{align*}
        \Vert f(\vartheta_\tau)\Vert &= \Vert f(\vartheta_\tau)-f(\theta^\star)\Vert\\
        &\leq L\underbrace{\Vert \vartheta_\tau-\theta^\star\Vert}_{\eqqcolon z_\tau}
    \end{align*}
    So 
    \begin{align*}
        z_t&\leq z_0+L \int_0^t z_t d\tau.
    \end{align*}
    Using proposition \ref{prop:26} (ii) with \(\beta_t\equiv L,\alpha_t\equiv z_0\)
    we get \[\Vert \vartheta_t-\theta^\star\Vert\leq \Vert \vartheta_t-\theta_0\Vert\exp(Lt)\]
    
    \highlight{(i):} take any \(\bar{\theta}\in\R^d\) and use the Lipschitz continuity 
    \begin{align*}
        \Vert f(\theta)\Vert &\leq \Vert f(\theta)-f(\bar{f})\Vert +\Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta-\bar{\theta}\Vert + \Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta\Vert + L\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert.
    \end{align*} 
    For any fixed \(\bar{\theta}\), define \(B_f=\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert/L\)
    which gives 
    \begin{align*}
        \Vert f(\theta)\Vert \leq L \left[\Vert \theta\Vert + B_f\right],\ \theta\in\R^d
    \end{align*}
    using (\ref{eq:fundamental_theorem})
    \begin{align*}
        \Vert \vartheta_t\Vert +B_f &\leq \Vert \vartheta_0\Vert + B_f +\underbrace{L}_\beta\int_0^t \left[\underbrace{\Vert\vartheta_\tau+ B_f}_{z_\tau}\right]d\tau\\
        &\leq \left[\Vert \vartheta_0\Vert +B_f\right]\exp(Lt) 
    \end{align*}
    where the last step follows by the same trick as in (ii), i.e. by using Grönwall.
\end{proof}

\section{Euler's method once more}
\marginnote{Explict Euler, implicit Euler is nicer to analyze}
\begin{equation}\label{eq:euler_v2}
    \frac{\hat{\vartheta}_{t_{n+1}}-\hat{\vartheta}_{t_{n}}}{\alpha_{n+1}} = f(\hat{\vartheta}_{t_n}),\ \hat{\vartheta}_0=\vartheta_0=\theta_0
\end{equation}
or \[\hat{\vartheta}_{t_{n+1}}= \hat{\vartheta}_{t_{n}}+\alpha_{n+1} f(\hat{\vartheta}_{t_n})\]
It can be shown for \(f\) globally Lipschitz
\begin{equation}
    \max_{0\leq t\leq T}\Vert \hat{\vartheta}_t-\vartheta_t\Vert \leq \underbrace{K(L,T)}_{\text{exponential in }L,T}\max\{\alpha_k\mid t_k< T\}
\end{equation} 

\section{Optimization}

\dhighlight{Goal:} Find, for some loss function \(\Gamma:\R^d\to \R_+\),
\begin{equation}
    \theta^\star\in\argmin \Gamma(\theta).
\end{equation} 
Use steepest-descent, formulated as ODE 
\begin{equation}\label{eq:gradient_flow}\frac{d}{dt}\vartheta=-\nabla_\theta \Gamma(\theta)\end{equation}
so called \dhighlight{gradient flow}.
\[\nabla \Gamma(\theta0)\perp \{\theta\in\R^d\mid \Gamma(\theta)=\Gamma(\theta_0)\}\eqqcolon S_{\Gamma}(\theta_0)\]
The gradient flow steers into the interior of \(S_{\Gamma}(\theta_0)\).

\begin{definition}\label{def:28}
    \begin{enumerate}
        \item[(i)] A set \(S\subset \R^d\) is \dhighlight{convex} if it contains all line segments with endpoints in \(S\)
        \item[(ii)] A function \(\Gamma:S\to\R\) with \(S\) convex, is called \dhighlight{convex} 
                    if for any \(\theta^0,\theta^1\in S\) and \(\rho\in (0,1)\) 
                    \[\Gamma((1-\rho)\theta^0+\rho\theta^1)\leq (1-\rho)\Gamma(\theta^0)+\rho\Gamma(\theta^1)\]
                    \(\Gamma\) is \dhighlight{strictly convex} if this inequality is strict whenever \(\theta^0\neq\theta^1\)
        \item[(iii)] If \(\Gamma\) is differentiable, then it is called \dhighlight{strongly convex} if for \(\delta_0>0\)
                     \[\langle\nabla \Gamma(\theta)-\nabla\Gamma(\theta^0),\theta-\theta^0\rangle\geq \delta_0 \Vert \theta-\theta_0\Vert^2,\ \forall \theta,\theta^0\in S\]           
    \end{enumerate}
\end{definition}

From numerical optimization we know:
\begin{theorem}\label{thm:29}
    Suppose that \(\Gamma:\R^d\to\R\) is convex. Then for given \(\theta^0\in R^d\)
    \begin{enumerate}
        \item[(i)] if \(\theta^0\) is a local minima, then it is also \dhighlight{a global minimum}
        \item[(ii)] if \(\Gamma\) is differentiable at \(\theta^0\), with \(\nabla\Gamma(\theta)=0\), then \(\theta^0\) is \dhighlight{a global minimum}
        \item[(iii)] if either(i) or (ii) hold, and if \(\Gamma\) is strictly convex, then \(\theta^0\) is the \dhighlight{unique global minimum }   
    \end{enumerate}
\end{theorem}

\begin{proposition}\label{prop:30}
    Suppose that \(\Gamma\) is continuously differentiable, convex and coercive,
    with unique minimizer \(\theta^\star\). Then the gradient flow 
    \[\frac{d}{dt}\vartheta=-\nabla\Gamma(\vartheta)\]
    is globally asymptotically stable, with unique equilibrium
    \(\theta^\star\). 
    
    If \(\Gamma\) is strongly convex, then the rate of convergence is exponential
    \[\Vert \vartheta_t-\theta^\star\Vert\leq e^{-\delta_0 t}\Vert \vartheta_0-\theta^\star\Vert,\]
    where \(\delta_0\) comes from theorem \ref{thm:29}.
\end{proposition}
\markeol{07}
\beginlecture{08}{08.05.2025}
\begin{proof}
    We use as Lyapunov function \(V(\theta)=\frac{1}{2}\Vert \theta-\theta^\star\Vert^2\). From the chain rule
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)=-\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\vartheta_t-\theta^\star\right]
    \end{align*}
    By convexity we get the following bound 
    \[\Gamma(\theta^\star)\geq \Gamma(\vartheta_t)+\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\theta^\star-\vartheta_t\right]\]
    using the support condition this becomes 
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)\leq \Gamma(\theta^\star)-\Gamma(\vartheta_t)\leq 0
    \end{align*}
    since \(\theta^\star\) is the minimum. The strict inequality \((<0)\) holds when \(\vartheta_t\neq \theta^\star\).
    \(V\) fulfills definition \ref{def:1.20} and proposition \ref{prop:1.21} gives 
    global asymptotic stability. \marginnote{Coercive, therefore inf-compact}

    Under strict convexity 
    \begin{align*}
            \frac{d}{dt}V(\vartheta_t)&=-\left[\nabla_\theta \Gamma(\vartheta_t)-\underbrace{\nabla_\theta\Gamma(\theta^\star)}_{=0}\right]^\intercal \left[\vartheta_t-\theta^\star\right]\\
            &\stackrel{\text{strong convexity}}{\leq} -\delta_0 \Vert \vartheta_t-\theta^\star\Vert^2=-2\delta_0V(\vartheta_t)
    \end{align*}
    This implies \(V(\vartheta_t)\leq V(\vartheta_0)\exp(-2\delta_0t) \forall t\) by integrating.
\end{proof}

\begin{theorem}\label{thm:31}
    If the Polyak-Lojasiewicz  
    (PL) inequality \marginnote{Used in stochastic gradient descent}
    \begin{equation}\label{eq:PL}
        \frac{1}{2}\Vert \nabla\Gamma (\theta)\Vert^2\geq \mu |\Gamma(\theta)-\Gamma(\theta^\star)|  
    \end{equation}
    holds then the gradient flow satisfies for each initial \(\vartheta_0\) 
    \[\Gamma(\vartheta_t)-\Gamma^\star\leq e^{-\mu t}(\gamma(\vartheta_0)-\Gamma^\star).\]
    If in addition \(\Gamma\) is coercive, then the solutions are bounded and 
    any limit point \(\theta_\infty\) of \(\{\vartheta_t\}\) is an optimizer 
    \[\Gamma(\theta_\infty)=\Gamma^\star\]
\end{theorem}

\begin{proof}
    We use \(V(\theta)=\frac{1}{2}\vert \Gamma(\theta)-\Gamma^\star\vert\)
    for the Lyapunov function. 
    \begin{align*}
        \implies \frac{d}{dt}V(\vartheta_t)&=\frac{1}{2}\nabla_\theta\Gamma(\vartheta_t)^\intercal \frac{d}{dt}\vartheta_t\\
        &=-\frac{1}{2}\Vert \nabla\Gamma(\vartheta_t)\Vert^2\leq -\mu V(\vartheta_t)
    \end{align*}
    This implies using the same technique as in the previous proof
    \begin{align*}
        \frac{1}{2}\left[\Gamma(\vartheta_t)-\Gamma^\star\right]&=V(\vartheta_t)\leq e^{-\mu t}V(\vartheta_0)\\
        &= e^{-\mu t}\frac{1}{2}\left[\Gamma(\vartheta_0)-\Gamma^\star\right]
    \end{align*}
    If \(\Gamma\) is coercive, then trajectories of \(\vartheta\) evolve
    in the compact set \(S=\{\theta\mid V(\theta)\leq V(\vartheta_0)\}\). If 
    \(theta_\infty\) is a limit point \(\theta_\infty=\lim_{n\to\infty}\vartheta_{t_n}\)
    for \(t_n\to\infty\).
    Using the continuity of the loss function, this implies optimality:
    \[\Gamma(\theta_\infty)=\lim_{n\to\infty}\Gamma(\vartheta_{t_n})=\Gamma^\star\qedhere\] 
\end{proof}
Consider the Euler method for the gradient flow:
\begin{equation}\label{eq:euler-gradient-flow}
    \theta_{k+1}=\theta_k-\alpha \nabla\Gamma(\theta_k)
\end{equation}
\begin{theorem}\label{thm:32}
    Suppose that \(\Gamma\) satisfies
    \begin{enumerate}\marginnote{These are more general version of global Lipschitz and convexity}
        \item[(i)] the \(L-\)smooth inequality (LSI) \[\Gamma(\theta')\leq\Gamma(\theta)+[\theta'-\theta]^\intercal\nabla\Gamma(\theta)+\frac{1}{2}L\Vert \theta'-\theta\Vert^2\]
        \item[(ii)] the PL inequality \ref{eq:PL}  
    \end{enumerate}
    Then it holds for \(\alpha\leq \frac{1}{2}\)
    \[\Gamma(\theta_{k})-\Gamma^\star\leq (1-\alpha\mu)^k[\Gamma(\theta_0)-\Gamma^\star].\]
\end{theorem}

\begin{proof}
    \begin{align*}
        \Gamma(\theta_{k+1}-\Gamma(\theta_k))& \stackrel{\text{LSI}}{\leq} [\theta_{k+1}-\theta_k]^\intercal\nabla\Gamma(\theta_k)+\frac{1}{2}L\Vert\theta_{k+1}-\theta_k \Vert^2\\
        &\stackrel{\ref{eq:euler-gradient-flow}}{=}-\alpha\Vert \nabla\Gamma(\theta_k)\Vert^2+\frac{1}{2}L\alpha^2\Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &=(-\alpha+\frac{1}{2}L\alpha^2)\Vert \nabla\Gamma(\theta_k)\Vert^2
    \end{align*}
    If \(\alpha\leq \frac{1}{L}\) then \((-\alpha+\frac{1}{2}L\alpha^2)\leq \frac{1}{2}\alpha\)
    
    \begin{align*}
        &\leq -\frac{1}{2}\alpha \Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &\stackrel{\text{LSI}}{\leq}-\alpha\mu\vert \Gamma(\theta_k)-\Gamma^\star\vert     
    \end{align*}
    and therefore 
    \[\Gamma(\theta_{k+1})-\Gamma^\star\leq (1-\alpha\mu)(\Gamma(\theta_k)-\Gamma^\star)\]
    after iterating \(k-1\) times  we obtain the result.
\end{proof}

\begin{lemma}\label{lem:33}
    Suppose that \(\nabla\Gamma\) is globally Lipschitz 
    \[\Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert\leq L \Vert \theta'-\theta\Vert,\ \forall \theta,\theta'\in S\]
    Then \begin{enumerate}
        \item[(i)]  \(\vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert\leq L \Vert \theta'-\theta\Vert^2\)
        \item[(ii)] if \(S\) is convex, then \(\Gamma\) is \(L\)-smooth 
    \end{enumerate}
\end{lemma}

\begin{proof}
    \highlight{(i)}
    \begin{align*}
        \vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert & \leq \Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert \Vert \theta'-\theta\Vert\\
        &\leq L \Vert \theta'-\theta\Vert
    \end{align*}
    \marginnote{\(\theta^t\) in \(S\), since \(S\) is convex}
    \highlight{(ii)} for \(\theta',\theta\in S\) denote \(S\ni\theta^t\coloneqq\theta+t(\theta'-\theta)\)
    and \(\xi^t=\Gamma(\theta^t)\).
    \begin{align*}
        \frac{d}{dt}\xi^t&=\langle\nabla\Gamma(\theta^t),\theta'-\theta \rangle\\
        \frac{d}{dt}xi^t-\frac{d}{dt}xi^0-&=\langle\nabla\Gamma(\theta^t)-\nabla\Gamma(\theta^0),\theta'-\theta \rangle\\
        &\stackrel{(i)}{\leq} tL \Vert \theta'-\theta\Vert^2
    \end{align*} 
    Now integrate 
    \begin{align*}
        \Gamma(\theta')=\xi^1&=\xi^0+\int_0^1\frac{d}{dt}\xi^t dt \\
        &\leq \xi^0+\frac{d}{dt}\xi^0+\frac{1}{2}L\Vert \theta'-\theta\Vert^2 \\
        &=\Gamma(\theta)+\langle \nabla\Gamma(\theta),\theta'-\theta\rangle+\frac{1}{2}L\Vert \theta'-\theta\Vert\qedhere
    \end{align*}
\end{proof}

\begin{remark}
    Strong convexity:\marginnote{This gives a bound on the loss function from both sides \dots} 
    \begin{align*}
        \langle\nabla\Gamma(\theta')-\Gamma(\theta),\theta'-\theta\rangle\geq \delta_0\Vert \theta'-\theta\Vert^2
    \end{align*}
    With \(D_\Gamma(y\mid x)=\Gamma(y)-\Gamma(x)+\langle \nabla\Gamma(x),y-x\rangle\)
    is the \dhighlight{Bregman divergence}.
    \[\frac{\mu}{2}\Vert \theta'-\theta\Vert^2\leq D_\Gamma(\theta'\mid\theta)\leq \frac{L}{2}\Vert \theta'-\theta\Vert^2\]
\end{remark}

\section{Qausi stochastic approximation}%TODO: FIX

Assume there are observations \(\Phi_n\subset \Omega\), which 
we might consider as realizations of a random variable \(\Phi\).
We have \[f:\R^d\times \Omega\to\R^d\]
\[\bar{f}(\theta)\coloneqq \bE(\underbrace{f(\theta,\Phi)}_{\text{what we observe}}), \theta\in\R^d\]
As before we look for \(\bar{f}(\theta^\star)=0\)
\[\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\]
A \dhighlight{key assumption} is that what happens when following the state 
dynamics in any step depends only on the current state.\marginnote{I.e. we have the Markov property}

\[\Phi_n=[\cos(\omega n),\sin(\omega n)],\omega>0\]
\textit{Markov chain on the unit circle}. We will talk about the probing signal \(\xi\)
and consider 
\marginnote{the book uses \(\Theta\) instead of \(\hat{\theta}\)}
\begin{equation}\label{eq:qsa-ode}
    \frac{d}{dt}\hat{\theta}_t=a_t  f(\hat{\theta}_t,\xi_t)
\end{equation}
a \dhighlight{quasistochastic approximation(QSA)}-ODE, \(a_t\) is the \dhighlight{step size}. 

For deterministic probing signals, we mainly consider two examples 
\marginnote{Mixture of sin functions}\[\xi_t=\sum_{i=1}^K \overbrace{V^i}^{\in \R^m}sin(2\pi[\Phi_i+\omega_i t])\]
\dhighlight{Mixture of periodic functions}, fixed \(K\), \dhighlight{phase \(\{\Phi_i\}\)}, \dhighlight{frequencies} \(\{\omega_i\}\).

\[\xi_t=\sum_{i=1}^K V^i[\Phi_i+\omega_it]_{\text{modulo 1}}\]

\begin{figure}[H]\label{fig:2.01}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 2.01}
\end{figure}

These signals have well defined steady-state means and covariance matrices.

Special case: \(\xi_t(i)=\sqrt{2}\sin(\omega_it),\ 1\leq i\leq m,\omega_i\neq \omega_j\forall i\neq j\). Then the \dhighlight{steady-state mean}
\begin{align*}
    \lim_{T\to\infty}\int_{0}^{T}\xi_t dt =0 
\end{align*}
and \dhighlight{covariance}
\begin{align*}
    \lim_{T\to\infty}\int_0^T\xi_i\xi_i^\intercal dt=\Id
\end{align*}
\markeol{08}
\beginlecture{09}{13.05.2025}
We now use a slightly different notation \(\hat{\theta}\) becomes \(\tilde{\theta}\).
\begin{equation}\label{eq:QSA-ODE}
    \frac{d}{dt}\tilde{\theta}_t=a_t f(\tilde{\theta},\xi_t)
\end{equation}
\(a_t\) non-negative.

Now consider integrating \(y:[0,1]\to\R\). Basic Monte-Carlo 
\begin{equation}\label{eq:monte-carlo-integration}
    \theta_n=\frac{1}{n}\sum_{i=0}^{n-1} y(\underbrace{\Phi(k)}_{\sim\text{Unif}([0,1])})
\end{equation} 

A \dhighlight{QSA} approach is to use the saw tooth function 
\[\xi_t=t(\text{modulo 1}).\]
Obtain estimate by 
\begin{equation}\label{eq:estimate_qsa}\tilde{\theta}=\frac{1}{t}\int_0^ty(\xi_t)dr\end{equation}
with a reasonable discretization afterwards.

To use (QSA-ODE (\ref{eq:QSA-ODE})) \(f(\theta,\xi)=y(\xi)-\theta\) with mean vector field 
\begin{align*}
    \bar{f}(\theta)&=\lim_{T\to\infty}\int_0^T f(\theta,\xi_t)dt\\
        &=\int_0^1 y(\xi_t)dt-\theta
\end{align*}
which gives \(\theta^\star=\int_0^1y(\xi_t)dt\) as the unique root of \(\bar{f}\). 
The QSA-ODE \ref{eq:QSA-ODE}
is \[\frac{d}{dt}\tilde{\theta}_t=a_t[y(\xi_t)-\tilde{\theta}_t]\]
(\ref{eq:estimate_qsa}) can be transformed into 
\begin{equation}
    \frac{d}{dt}\tilde{\theta}_t=\left[-\frac{1}{t^2}\int_0^t y(\xi_r)dr+\frac{1}{t}y(\xi_t)\right]=\underbrace{\frac{1}{t}}_{\eqqcolon a_t}\left[y(\xi_t)-\theta_t\right]
\end{equation}

\begin{example}
    \(y(\theta)=e^{4}(\sin(100\theta))\), mean \(\theta^\star\approx-0.5\approx -0.48\).
    Choose \(a_t=\frac{g}{1+t}\)
\end{example}

\section{Approximate Policy Improvement}

nonlinear state model in continuous time:
\begin{align}
    \frac{d}{dt}x_t=f(x_t,u_t),& t\geq 0\\
    J^\star(x)=\min_{\underbar{u}}\int_0^\infty c(x_t,u_t)dt & x=x_0
\end{align}

Given feedback law \(u_t=\phi(x_t)\), we have 
\begin{equation}
    J^\phi(x)=\int_0^\infty c(x_t,\phi(x_t))dt,\ x=x_0
\end{equation}
%TODO: y might be g here?
\begin{proposition}\label{prop:34}
    If \(J\) is finite, then for each initial condition \(x_0\) and each \(t\)
    \[\frac{d}{dt}J(x_t)=-c(x_t)\]
    If \(J\) is continuously differentiable, then the Lyapunov bound \(\frac{d}{dt} V(x_t)\)
    from definition \ref{def:1.20}  follows with equality
    \[\nabla J(x)f(x)=-c(x)\]
\end{proposition}

\begin{proof}
    For any \(T>0\), \(J(x_0)=\int_0^Tc(x_r)dr+J(x_T)\). For \(t\geq 0,\delta>0\) given,
    use \(T=t+\delta\) and \(T=t\) and subtract:
    \begin{align*}
        0=J(x_0)-J(x_0)&=\int_t^{t+\delta}c(x_r)dr+(J(x_{t+\delta})-J(x_t))\\
        &=\underbrace{\frac{1}{\delta}\int_t^{t+\delta}c(x_r)dr}_{\stackrel{\delta\to 0}{\to}c(x_t)}+\underbrace{\frac{1}{\delta}(J(x_{t+\delta})-J(x_t))}_{\stackrel{\delta\to 0}{\to}\frac{d}{dt}J(x_t)}\\
        &\implies \frac{d}{dt}J(x_t)=-c(x_t)
    \end{align*}
    Using the chain rule yields the second equation.
\end{proof}

For \(J^\phi\) we have 
\begin{align*}
    0=c(x,\phi(x))+\nabla J^\phi(x)\cdot f(x,\phi(x))
\end{align*}

\dhighlight{Policy Improvement in  continuous time:}
\[\phi^+(x)\in \argmin_{u}\{\underbrace{c(x,u)+\nabla J(x)\cdot f(x,u)}_{\text{need to approximate by }Q^\phi(x,u)}\}\]
Now aim for updating of \(Q\)-function. Add to the above \(J^\phi\) on both sides 
\begin{align*}
    J^\phi(x)&=J^\phi(x)+c(x,\phi(x))+\nabla J^\phi(x)\cdot f(x,\phi(x))        
\end{align*}
We solved for the optimal \(Q\)-function by using a fixed point equation,
with \(\underbar{Q}^\phi(x)=Q^\phi(x,\phi(x))\) we write \marginnote{\(\underbar{Q}\) for the fixed, but optimal choice of \(u\)}
\[Q^\phi(x,u)=\underbar{Q}^\phi(x)+c(x,u)+\nabla \underbar{Q}^\phi(x)f(x,u).\]
Consider \(\{Q^\theta\mid \theta\in \R^d\}\) family of approximations. 
Bellman errors (Temporal differences expressions?) gives 
\begin{equation}\label{eq:bellman_error_policy_improvement_continuous}
    B^\theta(x_t,u_t)=-Q^\theta(x_t,u_t)+\underbar{Q}^\theta(x)+c(x_t,u_t)+\underbrace{\nabla\underbar{Q}^\theta(x) f(x_t,u_t)}_{=\frac{d}{dt}Q^\theta(x_t)}
\end{equation}

Everything on the RHS is can be observed for any state-action pair without knowledge of \(f\).
Now, find \(\theta^\star\) that minimizes
\[\Vert B^\theta\Vert^2=\lim_{T\to\infty}\frac{1}{T}\int_0^T[B^\theta(x_t,u_t)]^2dt\]
Choose feedback law with exploration \(u_t=\tilde{\phi}(x_t,\xi_t)\). Assuming
bounded state trajectories, such that (\ref{eq:bellman_error_policy_improvement_continuous}) exists,
define \(\Gamma(\theta)=\frac{1}{2}\Vert B^\theta\Vert^2\), we get 
\begin{align*}
    0&\stackrel{!}{=}\nabla\Gamma(\theta)=\lim_{t\to\infty}\int_0^T\left[B^\theta(x_t,u_t)\right]\nabla_\theta B^\theta(x_t,u_t)dt
\end{align*}
Gradient flow 
\begin{align*}
    \frac{d}{dt}\vartheta_t=-\nabla_\theta\Gamma(\vartheta_t)
\end{align*}
QSA counterpart is (\ref{eq:bellman_error_policy_improvement_continuous})
with probing signal 
\[\frac{d}{dt}\tilde{\theta}_t=-a_tB^{\tilde{\theta}_t}(x_t,u_t)\kappa_t^{\tilde{\theta}_t}\]
with 
\begin{align*}
    \kappa_t^{\tilde{\theta}_t}&=\nabla_\theta B^theta(x_t,u_t)\\
    &=-\nabla_\theta Q^\theta(x_t,u_t)+\{\nabla_\theta Q^\theta(x_t,\phi(x_t))+ \frac{d}{dt}\nabla_\theta Q^\theta(x_t,\phi(x_t))\}
\end{align*}
assuming we can exchange differentiation w.r.t time and w.r.t \(\theta\).

(QSA-ODE)
\begin{align*}
    \frac{d}{dt}\tilde{\theta}_t=a_tf(\tilde{\theta}_t,\xi_t)
\end{align*}
aim to relate this to 
\[\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t).\]
\begin{lemma}\label{lem:35}
    Define the change of variables 
    \[\tau=s_t\coloneqq\int_0^t a_rdr,\ t\geq t_0.\]
    Let \(\{\vartheta_\tau\mid\tau\geq \tau_0\}\) the solution to 
    the ODE above initialized to \(\tau_0=s_{t_0}\) with \(\vartheta_{\tau_0}=\tilde{\theta}_{t_0}\).
    The solution to 
    \[\frac{d}{dt}\bar{\theta}_t=a_t\bar{f}(\bar{\theta}_t),\ t\geq t_0,\ \bar{\theta}_{t_0}=\tilde{\theta}_{t_0}\]
    is given by \(\bar{\theta}_t=\vartheta_\tau\).
\end{lemma}
\begin{proof}
    Change of variables and observing that \[d\tau=a_tdt.\]
\end{proof}
\markeol{09}
\beginlecture{10}{15.05.2025}
Recall \(\bar{f}(\theta)\coloneqq\lim_{T\to\infty}\int_0^T f(\theta,\xi_t)dt\) for all \(\theta\in\R^d\).
Remember the temporal transformation 
\[\tau=s_t=\int_0^t a_rdr\]
and lemma \ref{lem:35}. Define 
\(\hat{\theta}_\tau=\tilde{\theta}(s^{-1}(\tau))=\tilde{\theta}_t\mid_{t=s^{-1}(\tau)}\). By 
the chain rule and observing that \(d\tau=a_tdt\) yields 
\[\frac{d}{d\tau}\hat{\theta}_\tau=\frac{d}{d\tau}\tilde{\theta}(s^{-1}(\tau))=f(\tilde{\theta}(s^{-1}(\tau)),\xi(s^{-1}(\tau))).\]
\(\thetahat,\thetatilde\) differ only by a time scaling, so convergence of the one yields convergence of the other.
\begin{lemma}\label{lem:36}
    Consider the original ODE 
    \begin{equation}\label{eq:gradient_flow_ode}
        \frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)  
    \end{equation}
    and assume \(f\) is locally Lipschitz with constant \(L_f\).
    \marginnote{Version of proposition \ref{prop:27}}Then there exists a constant \(B_f\)
    depending only on \(f\), such that 
    \[\Vert\thetahat_t-\thetahat_0\Vert\leq \left(B_f+L_f|\Vert \thetahat_0\Vert\right)te^{L_ft},\ t\geq 0\] 
\end{lemma}

\begin{proof}
    Proof of proposition \ref{prop:27} in adapted notation.
\end{proof}

Now, denote by \(\vartheta_w^\tau,w\geq \tau\) the unique solution 
to (\ref{eq:gradient_flow_ode}):
\begin{equation*}
    \frac{\partial }{\partial w}\vartheta_w^\tau=\bar f(\vartheta_w^\tau),\ w\geq \tau,\ \vartheta_\tau^\tau=\thetahat_\tau
\end{equation*}

with that we get \marginnote{quasistochastic vs continuous}
\begin{enumerate}
    \item \(\vartheta_{\tau+v}^\tau=\thetahat_\tau+\int_0^{\tau+v}\bar{f}(\vartheta_w^\tau)dw,\ \tau,v\geq 0\)
    \item \(\thetahat_{\tau+v}=\thetahat_\tau+\int_\tau^{\tau+v} f(\thetahat_w,\xi(s^{-1}(w)))dw,\ \tau,v\geq 0\)
\end{enumerate}

The following assumptions will be used in the following:
\begin{enumerate}
    \item[\highlight{QSA1}] The process \(a\) is non-negative, monotonically decreasing and \(\lim_{t\to\infty} a_t=0,\int_0^\infty a_rdr=\infty\)\marginnote{it does not go to zero to fast} 
    \item[\highlight{QSA2}] The functions \(\hat{f},f\) are Lipschitz continuous with constant \(L_f\):\begin{align*}
        \Vert \bar{f}(\theta')-\bar{f}(\theta)\Vert &\leq \Vert L_f \Vert \theta'-\theta\Vert\\
        \Vert f(\theta',z)-f(\theta,z)\Vert &\leq \Vert L_f \Vert \theta'-\theta\Vert
    \end{align*} for all \(\theta,\theta'\in R^d,z\in\Omega\) and there exists a Lipschitz continuous functions \(b_0:R^d\to\R_+\), such that for all \(\theta\in\R^d\)\marginnote{Is my probing covering everything: ergocity, ergodic bound} 
\[\left\Vert \int_{t_0}^{t_1}f(\theta,\xi_t)-\bar{f}(\theta)dt\right\Vert\leq b_0(\theta),\ 0\leq t_1\leq t_1\]
    \item[\highlight{QSA3}] The ODE \(\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\) 
    has a globally asymptotically stable equilibrium \(\theta^\star\) 
\end{enumerate}
Consider first, arbitrary \(\theta\)
\begin{lemma}\label{lem:37}
    \marginnote{There is a connect to the law of large numbers ...}
    Assume (QSA1), (QSA2) hold for any fixed \(T>0\) and \(\theta\in\R^d\).
    \begin{align*}
        \left\Vert\int_{\tau}^{\tau+T}\left[f(\theta,\xi(s^{-1}(w)))-\bar{f}(\theta)\right]dw \right\Vert\leq b_0(\theta)\epsilon_\tau^f,
    \end{align*}   
    where \(\epsilon_\tau^f=3a_t\mid_{t=s^{-1}(\tau)}\) and \(b_0\) comes from (QSA2).
\end{lemma}
\begin{proof}
    Set \(\tilde{f}_w(\theta)=f(\theta,\xi_w)-\bar{f}(\theta)\) for each \(w,\theta\). Write 
    \marginnote{large \(\epsilon_t\) in the book? Prob. \(\mathcal{E}\)}
    \begin{align*}
        E_t=\int_0^t\tilde{f}_w(\theta)dw.
    \end{align*}
    By assumptions \(\Vert E_t\Vert\leq b_0(\theta),\ t\geq 0\).
    \begin{align*}
        \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt&\stackrel{\text{IbP}}{=}a_tE_t\mid_{t_0}^{t_1}-\int_{t_0}^{t_1}\vert a_t'\vert E_tdt \\
        \left\Vert \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt\right\Vert &\leq a_{t_0}\Vert E_{t_0}\Vert+ a_{t_1}\Vert E_{t_0}\Vert+\int_{t_0}^{t_1} \vert a_t'\vert E_tdt\\
        &\stackrel{a \text{ decreasing}}{\leq} 2 a_{t_0}b_0(\theta)-b_0(\theta)\int_{t_0}^{t_1} a_t' dt\\
        &\leq 3 a_{t_0} b_0(\theta) 
    \end{align*}
    Set \(t_0=s^{-1}(\tau),t_1=s^{-1}(\tau+T), t=s^{-1}(w)\),
    giving \(dw=a_tdt\)
    \begin{align*}
        \left\Vert \int_{\tau}^{\tau+T} [f(\theta,\xi(s^{-1}(w)))-\bar{f}(\theta)]dw \right\Vert&=
        \left\Vert \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt\right\Vert\\
        &\leq 3a_{t_0}b_0(\theta)=\epsilon_\tau^f b_0(\theta)
    \end{align*}
\end{proof}

\begin{proposition}\label{prop:38}
    Assuming that \(\thetahat\) is bounded. Then for any \(T>0\)
    \[\lim_{\tau\to\infty}\sup_{v\in[0,T]}\left\Vert \overbrace{\int_\tau^{\tau+v}\left[f(\hat{\theta}_w, \xi(s^{-1}(w)))-\bar{f}(\hat{\theta}_w)\right]dw}^{E_{\tau+v}^{\tau}}\right\Vert=0\]
    and 
    \[\lim_{\tau\to\infty}\sup_{v\in[0,T]}\left\Vert \hat{\theta}_{\tau+v}-\vartheta_{\tau+v}^\tau \right\Vert=0\]
\end{proposition}
\begin{proof}
    We use piecewise constant approximation, as in Riemannian integration, and set for \(\delta>0,\ \tau_k=\tau+k\delta,\ k\geq 0\) 
    \begin{align*}
        E_{\tau+v}^\tau=\sum_{k=0}^{n_v-1}\int_{\tau_k}^{\tau_{k+1}} \left[f(\thetahat_{\tau_k},\xi(s^{-1}(w)))-\bar{f}(\thetahat_{\tau_k})\right]dw +\epsilon_{v}^\tau,
    \end{align*}
    which holds due to (QSA1), Lipschitz condition, \(n_v=\lfloor \frac{v}{\delta}\rfloor\).
    and \[\Vert \epsilon_v^\tau\leq b_L v\delta\]
    for some finite constant \(b_L\). Assuming \(\thetahat\) is bounded, this bound is uniform in \(\tau\).
    For fixed \(\thetahat_{t_k}\) we can use lemma \ref{lem:37}, so 
    \begin{align*}
        \Vert E_{\tau+v}^\tau\Vert &\leq \sum_{k=0}^{n_v-1}\epsilon_{\tau_k}^fb_0(\thetahat_{t_k})+b_L v\delta\\
        &\leq \epsilon_\tau^f\sum_{k=0}^{n_v-1} b_0(\thetahat_{\tau_k})+b_L v\delta
    \end{align*}
    Let \(b<\infty\) denote a constant such that \(b_0(\thetahat_{\tau_k})\leq b\ \forall \tau\), which we can do since \(\thetahat\) is bounded, \(b_0\) Lipschitz.
    \begin{align*}
        \left\Vert E_{\tau+v}^\tau\right\Vert\leq b \frac{v}{\delta}\underbrace{\epsilon_\tau^f}_{\stackrel{\tau\to\infty}{\to }0\text{ by QSA1}}+b_Lv\delta
    \end{align*}
    For any \(T>0\)
    \begin{align*}
        \lim_{\tau\to\infty}\sup_{v\in [0,T]}\Vert E_{\tau+v}^\tau\Vert \leq 0+ b_L T\delta
    \end{align*}
    Since \(\delta>0\) was arbitrary, we have the first statement.


    For the second limit: \(E_r^\tau=\vartheta_r^\tau-\thetahat_r\). The pair of identities after lemma \ref{lem:36} give 
    using Lipschitz condition from (QSA2) we get
    \begin{align*}
        E_{\tau+v}^\tau&=0+\int_{\tau}^{\tau+v}\bar{f}(\thetahat_w)-f(\thetahat_w,\xi(s^{-1}(w)))dw + \int_\tau^{\tau+v}\underbrace{\left[\bar{f}(\vartheta_v^\tau)-\bar{f}(\thetahat_w)\right]}_{\Vert \dots \Vert \leq L_f \Vert E_w^\tau\Vert}dw\\
        \Vert E_{\tau+v}^\tau&\leq \delta^\tau+L_f \int_{\tau}^{\tau+v}\Vert E_w^\tau\Vert dw,
    \end{align*}
    where \[\delta^\tau\coloneqq \sup_{\tau'\geq \tau}\max_{0\leq v\leq T}\left\Vert \int_{\tau'}^{\tau'+v}\left[\bar{f}(\thetahat_w)-f(\thetahat_w,\xi(s^{-1}(w)))\right]dw\right\Vert\]    
    Grönwalls lemma gives 
    \[\Vert E_{\tau+v}^\tau\leq e^{Lf}\delta^\tau\forall \tau, \ 0\leq v\leq 1\]
    \(\delta^\tau\to 0\) for \(\tau\to\infty\) due to the first statement.
\end{proof}

\markeol{10}