\chapter{ODE methods for algorithm design}
\beginlecture{07}{06.05.2025}

\section{ODE methods for algorithm design} % TODO: Fix

Four steps:
\begin{itemize}
    \item Formulate the algorithmic goal as the root finding problem 
        \[\bar{f}(\theta^\star)=0\]
    \item if necessary, refine the design of \(\bar{f}\) to ensure that 
          the associated ODE is \highlight{globally asymptotically stable} \marginnote{\(\theta\) for descrete settings, \(\vartheta\) for continuous settings. Both do the same job}
          \[\frac{d}{dt}\vartheta=\bar{f}(\vartheta)\] 
    \item  Is an \highlight{Euler-approximation} appropriate? \marginnote{\(\theta_{n+1}\) is the next iterate, not the next time step!}
          \begin{equation}\label{eq:euler}\theta_{n+1}=\theta_n+\alpha_{n+1}\bar{f}(\theta_n)\end{equation}
    \item Design an algorithm to approximate (\ref{eq:euler}) based on whatever 
          observation is available.

\end{itemize}

\begin{remark}
    The idea is to transfer the global stability from the ODE to the algorithm.
\end{remark}

\dhighlight{Goal:} Construct a vector field \(f\) such that \(\vartheta_t\)
converges to the \highlight{target} \(\theta^\star\in \R^d\), where 
\(\theta^\star\) is an equilibrium \[f(\theta^\star)=0.\]

In ODE theory one uses so called \dhighlight{Picard-Iteration}
\begin{equation}\label{eq:picard-iterations}
    \vartheta_t^{n+1}1=\theta_0+\int_0^t f(\vartheta_\tau^n)d\tau,\ 0\leq t\leq T
\end{equation}
based on 
\begin{equation}\label{eq:fundamental_theorem}
    \vartheta_0+\int_0^t f(\vartheta_\tau)d\tau,\ 0 \leq t\leq T.
\end{equation}
\begin{proposition}\label{prop:25} % TODO: rename
    Suppose that the function \(f\) is globally Lipschitz continuous:
    \[\exists L >0: \forall x,y\in \R^d:\ \Vert f(x)-f(y)\Vert \leq L\Vert x -y \Vert\]    
    Then for each \(\theta_0\) there exists a unique solution to (\ref{eq:fundamental_theorem}).
    in the finite time horizon.
    Moreover, successive approximation is uniformly convergent:
    \[\lim_{n\to\infty}\max_{0\leq t\leq T}\Vert \vartheta_t^n-\vartheta_t=0\]
\end{proposition}

\begin{proposition}[Gruönwall-Bellman-inequality]\label{prop:26}
    Let \(\alpha,\beta\) and \(z\) be non-negative functions defined on 
    \([0,T],\ T>0\). Assume that \(\beta,z\) are continuous and that 
    \[z_t\leq \alpha_t+\int_0^t\beta_s z_sds,\ 0\leq t\leq T\]
    Then it holds
    \begin{enumerate}
        \item[(i)] \(z_t\leq \alpha_t+\int_0^t \alpha_s \beta_s \exp\left(\int_s^t B_r dr\right)ds\)
        \item[(ii)] if in addition the function \(\alpha\) is non-decreasing, then \[z_t\leq \alpha_t\exp\left(\int_0^t B_sds\right),\ 0\leq t\leq T\] 
    \end{enumerate} 
\end{proposition}

\begin{proof}
    Both proofs can be found in any textbook on ODEs. 
    The second one is also found in \cite{Meyn_2022}.
\end{proof}

\begin{proposition}\label{prop:27}\marginnote{Not that nice, but at least a bound \dots}
    Consider \(\frac{d}{dt}\vartheta=f(\vartheta),\ \vartheta_0=\theta_0\) with \(f\) globally Lipschitz. Then 
    \begin{enumerate}
        \item[(i)] There is a constant \(B_f\) depending only on \(f\) such that, with \(t\geq 0\)
                   \begin{eqnarray}\Vert \vartheta_t\Vert \leq \left(B_f+\Vert \vartheta_\Vert\right)e^{Lt}-B_f\\
                    \Vert \vartheta_t-\vartheta_0\Vert \vert  B_f+L\Vert \vartheta_0\Vert \vert t e^{lt}
                   \end{eqnarray}
        \item[(ii)] If there is an equilibrium \(\theta^\star\), then for each initial condition:
                   \begin{equation}
                        \Vert \vartheta_t-\theta^\star\Vert \leq \Vert \vartheta_0-\theta^\star\Vert e^{Lt}
                   \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \highlight{(ii):} use \ref{eq:fundamental_theorem} to get 
    \[\vartheta_t-\theta^\star=\vartheta_0-\theta^\star+\int_0^tf(\vartheta_\tau)d\tau\]
    Since \(f(\theta_\star)=0\), we see 
    \begin{align*}
        \Vert f(\vartheta_\tau)\Vert &= \Vert f(\vartheta_\tau)-f(\theta^\star)\Vert\\
        &\leq L\underbrace{\Vert \vartheta_\tau-\theta^\star\Vert}_{\eqqcolon z_\tau}
    \end{align*}
    So 
    \begin{align*}
        z_t&\leq z_0+L \int_0^t z_t d\tau.
    \end{align*}
    Using proposition \ref{prop:26} (ii) with \(\beta_t\equiv L,\alpha_t\equiv z_0\)
    we get \[\Vert \vartheta_t-\theta^\star\Vert\leq \Vert \vartheta_t-\theta_0\Vert\exp(Lt)\]
    
    \highlight{(i):} take any \(\bar{\theta}\in\R^d\) and use the Lipschitz continuity 
    \begin{align*}
        \Vert f(\theta)\Vert &\leq \Vert f(\theta)-f(\bar{f})\Vert +\Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta-\bar{\theta}\Vert + \Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta\Vert + L\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert.
    \end{align*} 
    For any fixed \(\bar{\theta}\), define \(B_f=\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert/L\)
    which gives 
    \begin{align*}
        \Vert f(\theta)\Vert \leq L \left[\Vert \theta\Vert + B_f\right],\ \theta\in\R^d
    \end{align*}
    using (\ref{eq:fundamental_theorem})
    \begin{align*}
        \Vert \vartheta_t\Vert +B_f &\leq \Vert \vartheta_0\Vert + B_f +\underbrace{L}_\beta\int_0^t \left[\underbrace{\Vert\vartheta_\tau+ B_f}_{z_\tau}\right]d\tau\\
        &\leq \left[\Vert \vartheta_0\Vert +B_f\right]\exp(Lt) 
    \end{align*}
    where the last step follows by the same trick as in (ii), i.e. by using Gruönwall.
\end{proof}

\subsection{Euler's method once more}
\marginnote{Explict Euler, implicit Euler is nicer to analyze}
\begin{equation}\label{eq:euler_v2}
    \frac{\hat{\vartheta}_{t_{n+1}}-\hat{\vartheta}_{t_{n}}}{\alpha_{n+1}} = f(\hat{\vartheta}_{t_n}),\ \hat{\vartheta}_0=\vartheta_0=\theta_0
\end{equation}
or \[\hat{\vartheta}_{t_{n+1}}= \hat{\vartheta}_{t_{n}}+\alpha_{n+1} f(\hat{\vartheta}_{t_n})\]
It can be shown for \(f\) globally Lipschitz
\begin{equation}
    \max_{0\leq t\leq T}\Vert \hat{\vartheta}_t-\vartheta_t\Vert \leq \underbrace{K(L,T)}_{\text{exponential in }L,T}\max\{\alpha_k\mid t_k< T\}
\end{equation} 

\subsection{Optimization}

\dhighlight{Goal:} Find, for some loss function \(\Gamma:\R^d\to \R_+\),
\begin{equation}
    \theta^\star\in\argmin \Gamma(\theta).
\end{equation} 
Use steepest-descent, formulated as ODE 
\begin{equation}\label{eq:gradient_flow}\frac{d}{dt}\vartheta=-\nabla_\theta \Gamma(\theta)\end{equation}
so called \dhighlight{gradient flow}.
\[\nabla \Gamma(\theta0)\perp \{\theta\in\R^d\mid \Gamma(\theta)=\Gamma(\theta_0)\}\eqqcolon S_{\Gamma}(\theta_0)\]
The gradient flow steers into the interior of \(S_{\Gamma}(\theta_0)\).

\begin{definition}\label{def:28}
    \begin{enumerate}
        \item[(i)] A set \(S\subset \R^d\) is \dhighlight{convex} if it contains all line segments with endpoints in \(S\)
        \item[(ii)] A function \(\Gamma:S\to\R\) with \(S\) convex, is called \dhighlight{convex} 
                    if for any \(\theta^0,\theta^1\in S\) and \(\rho\in (0,1)\) 
                    \[\Gamma((1-\rho)\theta^0+\rho\theta^1)\leq (1-\rho)\Gamma(\theta^0)+\rho\Gamma(\theta^1)\]
                    \(\Gamma\) is \dhighlight{strictly convex} if this inequality is strict whenever \(\theta^0\neq\theta^1\)
        \item[(iii)] If \(\Gamma\) is differentiable, then it is called \dhighlight{strongly convex} if for \(\delta_0>0\)
                     \[\langle\nabla \Gamma(\theta)-\nabla\Gamma(\theta^0),\theta-\theta^0\rangle\geq \delta_0 \Vert \theta-\theta_0\Vert^2,\ \forall \theta,\theta^0\in S\]           
    \end{enumerate}
\end{definition}

From numerical optimization we know:
\begin{theorem}\label{thm:29}
    Suppose that \(\Gamma:\R^d\to\R\) is convex. Then for given \(\theta^0\in R^d\)
    \begin{enumerate}
        \item[(i)] if \(\theta^0\) is a local minima, then it is also \dhighlight{a global minimum}
        \item[(ii)] if \(\Gamma\) is differentiable at \(\theta^0\), with \(\nabla\Gamma(\theta)=0\), then \(\theta^0\) is \dhighlight{a global minimum}
        \item[(iii)] if either(i) or (ii) hold, and if \(\Gamma\) is strictly convex, then \(\theta^0\) is the \dhighlight{unique global minimum }   
    \end{enumerate}
\end{theorem}

\begin{proposition}\label{prop:30}
    Suppose that \(\Gamma\) is continuously differentiable, convex and coercive,
    with unique minimizer \(\theta^\star\). Then the gradient flow 
    \[\frac{d}{dt}\vartheta=-\nabla\Gamma(\vartheta)\]
    is globally asymptotically stable, with unique equilibrium
    \(\theta^\star\). 
    
    If \(\Gamma\) is strongly convex, then the rate of convergence is exponential
    \[\Vert \vartheta_t-\theta^\star\Vert\leq e^{-\delta_0 t}\Vert \vartheta_0-\theta^\star\Vert,\]
    where \(\delta_0\) comes from theorem \ref{thm:29}.
\end{proposition}
% TODO: Fix Vert missing?
\markeol{07}
\beginlecture{08}{08.05.2025}
\begin{proof}
    We use as Lyapunov function \(V(\theta)=\frac{1}{2}\Vert \theta-\theta^\star\Vert^2\). From the chain rule
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)=-\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\vartheta_t-\theta^\star\right]
    \end{align*}
    By convexity we get the following bound 
    \[\Gamma(\theta^\star)\geq \Gamma(\vartheta_t)+\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\theta^\star-\vartheta_t\right]\]
    using the support condition this becomes 
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)\leq \Gamma(\theta^\star)-\Gamma(\vartheta_t)\leq 0
    \end{align*}
    since \(\theta^\star\) is the minimum. The strict inequality \((<0)\) holds when \(\vartheta_t\neq \theta^\star\).
    \(V\) fulfills definition \ref{def:1.20} and proposition \ref{prop:1.21} gives 
    global asymptotic stability. \marginnote{Coercive, therefore inf-compact}

    Under strict convexity 
    \begin{align*}
            \frac{d}{dt}V(\vartheta_t)&=-\left[\nabla_\theta \Gamma(\vartheta_t)-\underbrace{\nabla_\theta\Gamma(\theta^\star)}_{=0}\right]^\intercal \left[\vartheta_t-\theta^\star\right]\\
            &\stackrel{\text{strong convexity}}{\leq} -\delta_0 \Vert \vartheta_t-\theta^\star\Vert^2=-2\delta_0V(\vartheta_t)
    \end{align*}
    This implies \(V(\vartheta_t)\leq V(\vartheta_0)\exp(-2\delta_0t) \forall t\) by integrating.
\end{proof}

\begin{theorem}\label{thm:31}
    If the Polyak-Lojasiewiez % TODOFIX 
    (PL) inequality \marginnote{Used in stochastic gradient descent}
    \begin{equation}\label{eq:PL}
        \frac{1}{2}\Vert \nabla\Gamma (\theta)\Vert^2\geq \mu |\Gamma(\theta)-\Gamma(\theta^\star)|  
    \end{equation}
    holds then the gradient flow satisfies for each initial \(\vartheta_0\) 
    \[\Gamma(\vartheta_t)-\Gamma^\star\leq e^{-\mu t}(\gamma(\vartheta_0)-\Gamma^\star).\]
    If in addition \(\Gamma\) is coercive, then the solutions are bounded and 
    any limit point \(\theta_\infty\) of \(\{\vartheta_t\}\) is an optimizer 
    \[\Gamma(\theta_\infty)=\Gamma^\star\]
\end{theorem}

\begin{proof}
    We use \(V(\theta)=\frac{1}{2}\vert \Gamma(\theta)-\Gamma^\star\vert\)
    for the Lyapunov function. 
    \begin{align*}
        \implies \frac{d}{dt}V(\vartheta_t)&=\frac{1}{2}\nabla_\theta\Gamma(\vartheta_t)^\intercal \frac{d}{dt}\vartheta_t\\
        &=-\frac{1}{2}\Vert \nabla\Gamma(\vartheta_t)\Vert^2\leq -\mu V(\vartheta_t)
    \end{align*}
    This implies using the same technique as in the previous proof
    \begin{align*}
        \frac{1}{2}\left[\Gamma(\vartheta_t)-\Gamma^\star\right]&=V(\vartheta_t)\leq e^{-\mu t}V(\vartheta_0)\\
        &= e^{-\mu t}\frac{1}{2}\left[\Gamma(\vartheta_0)-\Gamma^\star\right]
    \end{align*}
    If \(\Gamma\) is coercive, then trajectories of \(\vartheta\) evolve
    in the compact set \(S=\{\theta\mid V(\theta)\leq V(\vartheta_0)\}\). If 
    \(theta_\infty\) is a limit point \(\theta_\infty=\lim_{n\to\infty}\vartheta_{t_n}\)
    for \(t_n\to\infty\).
    Using the continuity of the loss function, this implies optimality:
    \[\Gamma(\theta_\infty)=\lim_{n\to\infty}\Gamma(\vartheta_{t_n})=\Gamma^\star\qedhere\] 
\end{proof}
Consider the Euler method for the gradient flow:
\begin{equation}\label{eq:euler-gradient-flow}
    \theta_{k+1}=\theta_k-\alpha \nabla\Gamma(\theta_k)
\end{equation}
\begin{theorem}\label{thm:32}
    Suppose that \(\Gamma\) satisfies
    \begin{enumerate}\marginnote{These are more general version of global Lipschitz and convexity}
        \item[(i)] the \(L-\)smooth inequality (LSI) \[\Gamma(\theta')\leq\Gamma(\theta)+[\theta'-\theta]^\intercal\nabla\Gamma(\theta)+\frac{1}{2}L\Vert \theta'-\theta\Vert^2\]
        \item[(ii)] the PLE \ref{eq:PL}  
    \end{enumerate}
    Then it holds for \(\alpha\leq \frac{1}{2}\)
    \[\Gamma(\theta_{k})-\Gamma^\star\leq (1-\alpha\mu)^k[\Gamma(\theta_0)-\Gamma^\star].\]
\end{theorem}

\begin{proof}
    \begin{align*}
        \Gamma(\theta_{k+1}-\Gamma(\theta_k))& \stackrel{\text{LSI}}{\leq} [\theta_{k+1}-\theta_k]^\intercal\nabla\Gamma(\theta_k)+\frac{1}{2}L\Vert\theta_{k+1}-\theta_k \Vert^2\\
        &\stackrel{\ref{eq:euler-gradient-flow}}{=}-\alpha\Vert \nabla\Gamma(\theta_k)\Vert^2+\frac{1}{2}L\alpha^2\Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &=(-\alpha+\frac{1}{2}L\alpha^2)\Vert \nabla\Gamma(\theta_k)\Vert^2
    \end{align*}
    If \(\alpha\leq \frac{1}{L}\) then \((-\alpha+\frac{1}{2}L\alpha^2)\leq \frac{1}{2}\alpha\)
    
    \begin{align*}
        &\leq -\frac{1}{2}\alpha \Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &\stackrel{\text{LSI}}{\leq}-\alpha\mu\vert \Gamma(\theta_k)-\Gamma^\star\vert     
    \end{align*}
    and therefore 
    \[\Gamma(\theta_{k+1})-\Gamma^\star\leq (1-\alpha\mu)(\Gamma(\theta_k)-\Gamma^\star)\]
    after iterating \(k-1\) times  we obtain the result.
\end{proof}

\begin{lemma}\label{lem:33}
    Suppose that \(\nabla\Gamma\) is globally Lipschitz 
    \[\Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert\leq L \Vert \theta'-\theta\Vert,\ \forall \theta,\theta'\in S\]
    Then \begin{enumerate}
        \item[(i)]  \(\vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert\leq L \Vert \theta'-\theta\Vert^2\)
        \item[(ii)] if \(S\) is convex, then \(\Gamma\) is \(L\)-smooth 
    \end{enumerate}
\end{lemma}

\begin{proof}
    \highlight{(i)}
    \begin{align*}
        \vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert & \leq \Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert \Vert \theta'-\theta\Vert\\
        &\leq L \Vert \theta'-\theta\Vert
    \end{align*}
    \marginnote{\(\theta^t\) in \(S\), since \(S\) is convex}
    \highlight{(ii)} for \(\theta',\theta\in S\) denote \(S\ni\theta^t\coloneqq\theta+t(\theta'-\theta)\)
    and \(\xi^t=\Gamma(\theta^t)\).
    \begin{align*}
        \frac{d}{dt}\xi^t&=\langle\nabla\Gamma(\theta^t),\theta'-\theta \rangle\\
        \frac{d}{dt}xi^t-\frac{d}{dt}xi^0-&=\langle\nabla\Gamma(\theta^t)-\nabla\Gamma(\theta^0),\theta'-\theta \rangle\\
        &\stackrel{(i)}{\leq} tL \Vert \theta'-\theta\Vert^2
    \end{align*} 
    Now integrate 
    \begin{align*}
        \Gamma(\theta')=\xi^1&=\xi^0+\int_0^1\frac{d}{dt}\xi^t dt \\
        &\leq \xi^0+\frac{d}{dt}\xi^0+\frac{1}{2}L\Vert \theta'-\theta\Vert^2 \\
        &=\Gamma(\theta)+\langle \nabla\Gamma(\theta),\theta'-\theta\rangle+\frac{1}{2}L\Vert \theta'-\theta\Vert\qedhere
    \end{align*}
\end{proof}

\begin{remark}
    Strong convexity:\marginnote{This gives a bound on the loss function from both sides \dots} 
    \begin{align*}
        \langle\nabla\Gamma(\theta')-\Gamma(\theta),\theta'-\theta\rangle\geq \delta_0\Vert \theta'-\theta\Vert^2
    \end{align*}
    With \(D_\Gamma(y\mid x)=\Gamma(y)-\Gamma(x)+\langle \nabla\Gamma(x),y-x\rangle\)
    is the \dhighlight{Bregman divergence}.
    \[\frac{\mu}{2}\Vert \theta'-\theta\Vert^2\leq D_\Gamma(\theta'\mid\theta)\leq \frac{L}{2}\Vert \theta'-\theta\Vert^2\]
\end{remark}

\subsection{Qausi stochastic approximation}%TODO: FIx

Assume there are observations \(\Phi_n\subset \Omega\), which 
we might consider as realizations of a random variable \(\Phi\).
We have \[f:\R^d\times \Omega\to\R^d\]
\[\bar{f}(\theta)\coloneqq \bE(\underbrace{f(\theta,\Phi)}_{\text{what we observe}}), \theta\in\R^d\]
As before we look for \(\bar{f}(\theta^\star)=0\)
\[\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\]
A \dhighlight{key assumption} is that what happens when following the state 
dynamics in any step depends only on the current state.\marginnote{I.e. we have the Markov property}

\[\Phi_n=[\cos(\omega n),\sin(\omega n)],\omega>0\]
\textit{Markov chain on the unit circle}. We will talk about the probing signal \(\xi\)
and consider 
\marginnote{the book uses \(\Theta\) instead of \(\hat{\theta}\)}
\begin{equation}\label{eq:qsa-ode}
    \frac{d}{dt}\hat{\theta}_t=a_t  f(\hat{\theta}_t,\xi_t)
\end{equation}
a \dhighlight{quasistochastic approximation(QSA)}-ODE, \(a_t\) is the \dhighlight{step size}. 

For deterministic probing signals, we mainly consider two examples 
\marginnote{Mixture of sin functions}\[\xi_t=\sum_{i=1}^K \overbrace{V^i}^{\in \R^m}sin(2\pi[\Phi_i+\omega_i t])\]
\dhighlight{Mixture of periodic functions}, fixed \(K\), \dhighlight{phase \(\{\Phi_i\}\)}, \dhighlight{frequencies} \(\{\omega_i\}\).

\[\xi_t=\sum_{i=1}^K V^i[\Phi_i+\omega_it]_{\text{modulo 1}}\]

\begin{figure}[H]\label{fig:2.01}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 2.01}
\end{figure}

These signals have well defined steady-state means and covariance matrices.

Special case: \(\xi_t(i)=\sqrt{2}\sin(\omega_it),\ 1\leq i\leq m,\omega_i\neq \omega_j\forall i\neq j\). Then the \dhighlight{steady-state mean}
\begin{align*}
    \lim_{T\to\infty}\int_{0}^{T}\xi_t dt =0 
\end{align*}
and \dhighlight{covariance}
\begin{align*}
    \lim_{T\to\infty}\int_0^T\xi_i\xi_i^\intercal dt=\Id
\end{align*}

\markeol{08}