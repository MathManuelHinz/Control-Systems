\chapter{ODE methods for algorithm design}
\beginlecture{07}{06.05.2025}

\section{ODE methods for algorithm design} % TODO: Fix

Four steps:
\begin{itemize}
    \item Formulate the algorithmic goal as the root finding problem 
        \[\bar{f}(\theta^\star)=0\]
    \item if necessary, refine the design of \(\bar{f}\) to ensure that 
          the associated ODE is \highlight{globally asymptotically stable} \marginnote{\(\theta\) for descrete settings, \(\vartheta\) for continuous settings. Both do the same job}
          \[\frac{d}{dt}\vartheta=\bar{f}(\vartheta)\] 
    \item  Is an \highlight{Euler-approximation} appropriate? \marginnote{\(\theta_{n+1}\) is the next iterate, not the next time step!}
          \begin{equation}\label{eq:euler}\theta_{n+1}=\theta_n+\alpha_{n+1}\bar{f}(\theta_n)\end{equation}
    \item Design an algorithm to approximate (\ref{eq:euler}) based on whatever 
          observation is available.

\end{itemize}

\begin{remark}
    The idea is to transfer the global stability from the ODE to the algorithm.
\end{remark}

\dhighlight{Goal:} Construct a vector field \(f\) such that \(\vartheta_t\)
converges to the \highlight{target} \(\theta^\star\in \R^d\), where 
\(\theta^\star\) is an equilibrium \[f(\theta^\star)=0.\]

In ODE theory one uses so called \dhighlight{Picard-Iteration}
\begin{equation}\label{eq:picard-iterations}
    \vartheta_t^{n+1}1=\theta_0+\int_0^t f(\vartheta_\tau^n)d\tau,\ 0\leq t\leq T
\end{equation}
based on 
\begin{equation}\label{eq:fundamental_theorem}
    \vartheta_0+\int_0^t f(\vartheta_\tau)d\tau,\ 0 \leq t\leq T.
\end{equation}
\begin{proposition}\label{prop:25} % TODO: rename
    Suppose that the function \(f\) is globally Lipschitz continuous:
    \[\exists L >0: \forall x,y\in \R^d:\ \Vert f(x)-f(y)\Vert \leq L\Vert x -y \Vert\]    
    Then for each \(\theta_0\) there exists a unique solution to (\ref{eq:fundamental_theorem}).
    in the finite time horizon.
    Moreover, successive approximation is uniformly convergent:
    \[\lim_{n\to\infty}\max_{0\leq t\leq T}\Vert \vartheta_t^n-\vartheta_t=0\]
\end{proposition}

\begin{proposition}[Grönwall-Bellman-inequality]\label{prop:26}
    Let \(\alpha,\beta\) and \(z\) be non-negative functions defined on 
    \([0,T],\ T>0\). Assume that \(\beta,z\) are continuous and that 
    \[z_t\leq \alpha_t+\int_0^t\beta_s z_sds,\ 0\leq t\leq T\]
    Then it holds
    \begin{enumerate}
        \item[(i)] \(z_t\leq \alpha_t+\int_0^t \alpha_s \beta_s \exp\left(\int_s^t B_r dr\right)ds\)
        \item[(ii)] if in addition the function \(\alpha\) is non-decreasing, then \[z_t\leq \alpha_t\exp\left(\int_0^t B_sds\right),\ 0\leq t\leq T\] 
    \end{enumerate} 
\end{proposition}

\begin{proof}
    Both proofs can be found in any textbook on ODEs. 
    The second one is also found in \cite{Meyn_2022}.
\end{proof}

\begin{proposition}\label{prop:27}\marginnote{Not that nice, but at least a bound \dots}
    Consider \(\frac{d}{dt}\vartheta=f(\vartheta),\ \vartheta_0=\theta_0\) with \(f\) globally Lipschitz. Then 
    \begin{enumerate}
        \item[(i)] There is a constant \(B_f\) depending only on \(f\) such that, with \(t\geq 0\)
                   \begin{eqnarray}\Vert \vartheta_t\Vert \leq \left(B_f+\Vert \vartheta_\Vert\right)e^{Lt}-B_f\\
                    \Vert \vartheta_t-\vartheta_0\Vert \vert  B_f+L\Vert \vartheta_0\Vert \vert t e^{lt}
                   \end{eqnarray}
        \item[(ii)] If there is an equilibrium \(\theta^\star\), then for each initial condition:
                   \begin{equation}
                        \Vert \vartheta_t-\theta^\star\Vert \leq \Vert \vartheta_0-\theta^\star\Vert e^{Lt}
                   \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \highlight{(ii):} use \ref{eq:fundamental_theorem} to get 
    \[\vartheta_t-\theta^\star=\vartheta_0-\theta^\star+\int_0^tf(\vartheta_\tau)d\tau\]
    Since \(f(\theta_\star)=0\), we see 
    \begin{align*}
        \Vert f(\vartheta_\tau)\Vert &= \Vert f(\vartheta_\tau)-f(\theta^\star)\Vert\\
        &\leq L\underbrace{\Vert \vartheta_\tau-\theta^\star\Vert}_{\eqqcolon z_\tau}
    \end{align*}
    So 
    \begin{align*}
        z_t&\leq z_0+L \int_0^t z_t d\tau.
    \end{align*}
    Using proposition \ref{prop:26} (ii) with \(\beta_t\equiv L,\alpha_t\equiv z_0\)
    we get \[\Vert \vartheta_t-\theta^\star\Vert\leq \Vert \vartheta_t-\theta_0\Vert\exp(Lt)\]
    
    \highlight{(i):} take any \(\bar{\theta}\in\R^d\) and use the Lipschitz continuity 
    \begin{align*}
        \Vert f(\theta)\Vert &\leq \Vert f(\theta)-f(\bar{f})\Vert +\Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta-\bar{\theta}\Vert + \Vert f(\bar{\theta})\Vert \\
        &\leq L\Vert \theta\Vert + L\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert.
    \end{align*} 
    For any fixed \(\bar{\theta}\), define \(B_f=\Vert \bar{\theta}\Vert + \Vert f(\bar\theta)\Vert/L\)
    which gives 
    \begin{align*}
        \Vert f(\theta)\Vert \leq L \left[\Vert \theta\Vert + B_f\right],\ \theta\in\R^d
    \end{align*}
    using (\ref{eq:fundamental_theorem})
    \begin{align*}
        \Vert \vartheta_t\Vert +B_f &\leq \Vert \vartheta_0\Vert + B_f +\underbrace{L}_\beta\int_0^t \left[\underbrace{\Vert\vartheta_\tau+ B_f}_{z_\tau}\right]d\tau\\
        &\leq \left[\Vert \vartheta_0\Vert +B_f\right]\exp(Lt) 
    \end{align*}
    where the last step follows by the same trick as in (ii), i.e. by using Grönwall.
\end{proof}

\section{Euler's method once more}
\marginnote{Explict Euler, implicit Euler is nicer to analyze}
\begin{equation}\label{eq:euler_v2}
    \frac{\hat{\vartheta}_{t_{n+1}}-\hat{\vartheta}_{t_{n}}}{\alpha_{n+1}} = f(\hat{\vartheta}_{t_n}),\ \hat{\vartheta}_0=\vartheta_0=\theta_0
\end{equation}
or \[\hat{\vartheta}_{t_{n+1}}= \hat{\vartheta}_{t_{n}}+\alpha_{n+1} f(\hat{\vartheta}_{t_n})\]
It can be shown for \(f\) globally Lipschitz
\begin{equation}
    \max_{0\leq t\leq T}\Vert \hat{\vartheta}_t-\vartheta_t\Vert \leq \underbrace{K(L,T)}_{\text{exponential in }L,T}\max\{\alpha_k\mid t_k< T\}
\end{equation} 

\section{Optimization}

\dhighlight{Goal:} Find, for some loss function \(\Gamma:\R^d\to \R_+\),
\begin{equation}
    \theta^\star\in\argmin \Gamma(\theta).
\end{equation} 
Use steepest-descent, formulated as ODE 
\begin{equation}\label{eq:gradient_flow}\frac{d}{dt}\vartheta=-\nabla_\theta \Gamma(\theta)\end{equation}
so called \dhighlight{gradient flow}.
\[\nabla \Gamma(\theta0)\perp \{\theta\in\R^d\mid \Gamma(\theta)=\Gamma(\theta_0)\}\eqqcolon S_{\Gamma}(\theta_0)\]
The gradient flow steers into the interior of \(S_{\Gamma}(\theta_0)\).

\begin{definition}\label{def:28}
    \begin{enumerate}
        \item[(i)] A set \(S\subset \R^d\) is \dhighlight{convex} if it contains all line segments with endpoints in \(S\)
        \item[(ii)] A function \(\Gamma:S\to\R\) with \(S\) convex, is called \dhighlight{convex} 
                    if for any \(\theta^0,\theta^1\in S\) and \(\rho\in (0,1)\) 
                    \[\Gamma((1-\rho)\theta^0+\rho\theta^1)\leq (1-\rho)\Gamma(\theta^0)+\rho\Gamma(\theta^1)\]
                    \(\Gamma\) is \dhighlight{strictly convex} if this inequality is strict whenever \(\theta^0\neq\theta^1\)
        \item[(iii)] If \(\Gamma\) is differentiable, then it is called \dhighlight{strongly convex} if for \(\delta_0>0\)
                     \[\langle\nabla \Gamma(\theta)-\nabla\Gamma(\theta^0),\theta-\theta^0\rangle\geq \delta_0 \Vert \theta-\theta_0\Vert^2,\ \forall \theta,\theta^0\in S\]           
    \end{enumerate}
\end{definition}

From numerical optimization we know:
\begin{theorem}\label{thm:29}
    Suppose that \(\Gamma:\R^d\to\R\) is convex. Then for given \(\theta^0\in R^d\)
    \begin{enumerate}
        \item[(i)] if \(\theta^0\) is a local minima, then it is also \dhighlight{a global minimum}
        \item[(ii)] if \(\Gamma\) is differentiable at \(\theta^0\), with \(\nabla\Gamma(\theta)=0\), then \(\theta^0\) is \dhighlight{a global minimum}
        \item[(iii)] if either(i) or (ii) hold, and if \(\Gamma\) is strictly convex, then \(\theta^0\) is the \dhighlight{unique global minimum }   
    \end{enumerate}
\end{theorem}

\begin{proposition}\label{prop:30}
    Suppose that \(\Gamma\) is continuously differentiable, convex and coercive,
    with unique minimizer \(\theta^\star\). Then the gradient flow 
    \[\frac{d}{dt}\vartheta=-\nabla\Gamma(\vartheta)\]
    is globally asymptotically stable, with unique equilibrium
    \(\theta^\star\). 
    
    If \(\Gamma\) is strongly convex, then the rate of convergence is exponential
    \[\Vert \vartheta_t-\theta^\star\Vert\leq e^{-\delta_0 t}\Vert \vartheta_0-\theta^\star\Vert,\]
    where \(\delta_0\) comes from theorem \ref{thm:29}.
\end{proposition}
\markeol{07}
\beginlecture{08}{08.05.2025}
\begin{proof}
    We use as Lyapunov function \(V(\theta)=\frac{1}{2}\Vert \theta-\theta^\star\Vert^2\). From the chain rule
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)=-\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\vartheta_t-\theta^\star\right]
    \end{align*}
    By convexity we get the following bound 
    \[\Gamma(\theta^\star)\geq \Gamma(\vartheta_t)+\nabla_\theta\Gamma(\vartheta_t)^\intercal\left[\theta^\star-\vartheta_t\right]\]
    using the support condition this becomes 
    \begin{align*}
        \frac{d}{dt}V(\vartheta_t)\leq \Gamma(\theta^\star)-\Gamma(\vartheta_t)\leq 0
    \end{align*}
    since \(\theta^\star\) is the minimum. The strict inequality \((<0)\) holds when \(\vartheta_t\neq \theta^\star\).
    \(V\) fulfills definition \ref{def:1.20} and proposition \ref{prop:1.21} gives 
    global asymptotic stability. \marginnote{Coercive, therefore inf-compact}

    Under strict convexity 
    \begin{align*}
            \frac{d}{dt}V(\vartheta_t)&=-\left[\nabla_\theta \Gamma(\vartheta_t)-\underbrace{\nabla_\theta\Gamma(\theta^\star)}_{=0}\right]^\intercal \left[\vartheta_t-\theta^\star\right]\\
            &\stackrel{\text{strong convexity}}{\leq} -\delta_0 \Vert \vartheta_t-\theta^\star\Vert^2=-2\delta_0V(\vartheta_t)
    \end{align*}
    This implies \(V(\vartheta_t)\leq V(\vartheta_0)\exp(-2\delta_0t) \forall t\) by integrating.
\end{proof}

\begin{theorem}\label{thm:31}
    If the Polyak-Lojasiewicz  
    (PL) inequality \marginnote{Used in stochastic gradient descent}
    \begin{equation}\label{eq:PL}
        \frac{1}{2}\Vert \nabla\Gamma (\theta)\Vert^2\geq \mu |\Gamma(\theta)-\Gamma(\theta^\star)|  
    \end{equation}
    holds then the gradient flow satisfies for each initial \(\vartheta_0\) 
    \[\Gamma(\vartheta_t)-\Gamma^\star\leq e^{-\mu t}(\gamma(\vartheta_0)-\Gamma^\star).\]
    If in addition \(\Gamma\) is coercive, then the solutions are bounded and 
    any limit point \(\theta_\infty\) of \(\{\vartheta_t\}\) is an optimizer 
    \[\Gamma(\theta_\infty)=\Gamma^\star\]
\end{theorem}

\begin{proof}
    We use \(V(\theta)=\frac{1}{2}\vert \Gamma(\theta)-\Gamma^\star\vert\)
    for the Lyapunov function. 
    \begin{align*}
        \implies \frac{d}{dt}V(\vartheta_t)&=\frac{1}{2}\nabla_\theta\Gamma(\vartheta_t)^\intercal \frac{d}{dt}\vartheta_t\\
        &=-\frac{1}{2}\Vert \nabla\Gamma(\vartheta_t)\Vert^2\leq -\mu V(\vartheta_t)
    \end{align*}
    This implies using the same technique as in the previous proof
    \begin{align*}
        \frac{1}{2}\left[\Gamma(\vartheta_t)-\Gamma^\star\right]&=V(\vartheta_t)\leq e^{-\mu t}V(\vartheta_0)\\
        &= e^{-\mu t}\frac{1}{2}\left[\Gamma(\vartheta_0)-\Gamma^\star\right]
    \end{align*}
    If \(\Gamma\) is coercive, then trajectories of \(\vartheta\) evolve
    in the compact set \(S=\{\theta\mid V(\theta)\leq V(\vartheta_0)\}\). If 
    \(theta_\infty\) is a limit point \(\theta_\infty=\lim_{n\to\infty}\vartheta_{t_n}\)
    for \(t_n\to\infty\).
    Using the continuity of the loss function, this implies optimality:
    \[\Gamma(\theta_\infty)=\lim_{n\to\infty}\Gamma(\vartheta_{t_n})=\Gamma^\star\qedhere\] 
\end{proof}
Consider the Euler method for the gradient flow:
\begin{equation}\label{eq:euler-gradient-flow}
    \theta_{k+1}=\theta_k-\alpha \nabla\Gamma(\theta_k)
\end{equation}
\begin{theorem}\label{thm:32}
    Suppose that \(\Gamma\) satisfies
    \begin{enumerate}\marginnote{These are more general version of global Lipschitz and convexity}
        \item[(i)] the \(L-\)smooth inequality (LSI) \[\Gamma(\theta')\leq\Gamma(\theta)+[\theta'-\theta]^\intercal\nabla\Gamma(\theta)+\frac{1}{2}L\Vert \theta'-\theta\Vert^2\]
        \item[(ii)] the PL inequality \ref{eq:PL}  
    \end{enumerate}
    Then it holds for \(\alpha\leq \frac{1}{2}\)
    \[\Gamma(\theta_{k})-\Gamma^\star\leq (1-\alpha\mu)^k[\Gamma(\theta_0)-\Gamma^\star].\]
\end{theorem}

\begin{proof}
    \begin{align*}
        \Gamma(\theta_{k+1}-\Gamma(\theta_k))& \stackrel{\text{LSI}}{\leq} [\theta_{k+1}-\theta_k]^\intercal\nabla\Gamma(\theta_k)+\frac{1}{2}L\Vert\theta_{k+1}-\theta_k \Vert^2\\
        &\stackrel{\ref{eq:euler-gradient-flow}}{=}-\alpha\Vert \nabla\Gamma(\theta_k)\Vert^2+\frac{1}{2}L\alpha^2\Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &=(-\alpha+\frac{1}{2}L\alpha^2)\Vert \nabla\Gamma(\theta_k)\Vert^2
    \end{align*}
    If \(\alpha\leq \frac{1}{L}\) then \((-\alpha+\frac{1}{2}L\alpha^2)\leq \frac{1}{2}\alpha\)
    
    \begin{align*}
        &\leq -\frac{1}{2}\alpha \Vert \nabla\Gamma(\theta_k)\Vert^2\\
        &\stackrel{\text{LSI}}{\leq}-\alpha\mu\vert \Gamma(\theta_k)-\Gamma^\star\vert     
    \end{align*}
    and therefore 
    \[\Gamma(\theta_{k+1})-\Gamma^\star\leq (1-\alpha\mu)(\Gamma(\theta_k)-\Gamma^\star)\]
    after iterating \(k-1\) times  we obtain the result.
\end{proof}

\begin{lemma}\label{lem:33}
    Suppose that \(\nabla\Gamma\) is globally Lipschitz 
    \[\Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert\leq L \Vert \theta'-\theta\Vert,\ \forall \theta,\theta'\in S\]
    Then \begin{enumerate}
        \item[(i)]  \(\vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert\leq L \Vert \theta'-\theta\Vert^2\)
        \item[(ii)] if \(S\) is convex, then \(\Gamma\) is \(L\)-smooth 
    \end{enumerate}
\end{lemma}

\begin{proof}
    \highlight{(i)}
    \begin{align*}
        \vert\langle \nabla\Gamma(\theta')-\nabla\Gamma(\theta),\theta'-\theta\rangle\vert & \leq \Vert \nabla\Gamma(\theta')-\nabla\Gamma(\theta)\Vert \Vert \theta'-\theta\Vert\\
        &\leq L \Vert \theta'-\theta\Vert
    \end{align*}
    \marginnote{\(\theta^t\) in \(S\), since \(S\) is convex}
    \highlight{(ii)} for \(\theta',\theta\in S\) denote \(S\ni\theta^t\coloneqq\theta+t(\theta'-\theta)\)
    and \(\xi^t=\Gamma(\theta^t)\).
    \begin{align*}
        \frac{d}{dt}\xi^t&=\langle\nabla\Gamma(\theta^t),\theta'-\theta \rangle\\
        \frac{d}{dt}xi^t-\frac{d}{dt}xi^0-&=\langle\nabla\Gamma(\theta^t)-\nabla\Gamma(\theta^0),\theta'-\theta \rangle\\
        &\stackrel{(i)}{\leq} tL \Vert \theta'-\theta\Vert^2
    \end{align*} 
    Now integrate 
    \begin{align*}
        \Gamma(\theta')=\xi^1&=\xi^0+\int_0^1\frac{d}{dt}\xi^t dt \\
        &\leq \xi^0+\frac{d}{dt}\xi^0+\frac{1}{2}L\Vert \theta'-\theta\Vert^2 \\
        &=\Gamma(\theta)+\langle \nabla\Gamma(\theta),\theta'-\theta\rangle+\frac{1}{2}L\Vert \theta'-\theta\Vert\qedhere
    \end{align*}
\end{proof}

\begin{remark}
    Strong convexity:\marginnote{This gives a bound on the loss function from both sides \dots} 
    \begin{align*}
        \langle\nabla\Gamma(\theta')-\Gamma(\theta),\theta'-\theta\rangle\geq \delta_0\Vert \theta'-\theta\Vert^2
    \end{align*}
    With \(D_\Gamma(y\mid x)=\Gamma(y)-\Gamma(x)+\langle \nabla\Gamma(x),y-x\rangle\)
    is the \dhighlight{Bregman divergence}.
    \[\frac{\mu}{2}\Vert \theta'-\theta\Vert^2\leq D_\Gamma(\theta'\mid\theta)\leq \frac{L}{2}\Vert \theta'-\theta\Vert^2\]
\end{remark}

\section{Qausi stochastic approximation}%TODO: FIX

Assume there are observations \(\Phi_n\subset \Omega\), which 
we might consider as realizations of a random variable \(\Phi\).
We have \[f:\R^d\times \Omega\to\R^d\]
\[\bar{f}(\theta)\coloneqq \bE(\underbrace{f(\theta,\Phi)}_{\text{what we observe}}), \theta\in\R^d\]
As before we look for \(\bar{f}(\theta^\star)=0\)
\[\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\]
A \dhighlight{key assumption} is that what happens when following the state 
dynamics in any step depends only on the current state.\marginnote{I.e. we have the Markov property}

\[\Phi_n=[\cos(\omega n),\sin(\omega n)],\omega>0\]
\textit{Markov chain on the unit circle}. We will talk about the probing signal \(\xi\)
and consider 
\marginnote{the book uses \(\Theta\) instead of \(\hat{\theta}\)}
\begin{equation}\label{eq:qsa-ode}
    \frac{d}{dt}\hat{\theta}_t=a_t  f(\hat{\theta}_t,\xi_t)
\end{equation}
a \dhighlight{quasistochastic approximation(QSA)}-ODE, \(a_t\) is the \dhighlight{step size}. 

For deterministic probing signals, we mainly consider two examples 
\marginnote{Mixture of sin functions}\[\xi_t=\sum_{i=1}^K \overbrace{V^i}^{\in \R^m}sin(2\pi[\Phi_i+\omega_i t])\]
\dhighlight{Mixture of periodic functions}, fixed \(K\), \dhighlight{phase \(\{\Phi_i\}\)}, \dhighlight{frequencies} \(\{\omega_i\}\).

\[\xi_t=\sum_{i=1}^K V^i[\Phi_i+\omega_it]_{\text{modulo 1}}\]

\begin{figure}[H]\label{fig:2.01}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 2.01}
\end{figure}

These signals have well defined steady-state means and covariance matrices.

Special case: \(\xi_t(i)=\sqrt{2}\sin(\omega_it),\ 1\leq i\leq m,\omega_i\neq \omega_j\forall i\neq j\). Then the \dhighlight{steady-state mean}
\begin{align*}
    \lim_{T\to\infty}\int_{0}^{T}\xi_t dt =0 
\end{align*}
and \dhighlight{covariance}
\begin{align*}
    \lim_{T\to\infty}\int_0^T\xi_i\xi_i^\intercal dt=\Id
\end{align*}
\markeol{08}
\beginlecture{09}{13.05.2025}
We now use a slightly different notation \(\hat{\theta}\) becomes \(\tilde{\theta}\).
\begin{equation}\label{eq:QSA-ODE}
    \frac{d}{dt}\tilde{\theta}_t=a_t f(\tilde{\theta},\xi_t)
\end{equation}
\(a_t\) non-negative.

Now consider integrating \(y:[0,1]\to\R\). Basic Monte-Carlo 
\begin{equation}\label{eq:monte-carlo-integration}
    \theta_n=\frac{1}{n}\sum_{i=0}^{n-1} y(\underbrace{\Phi(k)}_{\sim\text{Unif}([0,1])})
\end{equation} 

A \dhighlight{QSA} approach is to use the saw tooth function 
\[\xi_t=t(\text{modulo 1}).\]
Obtain estimate by 
\begin{equation}\label{eq:estimate_qsa}\tilde{\theta}=\frac{1}{t}\int_0^ty(\xi_t)dr\end{equation}
with a reasonable discretization afterwards.

To use (QSA-ODE (\ref{eq:QSA-ODE})) \(f(\theta,\xi)=y(\xi)-\theta\) with mean vector field 
\begin{align*}
    \bar{f}(\theta)&=\lim_{T\to\infty}\int_0^T f(\theta,\xi_t)dt\\
        &=\int_0^1 y(\xi_t)dt-\theta
\end{align*}
which gives \(\theta^\star=\int_0^1y(\xi_t)dt\) as the unique root of \(\bar{f}\). 
The QSA-ODE \ref{eq:QSA-ODE}
is \[\frac{d}{dt}\tilde{\theta}_t=a_t[y(\xi_t)-\tilde{\theta}_t]\]
(\ref{eq:estimate_qsa}) can be transformed into 
\begin{equation}
    \frac{d}{dt}\tilde{\theta}_t=\left[-\frac{1}{t^2}\int_0^t y(\xi_r)dr+\frac{1}{t}y(\xi_t)\right]=\underbrace{\frac{1}{t}}_{\eqqcolon a_t}\left[y(\xi_t)-\theta_t\right]
\end{equation}

\begin{example}
    \(y(\theta)=e^{4}(\sin(100\theta))\), mean \(\theta^\star\approx-0.5\approx -0.48\).
    Choose \(a_t=\frac{g}{1+t}\)
\end{example}

\section{Approximate Policy Improvement}

nonlinear state model in continuous time:
\begin{align}
    \frac{d}{dt}x_t=f(x_t,u_t),& t\geq 0\\
    J^\star(x)=\min_{\underbar{u}}\int_0^\infty c(x_t,u_t)dt & x=x_0
\end{align}

Given feedback law \(u_t=\phi(x_t)\), we have 
\begin{equation}
    J^\phi(x)=\int_0^\infty c(x_t,\phi(x_t))dt,\ x=x_0
\end{equation}
%TODO: y might be g here?
\begin{proposition}\label{prop:34}
    If \(J\) is finite, then for each initial condition \(x_0\) and each \(t\)
    \[\frac{d}{dt}J(x_t)=-c(x_t)\]
    If \(J\) is continuously differentiable, then the Lyapunov bound \(\frac{d}{dt} V(x_t)\)
    from definition \ref{def:1.20}  follows with equality
    \[\nabla J(x)f(x)=-c(x)\]
\end{proposition}

\begin{proof}
    For any \(T>0\), \(J(x_0)=\int_0^Tc(x_r)dr+J(x_T)\). For \(t\geq 0,\delta>0\) given,
    use \(T=t+\delta\) and \(T=t\) and subtract:
    \begin{align*}
        0=J(x_0)-J(x_0)&=\int_t^{t+\delta}c(x_r)dr+(J(x_{t+\delta})-J(x_t))\\
        &=\underbrace{\frac{1}{\delta}\int_t^{t+\delta}c(x_r)dr}_{\stackrel{\delta\to 0}{\to}c(x_t)}+\underbrace{\frac{1}{\delta}(J(x_{t+\delta})-J(x_t))}_{\stackrel{\delta\to 0}{\to}\frac{d}{dt}J(x_t)}\\
        &\implies \frac{d}{dt}J(x_t)=-c(x_t)
    \end{align*}
    Using the chain rule yields the second equation.
\end{proof}

For \(J^\phi\) we have 
\begin{align*}
    0=c(x,\phi(x))+\nabla J^\phi(x)\cdot f(x,\phi(x))
\end{align*}

\dhighlight{Policy Improvement in  continuous time:}
\[\phi^+(x)\in \argmin_{u}\{\underbrace{c(x,u)+\nabla J(x)\cdot f(x,u)}_{\text{need to approximate by }Q^\phi(x,u)}\}\]
Now aim for updating of \(Q\)-function. Add to the above \(J^\phi\) on both sides 
\begin{align*}
    J^\phi(x)&=J^\phi(x)+c(x,\phi(x))+\nabla J^\phi(x)\cdot f(x,\phi(x))        
\end{align*}
We solved for the optimal \(Q\)-function by using a fixed point equation,
with \(\underbar{Q}^\phi(x)=Q^\phi(x,\phi(x))\) we write \marginnote{\(\underbar{Q}\) for the fixed, but optimal choice of \(u\)}
\[Q^\phi(x,u)=\underbar{Q}^\phi(x)+c(x,u)+\nabla \underbar{Q}^\phi(x)f(x,u).\]
Consider \(\{Q^\theta\mid \theta\in \R^d\}\) family of approximations. 
Bellman errors (Temporal differences expressions?) gives 
\begin{equation}\label{eq:bellman_error_policy_improvement_continuous}
    B^\theta(x_t,u_t)=-Q^\theta(x_t,u_t)+\underbar{Q}^\theta(x)+c(x_t,u_t)+\underbrace{\nabla\underbar{Q}^\theta(x) f(x_t,u_t)}_{=\frac{d}{dt}Q^\theta(x_t)}
\end{equation}

Everything on the RHS is can be observed for any state-action pair without knowledge of \(f\).
Now, find \(\theta^\star\) that minimizes
\[\Vert B^\theta\Vert^2=\lim_{T\to\infty}\frac{1}{T}\int_0^T[B^\theta(x_t,u_t)]^2dt\]
Choose feedback law with exploration \(u_t=\tilde{\phi}(x_t,\xi_t)\). Assuming
bounded state trajectories, such that (\ref{eq:bellman_error_policy_improvement_continuous}) exists,
define \(\Gamma(\theta)=\frac{1}{2}\Vert B^\theta\Vert^2\), we get 
\begin{align*}
    0&\stackrel{!}{=}\nabla\Gamma(\theta)=\lim_{t\to\infty}\int_0^T\left[B^\theta(x_t,u_t)\right]\nabla_\theta B^\theta(x_t,u_t)dt
\end{align*}
Gradient flow 
\begin{align*}
    \frac{d}{dt}\vartheta_t=-\nabla_\theta\Gamma(\vartheta_t)
\end{align*}
QSA counterpart is (\ref{eq:bellman_error_policy_improvement_continuous})
with probing signal 
\[\frac{d}{dt}\tilde{\theta}_t=-a_tB^{\tilde{\theta}_t}(x_t,u_t)\kappa_t^{\tilde{\theta}_t}\]
with 
\begin{align*}
    \kappa_t^{\tilde{\theta}_t}&=\nabla_\theta B^theta(x_t,u_t)\\
    &=-\nabla_\theta Q^\theta(x_t,u_t)+\{\nabla_\theta Q^\theta(x_t,\phi(x_t))+ \frac{d}{dt}\nabla_\theta Q^\theta(x_t,\phi(x_t))\}
\end{align*}
assuming we can exchange differentiation w.r.t time and w.r.t \(\theta\).

(QSA-ODE)
\begin{align*}
    \frac{d}{dt}\tilde{\theta}_t=a_tf(\tilde{\theta}_t,\xi_t)
\end{align*}
aim to relate this to 
\[\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t).\]
\begin{lemma}\label{lem:35}
    Define the change of variables 
    \[\tau=s_t\coloneqq\int_0^t a_rdr,\ t\geq t_0.\]
    Let \(\{\vartheta_\tau\mid\tau\geq \tau_0\}\) the solution to 
    the ODE above initialized to \(\tau_0=s_{t_0}\) with \(\vartheta_{\tau_0}=\tilde{\theta}_{t_0}\).
    The solution to 
    \[\frac{d}{dt}\bar{\theta}_t=a_t\bar{f}(\bar{\theta}_t),\ t\geq t_0,\ \bar{\theta}_{t_0}=\tilde{\theta}_{t_0}\]
    is given by \(\bar{\theta}_t=\vartheta_\tau\).
\end{lemma}
\begin{proof}
    Change of variables and observing that \[d\tau=a_tdt.\]
\end{proof}
\markeol{09}
\beginlecture{10}{15.05.2025}
Recall \(\bar{f}(\theta)\coloneqq\lim_{T\to\infty}\int_0^T f(\theta,\xi_t)dt\) for all \(\theta\in\R^d\).
Remember the temporal transformation 
\[\tau=s_t=\int_0^t a_rdr\]
and lemma \ref{lem:35}. Define 
\(\hat{\theta}_\tau=\tilde{\theta}(s^{-1}(\tau))=\tilde{\theta}_t\mid_{t=s^{-1}(\tau)}\). By 
the chain rule and observing that \(d\tau=a_tdt\) yields 
\[\frac{d}{d\tau}\hat{\theta}_\tau=\frac{d}{d\tau}\tilde{\theta}(s^{-1}(\tau))=f(\tilde{\theta}(s^{-1}(\tau)),\xi(s^{-1}(\tau))).\]
\(\thetahat,\thetatilde\) differ only by a time scaling, so convergence of the one yields convergence of the other.
\begin{lemma}\label{lem:36}
    Consider the original ODE 
    \begin{equation}\label{eq:gradient_flow_ode}
        \frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)  
    \end{equation}
    and assume \(f\) is locally Lipschitz with constant \(L_f\).
    \marginnote{Version of proposition \ref{prop:27}}Then there exists a constant \(B_f\)
    depending only on \(f\), such that 
    \[\Vert\thetahat_t-\thetahat_0\Vert\leq \left(B_f+L_f|\Vert \thetahat_0\Vert\right)te^{L_ft},\ t\geq 0\] 
\end{lemma}

\begin{proof}
    Proof of proposition \ref{prop:27} in adapted notation.
\end{proof}

Now, denote by \(\vartheta_w^\tau,w\geq \tau\) the unique solution 
to (\ref{eq:gradient_flow_ode}):
\begin{equation*}
    \frac{\partial }{\partial w}\vartheta_w^\tau=\bar f(\vartheta_w^\tau),\ w\geq \tau,\ \vartheta_\tau^\tau=\thetahat_\tau
\end{equation*}

with that we get \marginnote{quasistochastic vs continuous}
\begin{enumerate}
    \item \(\vartheta_{\tau+v}^\tau=\thetahat_\tau+\int_0^{\tau+v}\bar{f}(\vartheta_w^\tau)dw,\ \tau,v\geq 0\)
    \item \(\thetahat_{\tau+v}=\thetahat_\tau+\int_\tau^{\tau+v} f(\thetahat_w,\xi(s^{-1}(w)))dw,\ \tau,v\geq 0\)
\end{enumerate}

The following assumptions will be used in the following:
\begin{enumerate}
    \item[\highlight{QSA1}] The process \(a\) is non-negative, monotonically decreasing and \(\lim_{t\to\infty} a_t=0,\int_0^\infty a_rdr=\infty\)\marginnote{it does not go to zero to fast} 
    \item[\highlight{QSA2}] The functions \(\hat{f},f\) are Lipschitz continuous with constant \(L_f\):\begin{align*}
        \Vert \bar{f}(\theta')-\bar{f}(\theta)\Vert &\leq \Vert L_f \Vert \theta'-\theta\Vert\\
        \Vert f(\theta',z)-f(\theta,z)\Vert &\leq \Vert L_f \Vert \theta'-\theta\Vert
    \end{align*} for all \(\theta,\theta'\in R^d,z\in\Omega\) and there exists a Lipschitz continuous functions \(b_0:R^d\to\R_+\), such that for all \(\theta\in\R^d\)\marginnote{Is my probing covering everything: ergocity, ergodic bound} 
\[\left\Vert \int_{t_0}^{t_1}f(\theta,\xi_t)-\bar{f}(\theta)dt\right\Vert\leq b_0(\theta),\ 0\leq t_1\leq t_1\]
    \item[\highlight{QSA3}] The ODE \(\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\) 
    has a globally asymptotically stable equilibrium \(\theta^\star\) 
\end{enumerate}
Consider first, arbitrary \(\theta\)
\begin{lemma}\label{lem:37}
    \marginnote{There is a connect to the law of large numbers ...}
    Assume (QSA1), (QSA2) hold for any fixed \(T>0\) and \(\theta\in\R^d\).
    \begin{align*}
        \left\Vert\int_{\tau}^{\tau+T}\left[f(\theta,\xi(s^{-1}(w)))-\bar{f}(\theta)\right]dw \right\Vert\leq b_0(\theta)\epsilon_\tau^f,
    \end{align*}   
    where \(\epsilon_\tau^f=3a_t\mid_{t=s^{-1}(\tau)}\) and \(b_0\) comes from (QSA2).
\end{lemma}
\begin{proof}
    Set \(\tilde{f}_w(\theta)=f(\theta,\xi_w)-\bar{f}(\theta)\) for each \(w,\theta\). Write 
    \marginnote{large \(\epsilon_t\) in the book? Prob. \(\mathcal{E}\)}
    \begin{align*}
        E_t=\int_0^t\tilde{f}_w(\theta)dw.
    \end{align*}
    By assumptions \(\Vert E_t\Vert\leq b_0(\theta),\ t\geq 0\).
    \begin{align*}
        \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt&\stackrel{\text{IbP}}{=}a_tE_t\mid_{t_0}^{t_1}-\int_{t_0}^{t_1}\vert a_t'\vert E_tdt \\
        \left\Vert \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt\right\Vert &\leq a_{t_0}\Vert E_{t_0}\Vert+ a_{t_1}\Vert E_{t_0}\Vert+\int_{t_0}^{t_1} \vert a_t'\vert E_tdt\\
        &\stackrel{a \text{ decreasing}}{\leq} 2 a_{t_0}b_0(\theta)-b_0(\theta)\int_{t_0}^{t_1} a_t' dt\\
        &\leq 3 a_{t_0} b_0(\theta) 
    \end{align*}
    Set \(t_0=s^{-1}(\tau),t_1=s^{-1}(\tau+T), t=s^{-1}(w)\),
    giving \(dw=a_tdt\)
    \begin{align*}
        \left\Vert \int_{\tau}^{\tau+T} [f(\theta,\xi(s^{-1}(w)))-\bar{f}(\theta)]dw \right\Vert&=
        \left\Vert \int_{t_0}^{t_1} a_t \tilde{f}_t(\theta)dt\right\Vert\\
        &\leq 3a_{t_0}b_0(\theta)=\epsilon_\tau^f b_0(\theta)
    \end{align*}
\end{proof}

\begin{proposition}\label{prop:38}
    Assuming that \(\thetahat\) is bounded. Then for any \(T>0\)
    \[\lim_{\tau\to\infty}\sup_{v\in[0,T]}\left\Vert \overbrace{\int_\tau^{\tau+v}\left[f(\hat{\theta}_w, \xi(s^{-1}(w)))-\bar{f}(\hat{\theta}_w)\right]dw}^{E_{\tau+v}^{\tau}}\right\Vert=0\]
    and 
    \[\lim_{\tau\to\infty}\sup_{v\in[0,T]}\left\Vert \hat{\theta}_{\tau+v}-\vartheta_{\tau+v}^\tau \right\Vert=0\]
\end{proposition}

\begin{proof}
    We use piecewise constant approximation, as in Riemannian integration, and set for \(\delta>0,\ \tau_k=\tau+k\delta,\ k\geq 0\) 
    \begin{align*}
        E_{\tau+v}^\tau=\sum_{k=0}^{n_v-1}\int_{\tau_k}^{\tau_{k+1}} \left[f(\thetahat_{\tau_k},\xi(s^{-1}(w)))-\bar{f}(\thetahat_{\tau_k})\right]dw +\epsilon_{v}^\tau,
    \end{align*}
    which holds due to (QSA1), Lipschitz condition, \(n_v=\lfloor \frac{v}{\delta}\rfloor\).
    and \[\Vert \epsilon_v^\tau\leq b_L v\delta\]
    for some finite constant \(b_L\). Assuming \(\thetahat\) is bounded, this bound is uniform in \(\tau\).
    For fixed \(\thetahat_{t_k}\) we can use lemma \ref{lem:37}, so 
    \begin{align*}
        \Vert E_{\tau+v}^\tau\Vert &\leq \sum_{k=0}^{n_v-1}\epsilon_{\tau_k}^fb_0(\thetahat_{t_k})+b_L v\delta\\
        &\leq \epsilon_\tau^f\sum_{k=0}^{n_v-1} b_0(\thetahat_{\tau_k})+b_L v\delta
    \end{align*}
    Let \(b<\infty\) denote a constant such that \(b_0(\thetahat_{\tau_k})\leq b\ \forall \tau\), which we can do since \(\thetahat\) is bounded, \(b_0\) Lipschitz.
    \begin{align*}
        \left\Vert E_{\tau+v}^\tau\right\Vert\leq b \frac{v}{\delta}\underbrace{\epsilon_\tau^f}_{\stackrel{\tau\to\infty}{\to }0\text{ by QSA1}}+b_Lv\delta
    \end{align*}
    For any \(T>0\)
    \begin{align*}
        \lim_{\tau\to\infty}\sup_{v\in [0,T]}\Vert E_{\tau+v}^\tau\Vert \leq 0+ b_L T\delta
    \end{align*}
    Since \(\delta>0\) was arbitrary, we have the first statement.


    For the second limit: \(E_r^\tau=\vartheta_r^\tau-\thetahat_r\). The pair of identities after lemma \ref{lem:36} give 
    using Lipschitz condition from (QSA2) we get
    \begin{align*}
        E_{\tau+v}^\tau&=0+\int_{\tau}^{\tau+v}\bar{f}(\thetahat_w)-f(\thetahat_w,\xi(s^{-1}(w)))dw + \int_\tau^{\tau+v}\underbrace{\left[\bar{f}(\vartheta_v^\tau)-\bar{f}(\thetahat_w)\right]}_{\Vert \dots \Vert \leq L_f \Vert E_w^\tau\Vert}dw\\
        \Vert E_{\tau+v}^\tau\Vert&\leq \delta^\tau+L_f \int_{\tau}^{\tau+v}\Vert E_w^\tau\Vert dw,
    \end{align*}
    where \[\delta^\tau\coloneqq \sup_{\tau'\geq \tau}\max_{0\leq v\leq T}\left\Vert \int_{\tau'}^{\tau'+v}\left[\bar{f}(\thetahat_w)-f(\thetahat_w,\xi(s^{-1}(w)))\right]dw\right\Vert\]    
    Grönwalls lemma gives 
    \[\Vert E_{\tau+v}^\tau\Vert \leq e^{Lf}\delta^\tau\forall \tau, \ 0\leq v\leq 1\]
    \(\delta^\tau\to 0\) for \(\tau\to\infty\) due to the first statement.
\end{proof}

\markeol{10}
\beginlecture{11}{20.05.2025}

\begin{theorem}[Boundedness implies convergence]\label{thm:39}
    Suppose that (QSA1-QSA3) hold. % TODO LINK
    hold. Further assume \highlight{ultimate boundedness}, i.e. that a \(b<\infty\) exists,
    such that for each \(\theta\in \R^d\) and \(z\in\Omega\) there is a \(T_{\theta,z}\),
    such that \(\Vert \thetahat_\tau \Vert \leq b\) for all \(\tau\geq T_{\theta,z}\), where 
    \(\thetahat_0=\theta,\xi_0=z\). Then the solution to (\ref{eq:QSA-ODE}) %FIX ODE
    \[\frac{d}{dt}\thetatilde_t=a_t f(\thetatilde_t,\xi_t)\] 
    converges to \(\theta^\star\) for each initial condition.
\end{theorem} 

\begin{proof}
    Consider the the time scaled \(\thetahat_t\)
    \begin{align*}
        \Vert \vartheta_{\tau}^\tau\Vert = \Vert \thetahat_t\Vert \stackrel{\text{pA}}{\leq} b,\ \tau\geq T_{\theta,z}
    \end{align*}
    Using (QSA3), i.e. \(\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\) has a globally asymptotically stable
    equilibrium \(\theta^\star\), we have that for every \(\epsilon>0\), there is exists 
    \(T_\epsilon>0\) s.t. \(\Vert \vartheta_{\tau+v}^\tau-\theta^\star\Vert<\epsilon\ \forall v\geq T_\epsilon\), % \Vert \vartheta_{\tau+v}^\tau-\theta^\star\Vert<\epsilon vllt. ist das richtig
    whenever \(\Vert \vartheta_\tau^\tau\Vert \leq b\).

    Then 
    \begin{align*}
        \limsup_{\tau\to\infty} \Vert \thetahat_{\tau+T_\epsilon}-\theta^\star\Vert &\leq \underbrace{\limsup_{\tau\to\infty}\Vert \thetahat_{\tau+T_\epsilon}-\vartheta_{\tau+T_\epsilon}^\tau\Vert}_{\to 0 \text{ by proposition \ref{prop:38}}} + \underbrace{\limsup_{\tau\to\infty}\Vert \vartheta_{\tau+T_\epsilon}^\tau-\theta^\star\Vert}_{\leq \epsilon} \qedhere
    \end{align*}
\end{proof}

\begin{lemma}[Weaker form of proposition \ref{prop:38} (ii)]\label{lem:40}
    For some \(\bar{\delta}<\infty\) and any \(0\leq T\leq 1\)
    \[\Vert \thetahat_{\tau+T}-\vartheta_{\tau+T}^\tau\Vert \leq e^{L_f}b_0(\thetahat_\tau)\epsilon_{\tau}^f+\bar{b}(1+\Vert \thetahat_\tau\Vert)T^2\]
    where \(b_0(\theta)\) and \(L_f\) are from (QSA2).
\end{lemma}

\begin{proof}
    Write \(E_r^\tau=\vartheta_r^\tau-\thetahat_r,\ r\geq T\). The pair of
    identities after lemma \ref{lem:36} give, after inserting \(\pm \bar{f}(\theta_w)\) 
    \begin{align*}
        E_{\tau+T}^\tau=0+\int_\tau^{\tau+T}\left[\bar{f}(\thetahat_w)-f(\thetahat_w,\xi(s^{-1}(w)))\right]dw+\int_\tau^{\tau+T}\left[\bar{f}(\vartheta_w^\tau)-\bar{f}(\thetahat_w)\right]dw
    \end{align*} 
    using (QSA2) we can bound \marginnote{like last lecture ...}
    \begin{align*}
        \Vert \bar{f}(\thetahat_w)-\bar{f}(\thetahat_\tau)\Vert &\leq L_f\Vert \thetahat_w-\thetahat_\tau\Vert\\
        \Vert f(\thetahat_w,\xi(s^{-1}(w)))-f(\thetahat_\tau,\xi(s^{-1}(w)))\Vert &\leq L_f\Vert \thetahat_w-\thetahat_\tau\Vert \\
        \Vert \bar{f}(\vartheta_w^\tau)-\bar{f}(\vartheta_w)\Vert &\leq L_f\Vert E_w^\tau\Vert 
    \end{align*}

    With that, for any \(T>0\) by inserting terms with \(\thetahat_\tau\)
    \begin{align*}
        \Vert E_{\tau+T}^\tau\Vert &\leq \left\Vert \int_{\tau}^{\tau+T}\left[\bar{f}(\thetahat_\tau)-f(\thetahat_\tau,\xi(s^{-1}(w)))\right]dw\right\Vert + 2L_f\int_{\tau}^{\tau+T}\Vert \thetahat_w-\thetahat_\tau\Vert+L_f\int_{\tau}^{\tau+T}\Vert E_w^\tau\Vert dw\\
        &\leq \alpha_T^\tau+ L_f \int_{\tau}^{\tau+T}\Vert E_w^\tau\Vert dw,
    \end{align*}
    where
    %TODO: FIx
    %
    \[\alpha_T^\tau\coloneqq \underbrace{b_o(\thetahat_\tau)}_{\text{from (QSA2)}}\epsilon_\tau^f+2L_f\int_0^T\Vert \thetahat_{\tau+w}-\thetahat_\tau\Vert dw\]
    Using Grönwalls lemma, proposition \ref{prop:26} (ii) \[\Vert E_{\tau+T}^\tau\Vert\leq \alpha_T^\tau e^{L_f T}\]
    Repeating the proof for proposition \ref{prop:27}, we get 
    \[\Vert \thetahat_{\tau+w}-\thetahat_\tau\Vert \leq (B_f+L_f\Vert \thetahat_\tau\Vert)we^{L_f w}.\]
    Increase \(e^{L_f w}\) to \(e^{L_f T}\) to get 
    \begin{align*}
        2\int_{0}^T\Vert \thetahat_{\tau+w}-\thetahat_\tau\Vert dw&\leq 2 (B_f + L_f\Vert \thetahat_\tau)e^{L_f T}\int_0^T w dw\\
        &=(B_f+L_f\Vert\thetahat_\tau\Vert)T^2 e^{L_f T}
    \end{align*}
    Hence 
    \[\alpha_T^\tau\leq b_0(\thetahat_\tau)\epsilon_\tau^f+L_f(B_f+L_f\Vert\thetahat_\tau\Vert)T^2 e^{L_f T}\]
    Since \(0\leq T\leq 1\), we can find \(\bar{b}<\infty\) to bound \(L_f(B_f+L_f\Vert\thetahat_\tau\Vert)T^2 e^{L_f T}\)
    by \(\bar{b}(1+\Vert \thetahat_\tau)\Vert T^2\), where \(\bar{b}\) depends on fixed \(B_f,L_f\).
\end{proof}

Reminder, \highlight{drift condition}
\[\langle \nabla f(\theta),f(\theta)\rangle < 0,\ \theta\neq \theta^\star\]

\begin{definition}[ultimately bounded]\label{def:41}
    The ODE \[\frac{d}{d\vartheta}_t=f(\vartheta_t),\ \vartheta_0=\theta_0\] is called 
    \dhighlight{ultimately bounded} if there exists a bounded set \(S\subset \R^d\), such that 
    for each initial condition \(\theta_0\), there is a time \(T(\theta_0)\) such that 
    \(\vartheta_t\in S \ \forall t\geq T(\theta_0)\).
\end{definition}

\begin{proposition}\label{prop:42}
    Assume that there is a continuously differentiable function \(V\)\marginnote{Lyapunov function}
    \(V:\R^d\to \R_+\) satisfying the Lyapunov condition\marginnote{If we are not in \(S\), we are getting pointed into that direction}
    \[\langle \nabla V(\theta),f(\theta)\rangle\leq -\delta_0,\ \theta\in S^c\]
    for some \(\delta_0>0\) and some set \(S\subset \R^d\). Then \(T_S(\theta)\leq \delta_0^{-1}V(\theta)\)
    for \(\theta\in\R^d\), where \marginnote{\dhighlight{first entrance time} \(T_S\)}
    \[T_S(\theta)=\min \{t\mid \vartheta_t\in S\},\ \vartheta_0=\theta\in \R^d.\]
    
    If in addition \(S\) is compact and \(V\) inf-compact, then the corresponding ODE is ultimately bounded. 
\end{proposition}

\begin{proof}
    Assume \(\delta_0=1\) w.l.o.g., we interpret the condition as \textit{along a path}
    \[\frac{d}{dt}V(\vartheta_t)\leq 1,\]
    for \(0\leq t\leq T_S(\theta),\ \vartheta_0=\theta\in \R^d\).
    \(T_N=\min(N,T_s(\theta))\), integrate both sides from \(t=0\) to \(t=T_N\).
    \begin{align*}
        -V(\vartheta_0)\leq V(\vartheta_{T_n})-V(\vartheta_0)\leq \int_0^{T_N} \frac{d}{dt}V(\vartheta_t)dt\leq -T_N
    \end{align*}
    or \(\min(N,T_S(\theta))\leq V(\vartheta_0)\). Choosing \(N\geq V(\vartheta_0)\) gives 
    the bound on the first entrance time:
    \[T_S(\theta)\leq \delta_0^{-1}V(\theta).\]
    
    Now we need to show that it stays in some \(S\). Now, \(S\) is compact, \(V\)
    is inf-compact, so there exists \(N<\infty\) such that \(S\subset S_V(N)=\{\theta\mid V(\theta)\leq N\}\),
    with \(S_V(N)\) compact as well. Hence  
    \begin{align*}
        \langle \nabla V(\theta),f(\theta)\rangle \leq -1, \ \theta\in \R^d,\ V(\theta)\geq N
    \end{align*}
    writing \(V(\theta)>N\) corresponds to \(\theta\in S_V(N)^c\). 
    
    Now, \(V(\vartheta_t)\) is therefore decreasing, whenever \(\vartheta_t\in S_V(N)^c\), this shows 
    that the set \(S_V(N)\) is \highlight{absorbing}, which gives that \[\vartheta_t\in S_V(N)\ \forall t\geq T_S(\theta).\qedhere\]
\end{proof}

\dhighlight{Assumption (QSV):}\marginnote{QSV1 in the book}

There exists a continuous function \(V:\R^d\to\R\), and constants
\(c_0>0,\delta_0\) s.t. for any initial condition \(\vartheta_0\) of the ODE 
and \(0\leq T\leq 1\) it holds for \(\Vert \vartheta_s\Vert>c_0\), that \marginnote{\(V\) is strictly decreasing in that setting}
\[V(\vartheta_{s+T})-V(\vartheta_s)\leq -\delta_0\int_0^T\Vert  \vartheta_{s+t}\Vert dt.\]
The Lyapunov function \(V\) is Lipschitz continuous with constant \(L_V\).  

%not the assumption anymore

If \(V\) is differentiable, then \(QSV\) implies 
\[\frac{d}{dt}V(\vartheta_t)\leq -\delta_0\Vert \vartheta_t\Vert,\]
whenever \(\Vert \vartheta_t\Vert >c_0\).
\markeol{11}
\beginlecture{12}{22.05.2025}
\begin{lemma}\label{lem:43}
    Assume \(V:\R^d\to\R_+\) is Lipschitz continuous and that for some constant \(T>0\),
    \(0<\delta_1<1\) and \(\tau_0,b<\infty\) it holds 
    \[V(\thetahat_{\tau+T})-V(\thetahat_\tau)\leq -\delta_1\Vert \thetahat_\tau\Vert\]
    for all \(\tau\geq \tau_0\), \(\Vert\thetahat_\tau\Vert>b\). Then the solution 
    to the time-scaled ODE 
    \begin{align*}\label{eq:ts-ode}
        \frac{d}{d\tau} \thetahat_\tau = f(\thetahat(s^{-1}(\tau)),\xi(s^{-1}(\tau)))
    \end{align*}
    is ultimately bounded. 
\end{lemma}

\begin{proof}
    For each initial condition \(\thetahat_0=\theta\) and \(\tau\geq \tau_0\), denote by 
    \(\hat{\tau}=\hat{\tau}(\theta,\tau)\coloneqq \min(v\geq 0\mid \Vert \thetahat_{\tau+v}\Vert \leq b)\),
    where \(\tau_0,b\) as before. For clarity, if \(\Vert\thetahat_{\tau+v}\Vert>b\)
    for all \(v\geq 0\), set \(\hat{\tau}=\infty\).

    For \(m\in Z_+\), define \(\hat{\tau}_m=\min(\hat{\tau},m)\). Then 
    \begin{align*}
        -\hat{\tau}_m b\delta_1&\geq -\delta_1\int_\tau^{\tau+\tau_m}\underbrace{\Vert \thetahat_w\Vert}_{\leq b} dw\\
        &\geq \int_\tau^{\tau+\hat{\tau}_m}(V(\thetahat_{w+T})-V(\thetahat_w))dw \\
        &=\int_{\tau+\hat{\tau}_m}^{\tau+\hat{\tau}_m+T}V(\thetahat_w)dw-.\int_\tau^{\tau+T} V(\thetahat_w)dw\\
        &\geq -.\int_\tau^{\tau+T} V(\thetahat_w)dw
    \end{align*}
    This bound is independent of \(m\), holds for all \(\hat{\tau}_m\). Therefore 
    \begin{align*}
        \hat{\tau}\leq \frac{1}{b\delta_1}\int_\tau^{\tau+T} V(\thetahat_w)dw
    \end{align*}
    \begin{align*}
        \int_\tau^{\tau+T} V(\thetahat_w)dw&\leq\int_\tau^{\tau+T}\vert V(\thetahat_w)-V(\thetahat_\tau)\vert +\vert V(\thetahat_\tau)\vert dw\\
        &\leq \int_{\tau}^{\tau+T}\underbrace{L_V\Vert \thetahat_w\Vert}_{\text{prop \ref{prop:27}: }\leq (C(V)+\Vert \thetahat_\tau\Vert)c(L_V,T)} + L_V\Vert \thetahat_\tau\Vert + \vert V(\thetahat_\tau)\vert dw 
    \end{align*}
    So the integral can be bounded by constants depending on fixed values. So we can obtain a bound 
    \begin{align*}
        \hat{\tau}\leq b_V (1+\Vert \thetahat_\tau\Vert).
    \end{align*}
    Hence \(\hat{\tau}(\theta,\tau)\) is everywhere finite. 

    Denote by \(b_1\sup \{\Vert \thetahat_{\tau+v}\Vert\mid \tau \geq \tau_0,v\leq \hat{\tau}(\theta,\tau),\Vert \thetahat_\tau\Vert\leq b+1\}\).
    In words, \(b_1\) bounds the maximum norm of any breakout %FIX\(\underbar{\theta}\) that begins \marginnote{\(\underbar{\theta}\), since we are talking about the whole process}
    at time \(\tau\) if \(\thetahat_\tau\in S=\{\theta\mid \Vert \theta\Vert\leq b+1\}\)
    and ends at the arrival time to the set 
    \[S_0\coloneqq \{\theta\mid \Vert \theta\Vert \leq b\}\] 
    denoted \(\tau+\hat{\tau}(\theta,\tau)\).

    Now every trajectory enters \(S_0\subset S\) for some \(\tau\geq \tau_0\), so 
    it fulfills that \(\Vert\thetahat_\tau\Vert\leq b_1\) for all \(\tau\) sufficiently large,
    which gives ultimate boundedness.
\end{proof}

\begin{proposition}\label{prop:44}
    Under (QSV), the solution to (\ref{eq:ts-ode}) is ultimately bounded, i.e. there exists 
    some \(b<\infty\) such that for any 
    \begin{align*}
        \thetahat_0=\theta,\limsup_{\tau\to\infty}\Vert \thetahat_\tau\Vert \leq b
    \end{align*}
\end{proposition}

\begin{proof}
    \(V\) is from (QSV) and \(c_0\) the constant. For \(0\leq T\leq 1,\Vert \thetahat_\tau\Vert\geq c_0+1\)
    \begin{align*}
        V(\thetahat_{\tau+T})-V(\thetahat_\tau)&=V(\thetahat_{\tau+T})-V(\vartheta_{\tau+T}^\tau)+V(\vartheta_{\tau+T}^\tau)-\underbrace{V(\vartheta_{\tau}^\tau)}_{=\thetahat_T}\\
        &\leq \vert V(\thetahat_{\tau+T})-V(\vartheta_{\tau+T}^\tau)\vert+V(\vartheta_{\tau+T}^\tau)-V(\vartheta_{\tau}^\tau)\\
        &\leq L_V\Vert \thetahat_{\tau+T}-\vartheta_{\tau+T}^\tau\Vert - \delta_0\int_0^T\underbrace{\Vert \vartheta_{\tau+T}^\tau\Vert}_{\leq \Vert \thetahat_\tau\Vert +\Vert \int_\tau^{\tau+T} \bar{f}(\vartheta_w^\tau)dw\Vert} dt\\
        &\leq L_V\Vert \thetahat_{\tau+T}-\vartheta_{\tau+T}^\tau\Vert-\delta_0T\Vert \thetahat_\tau\Vert\\
        &\stackrel{\text{Lemma \ref{lem:40}}}{\leq} L_V(e^{L_f}b_0(\thetahat_\tau)\epsilon_\tau^f+\bar{b}(1+\Vert\thetahat_\tau\Vert)T^2)-\delta_0T\Vert \thetahat_\tau\Vert
    \end{align*}
    So, we can choose \(T>0\) small enough and 
    \(\tau_0\) large enough, so that 
    \[V(\thetahat_{\tau+T})-V(\thetahat_\tau)\leq -\frac{1}{2}\delta_0 T\Vert \thetahat_\tau\Vert,\ \tau\geq \tau_0,\ \Vert \thetahat_\tau\Vert c_0+1\]
    and we can use the lemma \ref{lem:43}.\marginnote{Now we can ultimate boundedness and therefore convergence!}


\end{proof}


\section{Gradient free Optimization}

Reminder: \[\min_{\theta\in\R^d}\Gamma(\theta)\] 
we assume it has a unique minimizer \(\theta^\star\).
\begin{align*}
    \bar{f}(\theta=\nabla\Gamma(\theta))
\end{align*}
we look for \(\theta^\star\) with \(\bar{f}(\theta^\star)=0\). But,
we are using \(f(\theta,\xi_t)\) due to lack of information.

Generally, we design some \(\tilde{\nabla}_\Gamma(t)\) to approximate the above in an average sense
\[\int_{T_0}^{T_1}a_t\tilde{\nabla}_\Gamma(t)dt \approx \int_{T_0}^{T_1}a_t \nabla\Gamma(\thetatilde_t)dt,\ T_1>T_0>0\]
and construct and ODE 
\begin{equation}
    \frac{d}{dt}\thetatilde_t=-a_t\tilde{\nabla}_\Gamma(t)
\end{equation}

We now assume \(\psi_t=\thetatilde_t+\epsilon \xi_t,\ t\geq 0,\epsilon\geq 0\)
and we observe \(\Gamma(\psi_t)\) for each \(t\). Here \(\psi_t\) is a \(d\)-dimensional probing signal.

We had \begin{align*}
    \lim_{T\to\infty}\frac{1}{T}\int_0^T \xi_t dt=0,\ \lim_{T\to\infty}\frac{1}{T}\int_0^T \xi\cdot \xi^\intercal dt=Id
\end{align*}
% ALGO 
\dhighlight{Quasi sotchastic gradient descent \#1: qSGD \#1}

\textbf{Input:} \(d\times d\) pos. def. matrix G, \(\thetatilde_0\in \R^d\)

\(\frac{d}{dt}\thetatilde_t=-a_t\frac{1}{\epsilon}G\xi_t\Gamma(\psi_t)\), where \(\psi_t=\tilde{\theta}_t+\epsilon\xi_t\)
In QSA-ODE 
we have therefore \(f(\theta_t,\xi_t)=-\frac{1}{\epsilon}G\xi_t\Gamma(\theta_t+\epsilon\xi_t)\)
If\(\Gamma\in C^2\):
\begin{align*}
    \Gamma(\theta+\epsilon\xi_t)=\Gamma(\theta)+\epsilon\xi_t^\intercal \nabla\Gamma(\theta)+\frac{1}{2}\epsilon^2\xi_t^\intercal\nabla^2 \Gamma(\theta)\xi_t+o(\epsilon^2).
\end{align*}
\begin{align*}
    f(\theta,\xi_t)&=-\frac{1}{\epsilon}G\xi_t\Gamma(\theta)-G \xi_t\xi_t^\intercal\nabla\Gamma(\theta)+O(\epsilon)\\
    \underbrace{\lim_{T\to\infty}\frac{1}{T}\int_0^Tf(\theta,\xi_t)dt}_{\bar{f}_\epsilon(\theta)}&=0-G\nabla\Gamma(\theta)+O(\epsilon) 
\end{align*}


For \(G=\Id\) qSGD\#1 will approximate the steepest descent algorithm. In (QSA2) we assumed that 
\(f, \bar{f}\) are Lipschitz, but while \(\nabla\Gamma\) usually is Lipschitz, \(\Gamma\) often is not!

\dhighlight{Algorithm qSDG \#3}

For a given \(d\times d\) pos. def. matrix \(G\) and \(\thetatilde_0\in\R^d\)
\begin{align*}
    \frac{d}{dt}\thetatilde_t=-a_t\frac{1}{2\epsilon} G\xi_t\left[\Gamma(\thetatilde_{t}+\epsilon\xi_t)-\Gamma(\thetatilde_t-\epsilon\xi_t)\right]\eqcolon a_t f(\thetatilde_t,\xi_t)
\end{align*}
\(f\) can be shown to be Lipschitz in \(\theta\), whenever \(\nabla\Gamma\) is Lipschitz. 
In this case 
\begin{align*}
    f(\theta,\xi_t)=-G \xi_t\xi_t^\intercal\nabla\Gamma(\theta)+o(\epsilon),\ \lim_{T\to\infty} \int_0^T f(\theta,\xi_t)=-G\nabla\Gamma(\theta)+o(\epsilon)
\end{align*} 
\markeol{12}
\beginlecture{13}{27.05.2025}
\begin{proposition}[Global constitency]\label{prop:45}
    Suppose that the following hold for \(\Gamma\) and the 
    algorithm parameters in QSGD\#3
    \begin{enumerate}\marginnote{Control on both sides of the loss function ...}
        \item (QSA1) holds 
        \item The probing signal satisfies \[\int_0^T\xi_t\xi_t^\intercal dt=\Id\]
        \item \(\nabla\Gamma\) is globally Lipschitz continuous, and \(\Gamma\) is strongly convex with unique minimizer \(\theta^\star\in\R^d\)
        \item the corresponding QSA-ODE is ultimately bounded
    \end{enumerate}
    Then there exists \(\bar{\epsilon}>0\) s.t. for all \(\epsilon\in (0,\bar{\epsilon})\) there is a 
    unique root \(\theta_\epsilon^\star\) of \(\bar{f}_\epsilon\), satisfying \[\Vert \theta_\epsilon^\star-\theta^\star\Vert=O(\epsilon)\]
    Moreover, convergence holds from each initial condition:
    \[\lim_{t\to\infty}\theta_t=\theta_\epsilon^\star\]
\end{proposition}
\begin{proof}
    The assumptions imply that (QSA2) holds for \marginnote{Exploid \(\nabla\Gamma\) is convex}
    \begin{align*}
        f(\theta,\xi)&=-G\xi\xi^\intercal\nabla\Gamma(\theta)+O(\epsilon)\\
        \bar{f}_\epsilon(\theta)&=\lim_{T\to\infty}\frac{1}{T}\int_0^T f(\theta,\xi)dt
    \end{align*} 
    \(\Gamma\) is strongly convex, therefore there is an \(\epsilon_0>0\) s.t.
    there is a unique solution to \(G\nabla\Gamma(\theta)=z\), whenever \(\Vert z\Vert\leq \epsilon_0\).
    From this (QSA3, the asymptotic stability condition), can be established for \(\epsilon>0\) small enough.

    Theorem \ref{thm:39} yields that for each \(\epsilon>0,\theta_t\) converges to the unique root
    \(\theta_\epsilon^\star\) of \(\bar{f}_\epsilon\) satisfying \[\Vert \nabla\Gamma(\theta_\epsilon)\Vert = O(\epsilon)\]
    Ffrom there, strong convexity gives 
    \begin{align*}
        \Gamma(\theta^\star)\geq \Gamma(\theta_\epsilon^\star)+\nabla\Gamma(\theta_\epsilon^\star)^\intercal(\theta^\star-\theta_\epsilon^\star)+\frac{\eta}{2}\Vert \theta_\epsilon^\star-\theta^\star\Vert^2
    \end{align*}
    for some \(\eta>0\).
    \begin{align*}
        \frac{\eta}{2}\Vert \theta_\epsilon^\star-\theta^\star\Vert^2&\leq \underbrace{\Gamma(\theta^\star)-\Gamma(\theta_\epsilon^\star)}_{\leq 0}+\nabla\Gamma(\theta_\epsilon^\star)^\intercal(\theta^\star-\theta_\epsilon^\star)\\
        &\leq \Vert \nabla\Gamma(\theta_\epsilon^\star)\Vert \Vert \theta^\star-\theta_\epsilon^\star\Vert 
    \end{align*}
    which gives \[\Vert \theta_\epsilon^\star-\theta_\epsilon\Vert^2=O(\epsilon).\qedhere\]
\end{proof}

\begin{remark}
    For the exam: About the structure of the proof / is it long / technical / which results does it use?
\end{remark}