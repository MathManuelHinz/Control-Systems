\chapter{Value and \(Q\)-Function approximation}

\section{A very short crash course in machine learning}

\dhighlight{How can we represent functions?}

Goal: \[h(x)=\sum_{i=1}^d \theta_i \psi_i(x).\]
We could also use neural networks, or kernels: \marginnote{For more informatiom about kernels, you can look at my lecture notes for scientific computing 2 (also held by Garcke)}
\[h(x)=\sum_{i=1}^d\theta_ik(x,x_i)\]
\[K_{ij}=k(x_i,x_j)\]
is a positive (semi)-definite matrix for each dataset.

\begin{enumerate}
    \item We need a way to represent a function \(h\in \mathcal{H}\)\begin{itemize}
        \item linear
        \item neural networks 
        \item piecewise polynomials 
        \item kernels
    \end{itemize}
    \item loss \(\Gamma(h),\ \Gamma(h)=\Gamma(h(z_1),h(z_2),\dots,h(z_N))\) evaluated at some samples \(z_i,\ 1\leq i\leq N\)
    \item algorithm to obtain \(\argmin_{h\in \mathcal{H}}\Gamma(h)\)

        Training data \(\{(z_i,y_i)\}_{i=1}^N\), \(y_i=h^\star(z_i)+\epsilon_i\),
        \[\Gamma(h)=\frac{1}{N}\sum_{i=1}^N \left(y_i-h(z_i)\right)^2\]
\end{enumerate}
We usually use \dhighlight{regularization} to avoid \dhighlight{overfitting}.

\dhighlight{Always} reserve samples for evaluating the quality of the prediction. 

\section{Reinforcement Learning}
\[\mathcal{D}_{k+1}(Q^\theta)=-Q^\theta(x(k),u(k))+c(x(k),u(k))+\underbrace{\underbar{Q}^\theta(x(k+1))}_{\substack{=\min_u Q^\theta(x,u)\\ \text{or }Q^\theta(x(k+1),\phi(x(k+1)))}}\]
We have a sequence of state-action pairs 
\[\{\underbrace{x(k),u(l)}_{z_k}\mid 0\leq k\leq N\}\]
\begin{align*}
    \Gamma(h)=\frac{1}{N}\sum_{k=1}^{N}D_k(h(z_k),h(z_{k+1}))^2
\end{align*}
where \[D(h(z_k),h(z_k+1))\coloneqq -h(x(k-1),u(k-1))+c(x(k-1),u(k-1))+\underbar{h}(x(k))\]
with \(\underbar{h}(x)=\min_{u}h(x,u)\).

\begin{align*}
    Q^\theta(x,u)=\theta^\intercal \Psi(x,u),\ \theta\in\R^d
\end{align*}
and \(\Psi\) a collection of basis functions \(\psi_i\). Write 
\begin{align*}
  \gamma_k&=c(x(k),u(k))\\ 
  \tilde{\gamma}_{k+1}&=\Psi(x(k),u(k))-\Psi(x(k+1),\phi(x(k+1))).  
\end{align*}
Rewrite \(D_{k+1}(Q^\theta)\) as \marginnote{Since \(D_{k+1}(Q^\theta)\) will be small ...}
\begin{align*}
    \gamma_k=\tilde{\gamma}_{k+1}^\intercal\theta+\underbrace{D_{k+1}(Q^\theta)}_{\coloneqq \epsilon_k}
\end{align*}
This looks like a regression problem:
\begin{align*}
    \Gamma(\theta)=\lim_{N\to\infty}\frac{1}{N}\sum_{k=0}^{N-1}\left[\underbrace{\gamma_k-\tilde{\gamma}_{k+1}^\intercal\theta}_{=D_{k+1}(Q^\theta)}\right]^2
\end{align*}
Look for \(\theta^\star=\argmin_{\theta}\Gamma(\theta)\).

\subsection{Algorithm: Least Squares Temporal Difference Learning (LSTD)}
\marginnote{One of three streams in RL}
For a given \(d\times d\) regularization matrix \(W\), \(W\) psd, integer \(N\), and 
obtained samples \(\{(x(k),u(k))\mid 0\leq k\leq N\}\), the minimizer is obtained.
\begin{equation}\label{eq:theta_lstd}
    \theta_{N}^{\text{LSTD}}=\argmin_\theta \Gamma_N(\theta),\ \Gamma_N(\theta)=\theta^\intercal W \theta+\frac{1}{N}\sum_{k=0}^{N-1}\left[\gamma_k-\tilde{\gamma}_{k+1}^\intercal (\theta)\right]^2
\end{equation}
\begin{align*}
    Q^{\theta_N^\text{LSTD}}=\sum_{i=1}^d\theta_N^{\text{LSTD}}(i\psi(i))
\end{align*}
is the approximation of the \(Q\)-function.

We have a positive definite quadratic objective, so the solution to (\ref{eq:theta_lstd})
can be obtained by solving for \(\nabla\Gamma(\theta)\stackrel{!}{=}0\).
\begin{proposition}\label{prop:46}
    Define \(R_N=\frac{1}{N}\sum_{i=1}^N\tilde{\gamma}_k\tilde{\gamma}_k^\intercal,\ \bar{\psi}_N^\gamma=\frac{1}{N}\sum_{k=0}^{N-1}\tilde{\gamma}_{k+1}\gamma_k\).
    Then \(\theta_N^\text{LSTD}=\left[\frac{1}{N}W+R_N\right]^{-1}\bar{\Psi}_N^\gamma\)
\end{proposition}  % 1/n wrong? No 1/n infront of W initially
The regularization \(W\) is introduced to ensure a unique solution.
\markeol{13}
\beginlecture{14}{03.06.2025}
\begin{proposition}[Redundant Parametrization]\label{prop:47}
    Suppose that \(R_N=\frac{1}{N}\sum_{i=1}^N\tilde{\gamma}_k\tilde{\gamma}_k^\intercal\) has rank less 
    than \(d\). Then there is a non zero vector \(v\in\R^d\) for which the following two statements 
    hold for each \(0\leq k\leq N-1\):
    \begin{enumerate}
        \item[(i)] For any \(\theta\in\R^d\) and \(r\in \R\): \[D_{k+1}(Q^\theta)=D_{k+1}(Q^{\theta'}),\] where \(\theta'=\theta+rv.\)
\marginnote{(i) really is about the interplay of recorded respones and our representation and not about an identification problem in the statistical sense.}
        \item[(ii)] From the on-policy implementation \(u(k)=\psi(x(k))\) \[v^\intercal \Psi(x(0),u(0)) = v^\intercal\Psi(x(k),u(k)).\] 
    \end{enumerate} 
\end{proposition}
\begin{proof}
\(R_N\) does not have full rank, therefore there exists \(v\neq 0\) s.t. 
\begin{align*}
    0=v^\intercal R_Nv=\frac{1}{N}\sum_{i=1}^N(v^\intercal \tilde{\gamma}_k)^2.
\end{align*}        
Therefore, \(v^\intercal \tilde{\gamma}_k=0\) for every observed sample.
\begin{equation}\label{eq:proof_p47}
    0=v^\intercal \Psi(x(k),u(k))-v^\intercal \Psi(x(k+1),\phi(x(x+1))),\ 0\leq k\leq N-1
\end{equation}

So, \begin{align*}
    D_{k+1}(Q^{\theta'})&=-Q^{\theta'}(x(k),u(k))+c(x(k),u(k)) +Q^{\theta'}(x(k+1),\phi(x(k+1)))\\
    &=c(x(k),u(k))+\left[\theta+rv\right]\left[-\Psi(x(k),u(k))+\Psi(x(k+1),\phi(x(k)))\right]\\
    &\stackrel{\ref{eq:proof_p47}}{=}c(x(k),u(k))+\theta \left[-\Psi(x(k),u(k))+\Psi(x(k+1),\phi(x(k)))\right],
\end{align*}
which yields (i).

If \(u(k)=\phi(x(k))\), use (\ref{eq:proof_p47})
\begin{align*}
    v^\intercal \Psi(x(k),u(k))=v^\intercal\Psi(x(k+1),u(k+1))
\end{align*}
repeated use for every \(k\) gives (ii).

\end{proof}

To avoid the convergence of the \(\Gamma(\theta)\to 0\) for long trajectories,
one can do restarts.

\subsection{Algorithms: LSTD-Learning with restarts}
For a given \(d\times d\) matrix \(W>0\), integers \(N,M\), and 
observed samples \[\left\{x^i(k),u^i(k)\mid 0\leq k\leq N,1\leq i\leq M\right\}\]
with user defined initial conditions \[\{x^i(0)\mid 1\leq i\leq M\}\]
and with action \marginnote{It is fine not to probe at all} \[u^i(k)=\tilde{\phi}(x^i(k),\xi^i(k))\]
the approximation \(Q^{\theta_N^{\text{LSTD}}}=\Psi^\intercal\theta_N^{\text{LSTD}}\)
is obtained. Here 
\[\theta_N^\text{LSTD}=\argmin_\theta \Gamma_N^i(\theta),\ \Gamma_N(\theta)=\frac{1}{M}\sum_{i=1}^M\Gamma_N^i\]
and 
\[\Gamma_N^i(\theta)=\theta^\intercal W \theta +\sum_{i=1}^{N-1}\left[\gamma_k^i-\xi\tilde{\gamma}_{k+1}^{i\intercal} \theta\right]\] % TODO: Check

\begin{remark} % TODO: Check
    The LSTD algorithm can be formulated as a recursive algorithm 
    \[\theta_{N+1}=\theta_N+G_N\tilde{\gamma}_{N+1}(\gamma_N-\tilde{\gamma}_{N+1}\theta_N)\]
    where \begin{align*}
        G_{N+1}&=G_N-\frac{1}{K_{N+1}}G_N\tilde{\gamma}_{N+1}\tilde{\gamma}_{N+1}G_N\\
        K_{N+1}&=1+\tilde{\gamma}_{N+1}^\intercal G_N \tilde{\gamma}_{N+1}
    \end{align*}
\end{remark}

\subsection{Galerkin relaxation}

Basis \(\{\psi_i\},h^\theta(z)=\sum_{i=1}^d \theta_i\psi_i(z)\), we want \(0\stackrel{!}{=}\nabla_\theta \Gamma(h^\theta)\).

For Bellman error 
\begin{align*}
    0&=\frac{1}{N}\sum_{k=1}^ND_k(h^\theta(z_k),h^\theta(z_{k+1}))\zeta^\theta(k)\\
    \zeta^\theta(k)&=\nabla_\theta D_k(h^\theta(z_k),h^\theta(z_{k+1}))
\end{align*}
Alternative is so-called Galerkin-relaxation, We construct a sequence \(\{\zeta(k)\},\ \zeta(k)\in \R^{d_\zeta}\)\marginnote{constraints}
\begin{align*}
    0&=\frac{1}{N}\sum_{k=1}^ND_{k}(h^\theta(z_k),h^\theta(z_{k+1}))\zeta_i(k)\ 1\leq i\leq d_\zeta
\end{align*}
We relax \(D_k(h^{\theta}(z_k),h^\theta(z_{k+1}))=0\ \forall k\)
\begin{center}
    \(\{\zeta(k)\}\) are called eligibility vectors in RL. 
\end{center}
\marginnote{One can introduce them in at least one other way ...}

\(\zeta(k)\) does not depend on \(\theta\), \(\zeta(k)\neq \zeta^\theta(k)\),
\textit{maybe} \(\zeta(k)\approx \zeta^\theta(k),\theta\in\) \textit{region of interest}.
It can make sense to have \(d_\zeta=d,\) if \(\theta\in \R^d\).

\section{Projected Bellman equation}

Consider \(h^\star=T(h^\star)\). \marginnote{Motivated by the solution of the Bellman equation}

Reminder \(Q^n(x,u)=c(x,u)+Q^n(x^+,u^+)\), where \(x^+=F(x,u),\ u^+=\phi(x^+)\). In our notation \(Q^\theta(x,u)\):

\begin{align*}
    T(h)\restrict{(x,u)}=c(x,u)+h(x^+,u^+),
\end{align*}

so \(Q^\theta=T\left(Q^\theta\right)\). Consider an approximation in a function class \(\mathcal{H}\).
\begin{equation}\label{eq:bellman_projection}
    \hat{h}=\hat{T}(\hat{h})=P_{\mathcal{H}}(T(\hat{h}))
\end{equation}

with \(P_\mathcal{H}(h)\in\mathcal{H}\) for \(h\in \mathcal{H}\).

Or, consider a second function class \(\mathcal{G}\) and solve 
for \(\hat{h}\in \mathcal{H}:\)
\begin{equation}\label{eq:bellman_projection2}
    0=P_\mathcal{G}(\hat{h}-T(\hat{h}))
\end{equation}

\begin{proposition}\label{prop:48}
    Suppose that the following hold
    \begin{enumerate}
        \item[(i)] \(\mathcal{H}=G\)
        \item[(ii)] \(\mathcal{H}\) is a linear function class, i.e. \(a_1h_1+a_2h_2\in\mathcal{H}\) for \(h_1,h_2\in\mathcal{H},\ a_1,a_2\in\R\) 
        \item[(iii)] The mapping \(P_\mathcal{H}\) is linear. For  \(h_1,h_2\in \cH,\ a_1,_2\in \R\): \[P_\cH(a_1h_1+a_2h_2)=a_1P_\cH(h_1)+a_2P_\cH(h_2)\]
    \end{enumerate}
    Then the solution to (\ref{eq:bellman_projection}) and (\ref{eq:bellman_projection2}) coincide.
\end{proposition}
\markeol{14}
\beginlecture{15}{05.06.2025}

\begin{proof}
    Trivial
\end{proof}

We assume for \(g\in G\): \(g:Z\to\R\), and \(G\) is a linear function class. We further assume 
there is a state-process \(\Phi\) on \(Z\), where \((x(k),u(k),\xi(k))=w(\Phi(k))\), where \(w\)
is Lipschitz. We define for a probability measure \(\omega\) with density \(\rho\)
\[\langle h_1,h_2\rangle_\omega=\bE_\omega(h_1(\Phi),h_2(\Phi))=\int_Z h_1(z)h_2(z)\rho(z)dz\] 
\[\Vert h\Vert_\omega=\sqrt{\langle h,h\rangle_\omega}.\]

\[L_2(\omega)=\{h\mid \Vert h\Vert_\omega<\infty\}.\] %TODO: FIX

For any \(h\in L_2(\omega)\), we define projection onto \(G\) as
\[\hat{h}=P_G(h)=\argmin_{g\in G}\{\Vert g-h\Vert_\omega\}.\]
For \(\hat{h}\in G\)
\[\langle h-\hat{h},g\rangle_\omega=0,\ g\in G\]
In particular, we assume that \(G\) has finite dimension. We choose \(d\) functions 
\[\{\zeta_i\mid 1\leq i\leq d\}\] \marginnote{We do not assume that \(d\) is the dimension of \(G\) in general}
stack them to get \(\zeta:Z\to\R^d\) and define \(G\{g=\theta^\intercal\zeta\mid \theta\in\R^d\}\).
\(\zeta(k)\coloneqq \zeta(\Phi(k))\) is the sequence of \dhighlight{eligibility vectors}.

\begin{proposition}\label{prop:49}
    Suppose that \(\zeta_i\in L_2(\omega)\) for each \(i\) and that the functions are linear independent in \(L_2^\omega\).
    That is \(\Vert \zeta^\intercal\zeta\Vert_\omega=0\). For each \(h\in L_2(\omega)\), the projection exists, is unique,
    and given by \[\hat{h}=(\omega^\star)^\intercal\zeta\]
    with  \(\theta^\star=[R^\zeta]^{-1}\bar{\psi}^h,\) \(\bar{\psi}^h\in \R^d,\bar{\psi}_i^h=\langle\zeta_i,h\rangle_\omega\)
    \(\R^zeta\in\R^{d\times d},R_{ij}^\zeta=\langle \zeta_i,\zeta_j\rangle_\omega\).
\end{proposition}

\begin{proof}[Sketch]
    The orthogonality principle gives 
    \begin{align*}
        \langle h-\hat{h},\zeta_i\rangle_\omega&=0
    \end{align*}
    we use this identity with \(\hat{h}=(\theta^\star)^\intercal \zeta\)
\end{proof}

\begin{proposition}\label{prop:50}
    \(0=P_G(\hat{h}-T(\hat{h}))\) holds if and only if 
    \[0=\langle \zeta_i,\hat{h}-T(\hat{h})\rangle_\omega 1\leq i\leq d.\]
    This is the \dhighlight{Galerkin relaxation} of \(h^\star=T(h^\star)\) in the \(L_2(\omega)\) setting.\marginnote{We saw this last time as well}
\end{proposition}

Consider \(\cH=\{h=\theta^\intercal \psi\mid \theta\in\R^d\},\) where \(\psi:X\times U\to \R\) \marginnote{\(\psi\) like a \(Q\)-function}.
Now, we use the above on the Bellman operator.
\begin{align*}
    0=\bE(\zeta_i(k)(\hat{h}(x(k),u(k))-[c(x(k),u(k))+\hat{h}(x(k+1),\phi(x(k+1)))]))
\end{align*}

Solutions of this root finding problem define \(Q^{\theta^\star}\in \cH\).

Recall \(D_{k+1}\), we can write equivalently
\begin{align*}
    0=\bE(\zeta(k)D_{k+1}(Q^{\theta}))\restrict{\theta=\theta^\star}.
\end{align*}

Given \(N\) observations, we approximate this by \marginnote{More concrete Galerkin estimation}
\[0=\frac{1}{N}\sum_{k=0}^{N-1}\zeta(k)D_{k+1}(Q^\theta)\restrict{\theta=\theta^\star}.\]

\subsection{Algorithm: TD\((\lambda)\)}\marginnote{This was introduced differently, maybe we will also see this later. In all there are three views}
% FIX phi and psi
Notation: \(\psi_{(k)}=\psi(x(k),u(k))\), \(c(k)=c(x(k),u(k))\), \(\zeta_k=\zeta(k)\)

For a given \(\lambda\in [0,1]\), nonnegative step size sequence \(\{\alpha_n\}\),
initial conditions \(\theta_0,\zeta_0\) and observed samples \(\{x(k),u(k)\mid 0\leq k\leq N\}\),
the sequence of estimates is defined by three coupled equations
\begin{align*}
    \theta_{n+1}&=\theta_n+\alpha_{n+1}D_{n+1}\zeta_n\\
    D_{n+1}     &= -Q^{\theta_n}(x(n),u(n))+c_n-Q^{\theta_n}(x(n+1),\phi(x(n+1)))\\
    \zeta_{n+1} &= \lambda \zeta_{n}+\psi_{(n+1)}
\end{align*}

This defines the approximation of the \(Q\)-function \(Q^{\theta_n}=\sum_{i=1}^d(\theta_N)_i\phi_i\).

We extend the state process 
\[\Phi(k)=(x(k),u(k),\xi(k),\zeta(k)).\]
This means that \(\zeta(k)\) is a linear function of the state process \(\Phi(k)\).

Denote \(\bar{f}_\lambda(\theta)=\bE_\omega\left[\zeta(k)D_{k+1}(Q^\theta)\right]\). TD\((\lambda)\)
is an approximation of the ODE 
\begin{eqnarray}\label{eq:tdlambda_ode}
    \frac{d}{dt}\vartheta=\bar{f}_\lambda(\vartheta)
\end{eqnarray}

\(\bar{f}_\lambda(\theta)=A(\theta-\theta^\star)\), where 
\(A=\bE_\omega\left[\zeta(k)\left[-\psi_{(k)}+\psi(x(k+1),\phi(k+1))\right]^\intercal\right]\).

For linear systems QSV-assumptions can be shown if all eigenvalues of the systemmatrix have 
strictly negative real parts,  i.e. \(A\) is Hurwitz.

This can be shown for the on-policy approach, so the algorithm converges. There is a counter example in the book
if we are off-policy., so convergence of TD\((\lambda)\) is not guaranteed in the off-policy setting.

\subsection{Algorithm TD\((\lambda)\)-learning with nonlinear function approximation}

In the setup as before in TD\((\lambda)\), \(\theta_{n+1},D_{n+1}\) are 
as for the linear case.
\begin{align*}
    \zeta_{n+1}&=\lambda\zeta_{n}+\zeta_{n+1}^0\\
    \zeta_{n+1}^0&=\nabla_\theta Q^\theta(x(n),u(n))\restrict{\theta=\theta_n}  
\end{align*}

Observe that \(\zeta_n^0=\Psi_{(n)}\) for a linear function class, so this is a consistent 
generalization.
\marginnote{\(\lambda=0\) means we don't have a history at all! TD\(\lambda\) is for a fixed policy}

To extend, we use instead of the so far fixed policy \(\phi\)
\[\phi_{(n)}^\theta=\argmin_{u} Q^\theta(x,u)\]
\subsection{Algorithm: \(Q\)-learning}
The change in comparison to TD\((\lambda)\) is  
\[D_{n+1}(Q^{\theta_n})=-Q^{\theta_n}(x(n),u(n))+c(k)-Q^{\theta_n}(x(n+1),\phi^{\theta_{n+1}})\]
A limit \(\theta^\star\) will save \(\bar{f}(\theta^\star)=0\) with 
\[\bar{f}(\theta) = \bE_\omega\left[\zeta(k),D_{k+1}(Q^\theta)\right].\]

At first glance this looks as for TD\((\lambda)\), but the last term of the update to \(D_{n+1}\)
is different! For \(\lambda=0\), we can apply proposition \ref{prop:48} to conclude that 
\(Q^{\theta^\star}\) solves 
\[Q^{\theta^\star}=P_\cH(T(Q^{\theta^\star})),\]
where \(T(Q)\restrict{(x,u)}=c(x,u)+\min_{u^+}Q(x^+,u^+)\) and \(x^+=F(x,u)\).

Theory for existence of a solution or stability (in the sense of global asymptotic stability)
is so far lacking in the context of ODE analysis.

\markeol{15}
\section{Deep Q-Networks and Batch methods}
\beginlecture{16}{17.06.2025}
Instead of the purely recursive form going over all 
\(N\), we break this into batches  \(T_0=0<T_1<T_2<T_B=N\)

\subsection{Algorithm: DQN}
With \(\theta_0\in \R^d\) given, and a sequence of positive scalars \(\{\alpha_n\}\) we Define
\begin{eqnarray}\label{eq:DQN_batch}
    \theta_{n+1}=\argmin_{\theta}\Gamma_n^\epsilon(\theta)+\frac{1}{\alpha_{n+1}}\Vert \theta-\theta_n\Vert^2,\\ 0\leq n\leq B-1
\end{eqnarray}
where for each \(n,\ r_n=T_{n+1}-T_n\)
\[\Gamma_n^\epsilon(\theta)=\frac{1}{2}\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}}\left[-Q^\theta(x(k),u(k))+c_k+\underbar{Q}^{\theta_n}(x(k+1))\right]^2\]
where \(\underbar{Q}^{\theta_n}(x)\coloneqq Q^{\theta_n}(x,\phi^{\theta_n}(x))\).

We collect some natural properties which hold for linear and nonlinear scenarios.
% TODO : FIx all sums
\begin{proposition}\label{prop:51}
    Suppose that \(\{Q^\theta(x,u)\mid \theta\in \R^d\}\) is continuously differentiable in \(\theta\)
    for each \(x,u\). Then 
    \begin{enumerate}
        \item The solution to \ref{eq:DQN_batch} solves thef fixed point equation \[\theta_{n+1}=\theta_n+\alpha_{n+1}\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}}\left[-Q^\theta(x(k),u(k))+\gamma_n(k)\right]+\nabla_\theta Q^\theta(x(k),u(k))\restrict{\theta=\theta_{n+1}}\] with \(\gamma_n(k)=c_k+\underbar{Q}^{\theta_n}(x(k+1))\)
        \item if the parametrization is linear, so that \[\nabla_\theta Q^\theta(x(k),u(k))=\Psi_{(k)},\] then \[\theta_{n+1}=\theta_n+\alpha_{n+1}\left[A_n\theta_{n+1}-b_n\right]\] with \(A_n=-\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}-1}\Psi_{(k)}\Psi_{(k)}^\intercal,\ b_n=-\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}-1}\gamma_n(k)\Psi_{(k)}\). 
    \end{enumerate}
\end{proposition}

In this can we can rearrange and invert \[\theta_{n+1}=\left[I-\alpha_{n+1}A_n\right]^{-1}(\theta_n-\alpha_{n+1}b_n).\]
For \(\alpha\) small enough, we can observe that \[\left[I-\alpha_{n+1}A_n\right]^{-1}\approx I+\alpha_{n+1}A_n\]
which gives
\begin{align*}
    \theta_{n+1}&\approx [I+\alpha_{n+1}A_n](\theta_n-\alpha_{n+1}b_n)\\
                &\approx \theta_n+\alpha_{n+1}(A_n\theta_n-b_n)
\end{align*}
Similarly, we aim for  an approximation in the nonlinear case. For \(Q^\theta\in C^1\), we have 
\(\Vert \theta_{n+1}-\theta_n\Vert\leq K \alpha_{n+1}\) for some  fixed \(K<\infty\), whenever \(\{\theta_n\}\)
is bounded. Consequently, \[\theta_{n+1}=\theta_n+\alpha_{n+1}\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}-1}\left[-Q^\theta(x(k)u(k))+\gamma_n(k)+\epsilon_{n+1}\right]+\nabla_\theta Q^{\theta_n}(x(k),u(k)),\]
where \(\Vert \epsilon_{n+1}\Vert =O(\alpha_{n+1})\).

\subsection{Algorithm: Batch \(Q(0)\) learning }
%TODO not + grad, but times ...
With \(\theta_0\in \R^d\) given, along with \(\{\alpha_n\}\), \(\alpha_n>0\) define recursively:
\begin{align*}
    \theta_{n+1}&=\theta_n+\alpha_{n+1}\frac{1}{r_n}\sum_{k=T_n}^{T_{n+1}-1} D_{k+1}(\theta_n)\nabla_\theta Q^{\theta_n}(x(k),u(k))\\
    D_{n+1}(\theta_n)&=-Q^{\theta_n}(x(k),u(k))+c_k-\underbar{Q}^{\theta_n}(x(k+1))
\end{align*}

\begin{proposition}\label{prop:52}
    Consider the DQN algorithm with a possibly nonlinear function approximation. Assuming \(Q^\theta\in C^1\) and that its gradient is 
    Lipschitz globally with constant independent of \((x,u)\). Suppose that \(B=\infty\), that the nonnegative \(\{\alpha_n\}\)
    satisfy \(\sum \alpha_n=\infty,\sum \alpha_n^2<\infty\) and suppose that the \(\{\theta_n\}\) obtained by our algorithm
    converge to a \(\theta_\infty\in \R^d\). 
    \begin{enumerate}
        \item \(\bar{f}(\theta_\infty)=0\) with \(\bar{f}\) as before: \[\bar{f}=\bE_\omega\left[\zeta(k)D_{k+1}(\theta)\right]\] and \(\zeta(n)=\nabla_\theta Q^\theta(x(k),u(k))\restrict{\theta=\theta_n}\)
        \item The algorithm admits the ODE approximation \[\frac{d}{dt}\vartheta_t = \bar{f}(\vartheta_t).\]
    \end{enumerate}  
\end{proposition}

Note, the states if we have convergence, then the behavior is consistent with the ODE view. 
Generally, we do not know if \(\bar{f}\) as defined above has a root.  Even if we would know,
the existence of \(bar{f}(\theta_\infty)=0\) does not tell us if the ODE  is stable,
nor if \(\theta_\infty\) has desirable properties.

\subsection{G\(Q(\lambda)\)-Learning} \marginnote{The G prob. stands for generalized}

Instead of aiming for \(\bar{f}(\theta^\star)=0\), aim to 
minimize \[\min_\theta\Gamma(\theta)=\min_\theta\frac{1}{2}\bar{f}^\intercal M \bar{f}(\theta)\]
for some \(d\times d\) matrix \(M\) spd.
\begin{align*}
    \frac{d}{dt}\vartheta_t&=-\left[\partial_\theta \bar{f}(\vartheta_t)^\intercal M \bar{f}(\vartheta_t)\right]
\end{align*}
choosing \(M=\bE\left[\zeta_n\zeta_n^\intercal\right]^{-1}\), one can derive the 
\dhighlight{D\(Q(\lambda)\)-Learning} algorithm. 

To avoid matrix inversion, one can use a two-time scale approach: 

Obtain first an ODE approximation of \(M \bar{f}\vartheta_t\) using 
\[\frac{d}{dt}w_t=b_t\left[\bar{f}(\vartheta)-R w_t\right]\] \marginnote{This \(b\) is not the same as earlier, here it is a scalar}
where \(R=M^{-1}\). 

Provided \(\{b_t\}\) chosen very large, and \(\vartheta_t\)  is bounded, one can derive that \(w_t\approx M\bar{f}(\vartheta_t)\) after some \(t\). 

\subsection{Algorithm: G\(Q(\lambda)\) Learning for linear function approximation}
With the same starting point of \(Q(\lambda)\) and an additional initialization
\(w_0\), we iterate:
\begin{align*}
    \theta_{n+1}&=\theta_n-\alpha_{n+1}A_{n+1}^\intercal w_n\\
    w_{n+1} &=w_n+b_{n+1}(f_{n+1}(\theta_n)-\zeta_{n+1}\zeta_{n+1}^\intercal w_n)\\
    \zeta_{n+1}&=\lambda \zeta_n+\Psi_{(n+1)}\\
    D_{n+1}&=-Q^{\theta_n}(x(n),u(n))+c_n-\underbar{Q}^{\theta_n}(x(k+1))\\
    f_{n+1}(\theta_n)&=D_{n+1}\zeta_{n+1},\ A_{n+1}=\partial_\theta f_{n+1}(\theta_n)=\zeta_n(-\Psi_{n}\bar{\Psi}_{n+1})^\intercal\\
    \Psi_{(n+1)}&=\Psi(x(n+1),u(n+1)),\ \bar{\Psi}_{(n+1)}=\Psi(x(n+1),\phi^{\theta_n}(x(n+1)))
\end{align*}

The approximation is successful if \[\lim_{n\to\infty} \frac{b_n}{\alpha_n}=\infty\]

\dhighlight{Problems}
\begin{itemize}
    \item \(\Gamma\) is not convex, so difficult to get global minima
    \item Even if \(\bar{f}(\theta^\star)=0\) does have a solution, there are numerical challenges. Consider 
    \begin{align*}
        \Gamma(\theta)&=\Gamma(\theta^\star)+\underbrace{0}_{\bar{f}(\theta^\star)=0}+(\theta-\theta^\star)\left[A^\star MA^{\star\intercal}\right](\theta-\theta^\star)
    \end{align*}
    if \(A^\star\) has a large condition number the observed condition number is squared, so even worse. Maybe \(M\) can be chosen to 
    avoid this.
    \item It is not obvious why minimizing \(\Gamma(\theta)\) is a reasonable goal
\end{itemize}
\markeol{16}
\beginlecture{17}{24.06.2025}

\section{Summary}

To summarize, inside TD taxonomy, we have seen 
\begin{enumerate}
    \item[(i)] approximate PIA using LSTD or TD\((\lambda)\). We can be sure it converges under two conditions: \begin{enumerate}
        \item linearity: the function class is linear 
        \item the function class is complete, in the sense that we have \(Q^{\theta_n}=Q^{\psi^n}\) for each \(n\)  
    \end{enumerate}
    \item[(ii)] Galerkin relaxations of the dynamic programming (DP) equation are obtained using \(Q(\lambda)\)-learning, DQN or batch \(Q(\lambda)\)-learning. There theory is almost nonexistent
    \item[(iii)] Generalized \(Q\)-learning, to obtain the minimal mean square Bellman error. 
          We are assured success, if \(Q^\star\) lies inside our function class and the 
          objective satisfies conditions aligned with gradient descent, e.g. the PL condition from the earlier chapter 
\end{enumerate}

\marginnote{Next couple of lecture also have parts from other books}

\section{Exploration}

We assume \(u(k)=\breve{\psi}(x(k),\xi(k))\), where \(\underline{\xi}\) is a 
bounded sequence on a set \(\Omega\subset \R^p\) for some \(p>1\)

We assume an autonomous state space model for \(\xi\) 
\[\xi(k+1)=H(\xi(k)),\ H \text{ continuous}.\].
\(\Phi(k)\coloneqq(x(k),u(k),\xi(k))\) has an analogous form in the state space \(Z\).

Remember (QSA2), ergodic limit, \(Z\), average of observations. Denote for 
\(g:Z\to\R,\ g\) continuous, \(N\geq 1\)
\[\bar{g}_N=\frac{1}{N}\sum_{k=1}^N g(\Phi(k)).\]
We will assume the existence of 
\begin{equation}\label{eq:ergodic_mean}
    E_\omega\left[g(\Phi)\right]\coloneqq \lim_{N\to\infty} \bar{g}_N.
\end{equation}
Often, we have \(\omega\) as a probability measure with density \(\rho\), s.t. 
\[E_\omega(g(\Phi))=\int_Z g(z)\rho(z)dz \]

\begin{lemma}\label{lem:53}
Consider the probing signal \(\xi(k)=\sin(2\pi k/T),\ k\geq 0\), provided that \(T\) is 
an irrational number, for any continuous function \(g:\R\to\R\) we have 
\[\lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^Ng(\xi(k))=\int_0^1g(\sin(2\pi k)dt)=\int_{-1}^1g (t)\rho(t)dt\]
where \(\rho(t)=\left[\pi\sqrt{1-t^2}\right]^{-1}\) is known as the arcsine density.    
\end{lemma}

\begin{proof}
    Consider \(\xi^0(k)=[k/T]_1=k/T-\lfloor k/T\rfloor\), which is the fractional part of \(k/T\).
    \(\xi^0(k)\) samples uniformly in \([0,1]\), for continuous functions \(h:\R\to\R\) it then holds 
    \begin{align*}
        \frac{1}{N}\sum_{k=1}^Nh(\xi^0(k))=\int_0^1 h(r)dr
    \end{align*}
    with \(h(\xi^0(k))=g(\sin(2\pi \xi^0(k)))=g(\xi(k))\) the first equality follows, the second equality is 
    standard calculus.
\end{proof}

\dhighlight{Assumption A\(\xi\):}

The state and action spaces are each subsets of Euclidean space \(F:X\times U\to X\), describing \(x(k+1)=F(x(k),u(k))\) 
\(\breve{\phi},H\) from above are continuous. The state process \(\Phi\) has the following properties 
\begin{enumerate}
    \item \(\Phi\) evolves on a closed subset of Euclidean space, denoted \(Z\), and \((x(k),u(k),\xi(k))=w(\Phi(k))\) 
           for each \(k\), where \(w:Z\to X\times U\times \Omega\) is Lipschitz
    \item there is a probability measure \(\omega\), s.t. for any continuous function \(g:Z\to\R\) the ergodic mean (\ref{eq:ergodic_mean}) exists for each initial condition 
    \item the limit (\ref{eq:ergodic_mean}) is uniform on\[G_L\coloneqq \{g\mid\Vert g(z')-g(z)\Vert \leq L\Vert z-z'\Vert,\ \forall z,z'\in Z\}\] for each \(L<\infty\).
        \[\lim_{N\to\infty}\sup_{g\in G_L}|\bar{g}_N-E_\omega[g(\Phi)]|=0\]
\end{enumerate}

\section{ODE approximation}

Consider a recursion \begin{equation}\label{eq:recursion}\theta_{n+1}=\theta_n+\alpha_{n+1}f_{n+1}(\theta_n)\end{equation}, here \(\{f_n\}\) is a 
sequence of functions that admit an ergodic limit 
\[\bar{f}(\theta)\coloneqq \lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^N f_k(\theta),\ \theta\in\R^d\]
We associate an ODE 
 \begin{equation}\label{eq:ode_approx}
    \frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)
\end{equation}

We can use the Euler scheme, \(\tau_0,\tau_n=\sum_{k=1}^n\alpha_k\), for \(n\geq 1\).
To get a continuous time process, we set \(\hat{\theta}_{\tau_n}=\theta_n\) and 
extend by piecewise linear interpolation. Let \(\{\vartheta_t^n\mid t\geq \tau_n\}\) denote the solution to 
(\ref{eq:ode_approx}) with starting condition \(\vartheta_{\tau_n}^n=\theta_n\).

The recursion (\ref{eq:recursion}) is said to admit an ODE approximation, if the error
\[\lim_{n\to\infty}\sup_{\tau_n\leq \tau\leq \tau_n+N}\Vert \hat{\theta}_\tau-\vartheta_\tau^n\Vert=0\]
If \(\{\theta_n\}\) is bounded, 
convergence can be shown similar to proposition \ref{prop:38}, which allows 
to use the ideas behind proposition \ref{thm:39}%TODO
to establish convergence if (\ref{eq:ode_approx})
is globally asymptotically stable.

\section{Convergence rates}

The rate of convergence is \(1/t^{\rho_0}\) if 
\[\limsup_{t\to\infty} t^\rho\Vert \tilde{\theta}_t\Vert = \begin{cases}
    \infty  & \rho>\rho_0\\ 
    0 & \rho<\rho_0
\end{cases}\]
where \(\tilde{\theta}_t=\theta_t-\theta^\star\). In our context, one can achieve \(\rho_0=1\),
which is optimal in most cases.

Generally, there is an influence of the gain \(\alpha\) on the convergence. Consider, a standard 
choice \(a_t=g/(1+t)^\rho\), where \(g>0,\ 0< \rho\leq 1\) are fixed.

The time scaling \(\tau=s_t\coloneqq \int_0^t a_rdr\) results in 
\begin{equation}\label{eq:tau_scaling}\tau=\begin{cases}
    g\log(1+t) & \rho=1\\
    g\frac{1}{1-\rho}(1+t)^{1-\rho} & 0<\rho < 1
\end{cases}\end{equation} 

\(\frac{d}{dt}\vartheta_t=\bar{f}(\vartheta_t)\) and assume exponential asymptotically: 

there exists \(\rho_0>0\), \(B_0<\infty\) s.t. for any solution to the ODE and any \(t\geq 0\)
\[\Vert \vartheta_t-\theta^\star\Vert\leq B_0\Vert \vartheta_0-\theta^\star\Vert \exp(-\rho_0t).\]
Remember from  lemma \ref{lem:35} \(\frac{d}{dt}\bar{\theta}_t=a_t\bar{f}(\bar{\theta}_t)\) that 
\(\theta_t=\vartheta_\tau,t\geq t_0\).

So that \(\Vert \vartheta_\tau-\theta^\star\Vert=\Vert \bar{\theta}_t-\theta^\star\Vert\). One can 
see two different aspects.

\(\rho<1\): \(\{\bar{\theta}_t\}\) converges to \(\theta^\star\) very quickly. But, 
the boundedness of \(\frac{1}{a_t}(\theta_t-\bar{\theta}_t)\) implies a suboptimal rate 
\[\Vert \theta_t-\bar{\theta}_t\Vert\leq B\frac{1}{(1+t)^\rho},\]
where \(B\) is a function of the initial condition \(\theta_0\).

\(\rho=1\) the above bound is ideal, but with (\ref{eq:tau_scaling}) we can observe 
\[\Vert \bar{\theta}_t-\theta^\star\Vert \leq B_0\Vert \bar{\theta}_0-\theta^\star\Vert \frac{1}{(1+t)^{g\rho_0}}.\]
So the rate of convergence of \(\{\bar{\theta}_t\}\) depends on \(g\). For the optimal one \(1/t\), one needs 
\(g\geq \frac{1}{\rho_0}\).

So, \(g\) can be large, which can lead to large transients/ vector fields. By averaging
techniques: \(\theta_T^{PR}\coloneqq \frac{1}{T-T_0}\int_{T_0}^T \theta_tdt\).
One can achieve the optimal rate of \(1\) overall.

F.s. \(T_0=T-T/5,\) averages over last 20\%.

\markeol{17}

\beginlecture{18}{26.06.2025}

This lecture is mostly based on \cite{BartoSutton1998}


\section{Examples of Off-policy divergence}

\begin{tikzcd}
    &\theta\arrow[r] & 2\theta\\
    \text{values}\arrow[ru]&\text{feature 1}\arrow[u] & \text{feature 2}\arrow[u] 
\end{tikzcd}

two states, whose estimated values are \(\theta,2\theta,\theta\in \R^n\). Feature vectors 
are \(1,2\). In state 1, only action is going to state 2 with cost 0. We consider discounted 
\[\sum_{k=1}^\infty\gamma^kc(x(k),(k))\]
and TD(0). Assume that \(\theta:0=10,\ \gamma\approx 1\). 
\begin{align*}
    \theta_{n+1}&=\theta_n+\alpha D_{n+1}\Psi_{(n+1)}\\
    D_{n+1}&=-\underbrace{Q^{\theta_n}(x(k),u(k))}_{10}+0+\underbrace{\gamma Q^{\theta_n}(x(k),\phi)}_{20}
\end{align*}
If \(\alpha=0.1,\ \theta_1\approx 11\)., do it once more to get \(\theta_2\approx 12.1\)
\begin{align*}
    D_{n+1}(Q)&=-\theta_n+0+\gamma 2 \theta_n=(2\gamma-1)\theta_n\\
    \theta_{n+1}&=\theta_n+\alpha(2\gamma-1)\theta_n\cdot 1=\underbrace{(1+\alpha(2\gamma-1))}_{>1 \text{ if }\gamma>0.5}\theta_n
\end{align*}

In the off-policy training, we do not follow the currently best action, whatever it may be.
In off-policy training, one usually uses \dhighlight{importance sampling} or \dhighlight{reweighting}
between target and behavior policy.

the update becomes 
\[\theta_{n+1}=\theta_n+\alpha\delta_n D_{n+1}\Psi_{(n+1)}\]
with \[\delta_n=\bP\left[\frac{\text{target policy takes u at x(n)}}{\text{behavior policy takes u at x(n)}}\right]\]

So \(\delta_n=0\) if \(\phi^b\) takes something, which \(\phi^t\) never would.

\subsection{Baird's counter examples} 
% FIG 11.1 add maybe?

Cost is 0 on all transitions, so the true value function is constant \(0\),
which can be achieved by \(\theta=0\), but it is not unique.\marginnote{due to our parametrization}

\begin{align*}
    \theta\in\R^n & \Psi(1)=\begin{pmatrix}
        2\\0\\\vdots\\0\\1
    \end{pmatrix}&\dots
\end{align*}
vectors linearly independent.

Consider exemplarily the solid transitions 
\begin{align*}
    D_{n+1}&=\begin{cases}
        -\left[\theta_n^8+2\theta_n^k\right]+\gamma\left[2\theta_n^8+\theta_n^7\right] & x(n)=k\leq 6\\
        -\left[2\theta_n^8+2\theta_n^7\right]+\gamma\left[2\theta_n^8+\theta_n^7\right] & x(n)=7
    \end{cases}\\
    \theta_{n+1}^8&=\theta_n^8+\begin{cases}
        \alpha[(2\gamma-1)\theta_n^8+\gamma\theta_n^7-2\theta_n^k] & x(n)=k\leq 6 \\ 
        \alpha[-(1-\gamma)[2\theta_n^8+\theta_n^7-]] & x(n)=7 
    \end{cases}
\end{align*}
Insert \(\frac{1}{7}\) for importance sampling.

6 of 7 times, \((2\gamma-1)\) for \(\gamma>0.5\) amplifies \(\theta_n^8\). TD\((\lambda)\) does 
not change the behavior. 

A DP-like algorithm with gradient updates and averaging, or expectation, over all 
states does not change the behavior. 

Similar counterexamples exist  for \(Q\)-learning. Generally, a behavior policy \textit{close enough}
to the target does not result in divergence, but so far there is no theory. 

\subsection{Tsitsiklis and Van Roy's counter example}


\begin{tikzcd}
    \theta\arrow[r] & 2\theta \arrow[dr,"\epsilon"]\arrow[loop right, "1-\epsilon"]&\\
    &&\substack{\square\\\text{terminal state}} 
\end{tikzcd}

\begin{align*}
    \theta_{n+1}&=\argmin_{\theta\in\R}\sum_{x\in X}\left(V^\theta(x)-\bE(0+\gamma V^\theta(x')\mid x'=F(x))\right)^2
\end{align*}
So \(\theta_{n+1}\) minimizes the MSE at each step between the approximation and the expected return.

\begin{align*}
    \theta_{n+1}&=\argmin_{\theta\in \R} (\theta-\gamma 2\theta_n)^2+(2\theta-(1-\epsilon)\gamma 2 \theta_n)^2  \\
    &=\frac{6-4\epsilon}{5}\gamma\theta_n
\end{align*}

When \(\gamma>\frac{5}{6-4-\epsilon}\) and \(\theta_0\neq 0\) the sequence diverges.

\subsection{The deadly triad}\marginnote{Non-Bootstrap methods would not have these problems?}

\begin{enumerate}
    \item Function approximation: scalable way to work with a state space much larger then the memory and compute resources 
    \item Bootstrapping: Updates that are based on current estimates, as in TD or DP methods. One could alternatively use so called Monte Carlo methods, which use only actual rewards and compute returns  
    \item Off-policy learning: Training on a distribution of transitions different to that of the target policy.
\end{enumerate}

Divergence arises if all three are present. \marginnote{The note this is not necessarily specific to the RL-setting and hint to operations research}
If there are only two present, instability can be avoided.
\begin{enumerate}
    \item For large problems function approximation cannot be avoided
    \item Bootstrapping can be avoided at the cost of computational and data efficiency. One advantage is the direct updating after each transition, or couple of transitions. It (bootstrapping) typically results in faster learning w.r.t data efficiency 
    \item Often on-policy is adequate, as long as the state-action space is reasonably covered \begin{enumerate}
        \item data re-use, in particular if data is costly. One would like to do \dhighlight{experience replay}, i.e. re-use data from \textit{earlier} policies
        \item learning multiple RL agents, i.e. several value functions and policies 
    \end{enumerate} 
              The behavior policy likely reflects only one task of many, i.e. there is only one target policy, but it may overlap partly with other tasks.
\end{enumerate}
\markeol{18}
\beginlecture{19}{01.07.2025}

This lecture is based on \cite{Bohn.Garcke.Griebel:2024} and \cite{Bertsekas2000} (fourth edition) chapter 6.1 and 6.2. 

\section{Monte Carlo Sampling / Simulation}\marginnote{Todays focus is on value functions}

\dhighlight{Aim:} Generate trajectories, use observed states,
actions and costs. Use that to directly estimate \(V^\phi, J^\phi\).\marginnote{Policy evaluation, both the same}

\marginnote{The uniform choice implies that \(X\) is bounded in some sense I think}

\begin{algorithm}[H]
    \caption{Episodes first-visit  MC policy evaluation}\label{alg:fvmcpe}
    \begin{algorithmic}
    \State initialize return(\(x_i\)) as an empty list for all states \(x_i\)
    \While{stopping criteria not fullfilled}
        \State choose \(x(0)\in X\) uniformly random among possible start positions
        \State Sample \(\phi\) to generate a trajectory \(x(0),u(0),c(0),\dots, x(l-1),u(l-1),c(l-1)\)
        \State target \(\leftarrow 0\)
        \For{\(i\in l-1,\dots,0\)}
            \State target \(\leftarrow c(i+1)+\gamma\cdot\)target 
            \If{\(x(i)\notin \{x(0),\dots,x(i-1)\}\)}
                \State append target to return\((x(i))\)
                \State \(V(x(i))\leftarrow \) average(return\((x(i))\))
            \EndIf
        \EndFor
    \EndWhile
\end{algorithmic}
\end{algorithm}

\begin{remark}
    \(\phi\) is fixed, so sampling it means following the path (which might be random).
\end{remark}

So, we compute in each update 
\[\tilde{c}(x(i))=\sum_{k=i}^{l-1}\gamma^{k-i}c(k-1)\]
which is an estimate of \(V^\phi(x_i)\). Using only the first visit, one can see that 
we have independent identically distributed estimates, convergence of 
the average to \(V^\phi\) follows by the law of large numbers.  

Consider a \(q\times d\) matrix \(\Psi\), which we can view as some basis representation,
and some subspace \(S\) spanned by \[S=\{\Psi\cdot \theta\mid \theta\in\R^d\}\].
The projected Bellman equation 
\[\Psi\theta=\pi T^\phi(\Psi\theta)\]
where \(T^\phi\) is the Bellman operator and \(\pi\) is the projection onto \(S\) w.r.t. \(\Vert \cdot \Vert\).

This solves approximately \(J^\phi=T^\phi J^\phi\). In the terminology of \cite{Bertsekas2000} 
this us called the \dhighlight{indirect approach}.

The \dhighlight{direct approach} is finding \(\tilde{J}\in S\) via 
\[\min_{J\in S} \Vert J^\phi-\tilde{J}\Vert\]
or 
\[\min_{\theta\in\R^d}\Vert J^\phi-\Psi \theta.\]
If \(\Psi\) has independent columns, then the solution \(\theta^\star\)
is unique. 

Now consider \(\Vert \cdot\Vert_\xi\), \marginnote{not totally different to the previous \(\xi\)}
\[\xi_i\geq 0,\ i=1,\dots,q,\ \Vert J\Vert_\xi^2=\sum_{i=1}^q \xi_i(J_i)^2.\]
\[\theta^\star=\argmin_{\theta\in\R^d}\sum_{i=1}^q \xi(\Psi(i)^\intercal)\cdot \theta-J_i,\]
where \(\Psi(i)\) is the ith row of \(\Psi\). 

Setting the gradient to \(0\) gives 
\begin{align*}
    \theta^\star&=\underbrace{\left(\sum_{i=1}^q \xi_i\Psi(i)\Psi(i)^\intercal\right)^{-1}}_{\hat{A}}\underbrace{\sum_{i=1}^q \xi_i\Psi(i)J_i}_{\hat{B}}. 
\end{align*}

Now assume \(\xi\) is a probability distribution, so we can consider both terms as 
expected values and can approximate them by Monte Carlo estimates.

So, we generate a sequence of samples of indices \(i_t,\ t=1,\dots, K\)
according to \(\xi\) and obtain 
\begin{align*}
    A&=\frac{1}{K}\sum_{t=1}^K\Psi(i_t)\Psi(i_t)\approx \hat{A}\\
    B&=\frac{1}{K}\sum_{t=1}^K \Psi(i_t)J_{i_t}\approx \hat{B}
\end{align*} 

Generally, \(\hat{\phi}_k\to\theta^\star\) as \(k\) is increasing. For that 
\[\xi_i=\lim_{k\to\infty} \frac{1}{K}\sum_{t=1}^K \delta(i_t=i),\ i=1,\dots,q\]
the long term empirical frequencies should be consistent with the probabilities \(\xi_i\).

\(\xi_i\) does not need to tbe pre-determined, i.e. it can 
be implicitly defined as above, given a reasonable sampling scheme.

We can also solve \[\hat{\theta}_K=\argmin_{\theta\in\R^d}\sum_{t=1}^K (\Psi(i_t)^\intercal \theta-J(i_t))^2.\]
So, we reduce \(q\) dimensional linear algebra operations to \(d\)-dimensional ones, using Monte Carlo sampling, 
and perform lower dimensional linear algebra.

\subsection{MC estimation and the solution of linear equations systems}

Generally \[Cr=d,\]
where \(C,\ d\) might be difficult to compute directly. We aim for 
\(\hat{C},\ \hat{d}\) as simulation generated estimates.

In our context, we aim to find \(\tilde{J}(\cdot,\theta)\) as an approximation 
of \(J^\phi\).
\[\min_\theta\sum_{k=1}^q\left(J_i^\phi-\tilde{J}(i,\theta)\right)^2\]
We use a subset \(\tilde{I}\) of \textit{representative} states 

For each \(i\in \tilde{I}\) we obtain \(M(i)\) samples of \(J_i^\phi\), with \(c(i,m)\)
denoting the m-th sample.
\[c(i,m)\approx J_i^\phi+\text{ noise }+\text{ simulation error}\]
\begin{align*}
    \min_{\theta}\sum_{i\in\tilde{I}}\sum_{m=1}^{M(i)}\left(c(i,m)-\tilde{J}(i,\theta)\right)^2
\end{align*}
\marginnote{this is now actually computable and the key idea of this lecture}

If \(\tilde{J}(i,\theta)=\Psi(i)^\intercal\theta\), solve 
\[\sum_{i\in \tilde{I}}\sum_{m=1}^{m(i)}(c(i,m)-\tilde{J}(i,\theta))^2\]
If \(\tilde{I}=\{1,\dots,q\}\), then for \(M(i)\to\infty,\Psi\theta\) converges 
to the projection of \(J^\Phi\) into \(\{\Psi\theta\mid\theta\in\R^d\}\) w.r.t 
some weighted Euclidean norm. The weights of the norm are specified by the relative 
frequencies of the different states:
\[\lim_{K\to\infty} \frac{M(i)}{\sum_{i=1}^q} M(i)\]
\marginnote{here \(K=\sum_{i=1}^q M(i)\)}

\subsection{Importance Sampling}

\[\Vert \Psi\theta-J\Vert_\xi^2\]
Projection is an expected value according to \(\xi\), where 
there are multiple alternative distribution according to which 
we may represent the error above as an expected value.

It should be more effective to sample  \textit{important} terms/ states 
more often, i.e. large vs small size of \(J_i^\phi\). This is known as 
\dhighlight{important samplings}.

Generally, consider 
\[z=\sum_{w\in W}v(w),\]
where \(W\) is a finite set and \(v:W\to\R\). Consider a sampling distribution 
\(\xi\) over \(W\), and sample according to it. Write first 
\[z=\sum_{w\in W} \xi(w)\frac{v(w)}{\xi(w)}\]
and estimate it by 
\begin{equation}\label{eq:estimate_importance_sampling}
    \hat{z}_K=\frac{1}{K}\sum_{i=1}^K \frac{v(w(i))}{\xi(w(i))}.  
\end{equation}
For this to valid, we want \(\xi(w)=\lim_{K\to\infty} \frac{1}{K}\sum_{i=1}^{K}\delta(w_i=w)\ \forall w\in W\).

The expression (\ref{eq:estimate_importance_sampling}) suggests that \(\xi\) should be chosen s.t. 
the variance of the random variable \(\frac{v(w)}{\xi(w)}\) is small. In the extreme the 
variance is 0 and \(\xi(w)=v(w)/z\ \forall w\in W\) and \(v(w)>0\), then a single sample is enough. 

\markeol{19}

\beginlecture{20}{03.07.2025}

\section{Gradient Methods for direct Policy Evaluation}

\[\min_{\theta}\sum_{i\in I}\sum_{m=1}^{M(i)}\left(\tilde{J}(i,\theta)-c(i,m)\right)^2\]
Now, use gradient descent to solve this, and use the data in batches.

We have a \(N\)-transition portion \((i_0,\dots,i_N)\) of 
a sampled trajectory, a \dhighlight{batch}.
\[\sum_{t=k}^{N-1}\gamma^{t-k}c(i_t,u(i_t),i_{t+1}),\ k=0,\dots,N-1\]
are \dhighlight{cost samples} as we  had in Monte Carlo Policy evaluation (MC PE).

To approximate in \(L^2\)-sense 
\[\min_\theta\sum_{k=0}^{N-1}\left(\tilde{J}(i_k,\theta)-\sum_{t=k}^{N-1} \gamma^{t-k}c(i_t,u(i_t),i_{t+1})\right)^2\]
\(\to\) use gradient descent to update 
\[\theta_{n+1}=\theta_n-\alpha\sum_{n=0}^{N-1}\nabla \tilde{J}(i_k,\theta)\left(\tilde{J}(i_k,\theta)-\sum_{t=k}^{N-1} \gamma^{t-k}c(i_t,u(i_t),i_{t+1})\right)\]

In traditional gradient descent, this iteration is repeated until convergence. In part, this 
one \(N\)-transition is  used. Balancing
\begin{itemize}
    \item Large \(N\) \(\to\) reduce sample error, and to obtain multiple estimates per state and cover all states \marginnote{Or a representative subset} 
    \item Small \(N\) \(\to \) to keep effort per GD step small 
\end{itemize} 

In RL, batches may be changed after (some) iterations. Batches might come from different sampling 
strategies, might be part of a long trajectory, might overlap,\dots

Clearly this connects to aspect of exploration (which we have previously seen).

\begin{remark}
    Convergence analysis taking the stochastic nature into account is possible, but can be mathematically
    involved (due to the several aspects: sampling, stochastic aspects,\dots and the interactions)      
\end{remark}

\subsection{Incremental Gradient Method for direct Policy Evaluation}

Instead of updating \(\theta\)  after \(N\) transitions, processing all \(N\)
at once, we can incrementally update \(\theta\) \(N\) times. After each 
transition \((i_k,i_{k+1})\):
\begin{enumerate}
    \item Evaluate \(\nabla\tilde{J}(i_k,\theta)\)
    \item Sum all terms that involve \((i_k,i_{k+1})\) and update 
    \[\theta'=\theta-\alpha\left[\nabla \tilde{J}(i_k,\theta)\tilde{J}(i_k,\theta)-\left(\sum_{t=0}^{k}\gamma^{k-t}\nabla\tilde{J}(i_t,\theta)\right)c(i_k,u(i_k),i_{k+1}) \right]\]
\end{enumerate}

After \(N\) transitions all the terms of the batch iteration have been accumulated. Here, 
\(\theta\) is updated during the batch processing, and 
\(\nabla\tilde{J}\) is evaluated at a different (updated) \(\theta\) after each transition.

Since \(\theta\) is updated all the time, the location of the end of the batch becomes less relevant. 
As before, the \(\Vert \cdot\Vert_\xi\), will be implicitly weighted 
in proportion to the frequency of  occurrence of each state.

Connection to TD Error: \(-D_{k+1}=td_k=\tilde{J}(i_k,\theta)-\gamma\tilde{J}(i_{k+1},\theta)-c(i_k,u(i_k),i_{k+1})\)

with \(td_{N-1}=\tilde{J}(i_{N-1},\theta)-c(i_{N-1},u(i_{N-1}),i_N)\).
We can write this as: 
\begin{align*}
    td_k+\gamma td_{k+1}+\dots+\gamma^{N-1-k}td_{N-1}
\end{align*}
With that we can implement the batch iteration as 
\begin{itemize}
    \item after \((i_0,i_{1})\) set \[\theta'=\theta-\alpha td_0\nabla \tilde{J}(i_0,\tilde{\theta})\]
    \item after \(i_{1},i_{2}\) set \(\theta=\theta'\) and \[\theta'=\theta-\alpha td_{1}\left(\gamma\nabla\tilde{J}(i_0,\tilde{\theta})+\nabla\tilde{J}(i_1,\tilde{\theta})\right)\]
\end{itemize}
Repeating gives after \((i_{N-1},i_N)\) set 
\[\theta'=\theta-\alpha td_{N-1}\left(\gamma^{N-1}\nabla\tilde{J}(i_0,\tilde{\theta})+\gamma^{N-2}\nabla\tilde{J}(i_1,\tilde{\theta})+\dots+\nabla\tilde{J}(i_{N-1}\tilde{\theta})\right)\]
Here \(\tilde{\theta}=\theta\) a the beginning of the batch. In the incremental version \(\tilde{\theta}=\theta\) at transition \((i_k,i_{k+1})\) for each \(\nabla\tilde{J}(i_k,\tilde{\theta})\).

In particular, start with \(\theta_0\) and for \(k=0,\dots,N_1\) set 
\[\theta_{k+1}=\theta_k-\alpha td_k\sum_{t=0}^k\gamma^{k-t}\nabla\tilde{J}(i_t,\theta_t).\]

For linear approximation \(\tilde{J}(i,\theta)=\Psi(i)^\intercal\theta,\ i=1,\dots,q,\ \Psi(i)\in \R^5\).
\[\theta_{k+1}=\theta_k-\alpha td_k\sum_{t=0}^k \Psi(i_t).\]
\dhighlight{This is TD(1)!} \marginnote{in slightly different notation \dots}

\subsection{Multistep methods with sampling}

Fixed point view: \(J=TJ,\ J=\Pi TJ\)

We can replace \(T\) by either \(T^l,\ l>1\) or consider 
\[T^{(\lambda)}=(1-\lambda)\sum_{l=0}^\infty \lambda^l T^{l+1}\]
\marginnote{On can show \(\lambda \to 1\) connects to TD(1) as previously considered}
One can show under natural assumptions that\(T^{(\lambda)}\) and \(\Pi T^{(\lambda)}\) are contractions 
of modulus \(\gamma_\lambda=\frac{\gamma(1-\lambda)}{1-\gamma\lambda}\):
\begin{align*}
    \Vert T^{(\lambda)}y-T^{(\lambda)}z\Vert \leq \gamma_\lambda\Vert y-z \Vert.
\end{align*}

Furthermore 
\begin{align*}
    \Vert J^\phi-Psi\theta_\lambda^\star\Vert_\xi\leq \frac{1}{\sqrt{1-\gamma_\lambda^2}}\Vert J^\phi-\Pi J^phi\Vert_\xi 
\end{align*}

We can see from this, that we want \(\lambda\) close to 1, as \(\gamma_\lambda\stackrel{\lambda\to 1}{\to}1\)
and \(T^{(\lambda)}\) is a contraction for any given norm for \(\lambda\) close enough to \(1\).
Same goes for \(\Pi T^{(\lambda)}\). Further with \(\lambda\to 1\) the error bound becomes better.

\subsection{Bias-Variance Tradeoff}

\marginnote{It is not clear how good one needs to approximate the value function to update the policy? Adding a constant worsens the error, but does not hange the policies!}
We can consider \(\Psi\theta^\star-\Pi J^\phi\) as a form of bias.
But, one can observe that the sampling error becomes larger as \(\lambda\) increases.
\[T^{(\lambda)}=\sum_{l=0}^\infty \underbrace{(1-\lambda)\lambda^l}_{\text{increases with }\lambda} \underbrace{T^{l+1}}_{\text{noise variance due to the approximation of the lth power. Increases with } l}\]
Therefore one needs to balance bias-variance, and experiment with \(\lambda\).
\markeol{20}

\beginlecture{21}{08.07.2025}

\section{Policy Gradient Methods}

%TODO Tradionally very stochastic, some technicallities are ignored 

%TODO: a bit of Barto 

Fix a class of policies, parametrized by \(\theta\in \R^d\), i.e. we have \(\phi^\theta\). 

\dhighlight{Goal:} minimize \[\theta\mapsto V^{\phi^\theta}(x)=J_x(\theta).\]
We use gradient procedures, these are called policy gradient methods. We assume \(\theta\mapsto \phi^\theta\)
is differentiable in \(\theta\) for all states of \(X\).If we use \(Q\)-view, also for all actions.
\begin{equation}\label{eq:pg_update}
    \theta_{n+1}=\theta_n+\alpha\nabla J(\theta_n)  
\end{equation}

PG methods are typically stochastic. Consider a finite state action space, \(d=|X|\cdot |U|\).
Denote \(\theta=(\theta_{x,u})_{x\in X,\ u\in U}\) and define \dhighlight{softmax} policy
\begin{align*}
    \phi^\theta(u;x)&=\bP(U_t=u\mid X_t=x,\theta_t=\theta)\\
                    &=\bP(U=u\mid X=x,\theta=\theta) \\
                    &=\frac{e^{\theta_{x,u}}}{\sum_{u'\in U} e^{\theta_{x,u'}}} 
\end{align*}

%Key observation: Write J in terms of \(\theta\)

Let \(C_t^T=\sum_{k=t}^{T-1}c(x(k),u(k))\) be the cost after time \(t\).
\begin{theorem}[Policy Gradient Theorem]\label{thm:54}
    Assume that we have \(T\)-step MDP with finite 
    state action spaces and consider (stationary, in the sense of (\ref{eq:pg_update})) differentiable family 
    of policies \(\phi^\theta,\ \theta\in \R^d\). Then the gradient 
    of the value function is 
    \[\nabla_\theta J_x(\theta)=\bE_x^{\phi^\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta\log \phi^\theta(u(t),x(t))C_t^T\right] \]
\end{theorem}

\begin{proof}[Proof idea]
    Basically calculus / chain rule, re-arranging terms and using the log trick 
    \[\nabla \log = \frac{\nabla f}{f},\ \nabla f\frac{f}{f}=(\nabla\log f)f\] 
\end{proof}

Using \(\bE_x^{\phi^\theta}\left[C_t^T\mid x_t=x,u_t=u\right]=Q_t^{\phi^\theta}(x,u)\),
we can write \[\nabla_\theta J_x(\theta)=\bE_x^{\phi^\theta}\left[\sum_{t=0}^{T-1}\nabla_\theta \left(\log \phi^\theta(u(t));x(t)Q_t^{\phi^\theta}(x(t),u(t))\right)\right]\]
% Actor critic methods
\begin{definition*}
    If \(\phi^\theta\) is a policy, then the vector \(\nabla_\theta\log\phi^\theta(u;x)\)
    is  called the \dhighlight{score function} of the policy.
\end{definition*}

We sample to estimate the expectation and perform stochastic gradient descent using 
\(K\) trajectories \((x^i(0),u^i(0),x^i(1),c^i(1),u^i(1),\dots, u^i(T-1),x^i(T),c^i(T))\) sampled according to
the policy \(\phi^\theta\)
\begin{align*}
    \tilde{\nabla}_\theta J_x(\theta)=\frac{1}{K}\sum_{i=1}^K\left[\sum_{t=0}^{T-1}\nabla_\theta \left(\log \phi^\theta(u^i(t),x^i(t))\sum_{t'=t}^{T-1}c(t'+1)\right)\right]
\end{align*}


\begin{algorithm}[H]
    \caption{REINFORCE-(Batch) Stochastic Gradient Algorithm}\label{alg:reinforce}
    \underline{Given:} \(\theta_0,K\geq 1\) initial state distribution \(\mu\)
    \begin{algorithmic}
    \State \(l=0\)
    \While{stopping criteria not fullfilled}
        \For{\(i=1,\dots K\)} 
            \State sample trajectory \(i\): \((x^i(0),u^i(0),x^i(1),c^i(1),u^i(1),\dots, u^i(T-1),x^i(T),c^i(T))\)
        \EndFor
        \State choose \(\alpha\)
        \State \begin{align*}
            \tilde{\nabla}_\theta J_x(\theta)=\frac{1}{K}\sum_{i=1}^K\left[\sum_{t=0}^{T-1}\nabla_\theta \left(\log \phi^\theta(u^i(t),x^i(t))\sum_{t'=t}^{T-1}c(t'+1)\right)\right]
        \end{align*} with \(\theta=\theta_l\)
        \State \(\theta_{l+1}=\theta_k\alpha \tilde{\nabla}J(\theta_l)\)
        \State \(l=l+1\) 
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Infinite Horizon}

We measure the \dhighlight{discounted state visitations} and denote \(\bP_\mu^\phi(X_t=x')=\bP(\mu\to x'\mid t,\phi)\)
\[\rho_\mu(x')=\sum_{t=0}^{\infty}\gamma^t\bP_\mu^\phi(X_t=x')=\bE_\mu^\phi\left[\sum_{t=0}^{\infty}\gamma^t 1_{X_t=x'}\right]\]
We define a measure from it 
\[d_\mu^\phi(x)=\frac{\rho_\mu^\phi(x)}{\sum_{x'}\rho_\mu^\phi(x')}=(1-\gamma)\rho_\mu^\phi(x)\]
\[\sum_{x'}\bE(1_{X_t=x'})=\frac{1}{1-\gamma}\] %TODO: FIX
Using dynamic programming equation, repeating chain rule, one can show 
\begin{theorem}\label{thm:55}
Under the assumption that \(J_x(\theta)\) is differentiable for every state \(x\in X\) it holds 
that 
\[\nabla_\theta J_x(\theta)=\frac{1}{1-\gamma}\bE_{\substack{X\sim dx\\u\sim \phi^\theta(x)}}\left[\nabla\log\left(\phi^\theta(U;X)\right)Q^{\phi^\theta}(X,U)\right]\]    
\end{theorem}

\begin{proof}[Proof sketch]

\begin{align*}
    \nabla J_x(\theta)&=\sum_{x'\in X}\sum_{u\in U_{x'}}\overbrace{\rho_x^{\phi^\theta}}^{\frac{1}{1-\gamma}d_x^{\phi^\theta}(x')}\nabla\phi^\theta(u;x')Q^{\phi^\theta}\\
    &=\frac{1}{1-\gamma}\sum_{x'\in X}\sum_{u\in U}\nabla\log\left(\phi^\theta(u;x')\right)Q^{\phi^\theta}(x',u)\phi^\theta(u;x')d_x^{\phi^\theta}(x')\\
    &=\frac{1}{1-\gamma}\bE_{x\sim d_x^{\phi^\theta},U\sim\phi^\theta(\cdot;x)}\left[\nabla\log (\phi^\theta(u,x'))Q^{\phi^\theta}(x',u)\right]
\end{align*}

\end{proof}

Sampling from \(d^\phi\) can be achieved as follows:\marginnote{This is esentially bootstrap sampling, but worse?}
\begin{enumerate}
    \item follow rollout until an independent time according to Geom\((1-\gamma)\)
    \item estimate empirical distribution by counting the number of visits 
    \item sample from the thereby estimated occupancy measure
\end{enumerate}

\begin{theorem}\label{thm:57}
    Suppose that \((x,u)\mapsto \nabla_\theta\left(\log\phi^\theta(u;x)\right)Q^{\phi^\theta}\) is bounded.
    Then 
    \begin{align*}
        \nabla_\theta J_x(\theta)&=\bE_x^{\phi^\theta}\left[\sum_{t=0}^{\infty}\gamma^t\nabla_\theta(\log \phi^\theta(U_t,X_t))Q^{\phi^\theta}(X_t,U_t)\right]
    \end{align*}
\end{theorem}

The boundedness does not hold in general and might be hard to check. For softmax policies 
the score function can be computed and is bounded for bounded feature vectors.. If the rewards are bounded, so is \(Q\).

\begin{proof}
    \begin{align*}
        \nabla J_x(\theta) &= \sum_{t=0}^{\infty}\gamma^t\sum_{x'\in X}\bP_x^{\phi^\theta}(X_t=x')\sum_{u\in U_{x'}}\nabla\phi^\theta(u;x')Q(x',u)\\
        &=\sum_{t=0}^\infty \gamma^t\bE_x^{\phi^\theta}\left[\sum_{u\in U}\underbrace{\nabla\phi^\theta(u;X_t)}_{\phi^\theta(\dots)\log \phi^\theta}Q^{\phi^\theta}(X_t,u)\right]\\
        &=\sum_{t=0}^\infty \gamma^t\bE_x^{\phi^\theta}\left[\nabla \log \phi^\theta(U_t;X_t)Q^{\phi^\theta}(X_t,U_t)\right]\\
        &=\bE_x^{\phi^\theta}\left[\sum_{t=0}^{\infty}\gamma^t\nabla \log \phi^\theta(U_t;X_t)Q^{\phi^\theta}(X_t,U_t)\right]
    \end{align*}
\end{proof}

With that, one can use the REINFORCE algorithm \ref{alg:reinforce} up to some large \(T\),
use the truncated series as an estimate for the gradient. This estimator is biased!

\markeol{21}
