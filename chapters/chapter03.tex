\chapter{Value and \(Q\)-Function approximation}

\section{A very short crash course in machine learning}

\dhighlight{How can we represent functions?}

Goal: \[h(x)=\sum_{i=1}^d \theta_i \psi_i(x).\]
We could also use neural networks, or kernels: \marginnote{For more informatiom about kernels, you can look at my lecture notes for scientific computing 2 (also held by Garcke)}
\[h(x)=\sum_{i=1}^d\theta_ik(x,x_i)\]
\[K_{ij}=k(x_i,x_j)\]
is a positive (semi)-definite matrix for each dataset.

\begin{enumerate}
    \item We need a way to represent a function \(h\in \mathcal{H}\)\begin{itemize}
        \item linear
        \item neural networks 
        \item piecewise polynomials 
        \item kernels
    \end{itemize}
    \item loss \(\Gamma(h),\ \Gamma(h)=\Gamma(h(z_1),h(z_2),\dots,h(z_N))\) evaluated at some samples \(z_i,\ 1\leq i\leq N\)
    \item algorithm to obtain \(\argmin_{h\in \mathcal{H}}\Gamma(h)\)

        Training data \(\{(z_i,y_i)\}_{i=1}^N\), \(y_i=h^\star(z_i)+\epsilon_i\),
        \[\Gamma(h)=\frac{1}{N}\sum_{i=1}^N \left(y_i-h(z_i)\right)^2\]
\end{enumerate}
We usually use \dhighlight{regularization} to avoid \dhighlight{overfitting}.

\dhighlight{Always} reserve samples for evaluating the quality of the prediction. 

\section{Reinforcement Learning}
\[\mathcal{D}_{k+1}(Q^\theta)=-Q^\theta(x(k),u(k))+c(x(k),u(k))+\underbrace{\underbar{Q}^\theta(x(k+1))}_{\substack{=\min_u Q^\theta(x,u)\\ \text{or }Q^\theta(x(k+1),\phi(x(k+1)))}}\]
We have a sequence of state-action pairs 
\[\{\underbrace{x(k),u(l)}_{z_k}\mid 0\leq k\leq N\}\]
\begin{align*}
    \Gamma(h)=\frac{1}{N}\sum_{k=1}^{N}D_k(h(z_k),h(z_{k+1}))^2
\end{align*}
where \[D(h(z_k),h(z_k+1))\coloneqq -h(x(k-1),u(k-1))+c(x(k-1),u(k-1))+\underbar{h}(x(k))\]
with \(\underbar{h}(x)=\min_{u}h(x,u)\).

\begin{align*}
    Q^\theta(x,u)=\theta^\intercal \Psi(x,u),\ \theta\in\R^d
\end{align*}
and \(\Psi\) a collection of basis functions \(\psi_i\). Write 
\begin{align*}
  \gamma_k&=c(x(k),u(k))\\ 
  \tilde{\gamma}_{k+1}&=\Psi(x(k),u(k))-\Psi(x(k+1),\phi(x(k+1))).  
\end{align*}
Rewrite \(D_{k+1}(Q^\theta)\) as \marginnote{Since \(D_{k+1}(Q^\theta)\) will be small ...}
\begin{align*}
    \gamma_k=\tilde{\gamma}_{k+1}^\intercal\theta+\underbrace{D_{k+1}(Q^\theta)}_{\coloneqq \epsilon_k}
\end{align*}
This looks like a regression problem:
\begin{align*}
    \Gamma(\theta)=\lim_{N\to\infty}\frac{1}{N}\sum_{k=0}^{N-1}\left[\underbrace{\gamma_k-\tilde{\gamma}_{k+1}^\intercal\theta}_{=D_{k+1}(Q^\theta)}\right]^2
\end{align*}
Look for \(\theta^\star=\argmin_{\theta}\Gamma(\theta)\).

\subsection{Algorithm: Least Squares Temporal Difference Learning (LSTD)}
\marginnote{One of three streams in RL}
For a given \(d\times d\) regularization matrix \(W\), \(W\) psd, integer \(N\), and 
obtained samples \(\{(x(k),u(k))\mid 0\leq k\leq N\}\), the minimizer is obtained.
\begin{equation}\label{eq:theta_lstd}
    \theta_{N}^{\text{LSTD}}=\argmin_\theta \Gamma_N(\theta),\ \Gamma_N(\theta)=\theta^\intercal W \theta+\frac{1}{N}\sum_{k=0}^{N-1}\left[\gamma_k-\tilde{\gamma}_{k+1}^\intercal (\theta)\right]^2
\end{equation}
\begin{align*}
    Q^{\theta_N^\text{LSTD}}=\sum_{i=1}^d\theta_N^{\text{LSTD}}(i\psi(i))
\end{align*}
is the approximation of the \(Q\)-function.

We have a positive definite quadratic objective, so the solution to (\ref{eq:theta_lstd})
can be obtained by solving for \(\nabla\Gamma(\theta)\stackrel{!}{=}0\).
\begin{proposition}\label{prop:46}
    Define \(R_N=\frac{1}{N}\sum_{i=1}^N\tilde{\gamma}_k\tilde{\gamma}_k^\intercal,\ \bar{\psi}_N^\gamma=\frac{1}{N}\sum_{k=0}^{N-1}\tilde{\gamma}_{k+1}\gamma_k\).
    Then \(\theta_N^\text{LSTD}=\left[\frac{1}{N}W+R_N\right]^{-1}\bar{\Psi}_N^\gamma\)
\end{proposition}  % 1/n wrong? No 1/n infront of W initially
The regularization \(W\) is introduced to ensure a unique solution.
\markeol{13}
\beginlecture{14}{03.06.2025}
\begin{proposition}[Redundant Parametrization]\label{prop:47}
    Suppose that \(R_N=\frac{1}{N}\sum_{i=1}^N\tilde{\gamma}_k\tilde{\gamma}_k^\intercal\) has rank less 
    than \(d\). Then there is a non zero vector \(v\in\R^d\) for which the following two statements 
    hold for each \(0\leq k\leq N-1\):
    \begin{enumerate}
        \item[(i)] For any \(\theta\in\R^d\) and \(r\in \R\): \[D_{k+1}(Q^\theta)=D_{k+1}(Q^{\theta'}),\] where \(\theta'=\theta+rv.\)
\marginnote{(i) really is about the interplay of recorded respones and our representation and not about an identification problem in the statistical sense.}
        \item[(ii)] From the on-policy implementation \(u(k)=\psi(x(k))\) \[v^\intercal \Psi(x(0),u(0)) = v^\intercal\Psi(x(k),u(k)).\] 
    \end{enumerate} 
\end{proposition}
\begin{proof}
\(R_N\) does not have full rank, therefore there exists \(v\neq 0\) s.t. 
\begin{align*}
    0=v^\intercal R_Nv=\frac{1}{N}\sum_{i=1}^N(v^\intercal \tilde{\gamma}_k)^2.
\end{align*}        
Therefore, \(v^\intercal \tilde{\gamma}_k=0\) for every observed sample.
\begin{equation}\label{eq:proof_p47}
    0=v^\intercal \Psi(x(k),u(k))-v^\intercal \Psi(x(k+1),\phi(x(x+1))),\ 0\leq k\leq N-1
\end{equation}

So, \begin{align*}
    D_{k+1}(Q^{\theta'})&=-Q^{\theta'}(x(k),u(k))+c(x(k),u(k)) +Q^{\theta'}(x(k+1),\phi(x(k+1)))\\
    &=c(x(k),u(k))+\left[\theta+rv\right]\left[-\Psi(x(k),u(k))+\Psi(x(k+1),\phi(x(k)))\right]\\
    &\stackrel{\ref{eq:proof_p47}}{=}c(x(k),u(k))+\theta \left[-\Psi(x(k),u(k))+\Psi(x(k+1),\phi(x(k)))\right],
\end{align*}
which yields (i).

If \(u(k)=\phi(x(k))\), use (\ref{eq:proof_p47})
\begin{align*}
    v^\intercal \Psi(x(k),u(k))=v^\intercal\Psi(x(k+1),u(k+1))
\end{align*}
repeated use for every \(k\) gives (ii).

\end{proof}

To avoid the convergence of the \(\Gamma(\theta)\to 0\) for long trajectories,
one can do restarts.

\subsection{Algorithms: LSTD-Learning with restarts}
For a given \(d\times d\) matrix \(W>0\), integers \(N,M\), and 
observed samples \[\left\{x^i(k),u^i(k)\mid 0\leq k\leq N,1\leq i\leq M\right\}\]
with user defined initial conditions \[\{x^i(0)\mid 1\leq i\leq M\}\]
and with action \marginnote{It is fine not to probe at all} \[u^i(k)=\tilde{\phi}(x^i(k),\xi^i(k))\]
the approximation \(Q^{\theta_N^{\text{LSTD}}}=\Psi^\intercal\theta_N^{\text{LSTD}}\)
is obtained. Here 
\[\theta_N^\text{LSTD}=\argmin_\theta \Gamma_N^i(\theta),\ \Gamma_N(\theta)=\frac{1}{M}\sum_{i=1}^M\Gamma_N^i\]
and 
\[\Gamma_N^i(\theta)=\theta^\intercal W \theta +\sum_{i=1}^{N-1}\left[\gamma_k^i-\xi\tilde{\gamma}_{k+1}^{i\intercal} \theta\right]\] % TODO: Check

\begin{remark} % TODO: Check
    The LSTD algorithm can be formulated as a recursive algorithm 
    \[\theta_{N+1}=\theta_N+G_N\tilde{\gamma}_{N+1}(\gamma_N-\tilde{\gamma}_{N+1}\theta_N)\]
    where \begin{align*}
        G_{N+1}&=G_N-\frac{1}{K_{N+1}}G_N\tilde{\gamma}_{N+1}\tilde{\gamma}_{N+1}G_N\\
        K_{N+1}&=1+\tilde{\gamma}_{N+1}^\intercal G_N \tilde{\gamma}_{N+1}
    \end{align*}
\end{remark}

\subsection{Galerkin relaxation}

Basis \(\{\psi_i\},h^\theta(z)=\sum_{i=1}^d \theta_i\psi_i(z)\), we want \(0\stackrel{!}{=}\nabla_\theta \Gamma(h^\theta)\).

For Bellmann error 
\begin{align*}
    0&=\frac{1}{N}\sum_{k=1}^ND_k(h^\theta(z_k),h^\theta(z_{k+1}))\zeta^\theta(k)\\
    \zeta^\theta(k)&=\nabla_\theta D_k(h^\theta(z_k),h^\theta(z_{k+1}))
\end{align*}
Alternative is so-called Galerkin-relaxation, We construct a sequence \(\{\zeta(k)\},\ \zeta(k)\in \R^{d_\zeta}\)\marginnote{constraints}
\begin{align*}
    0&=\frac{1}{N}\sum_{k=1}^ND_{k}(h^\theta(z_k),h^\theta(z_{k+1}))\zeta_i(k)\ 1\leq i\leq d_\zeta
\end{align*}
We relax \(D_k(h^{\theta}(z_k),h^\theta(z_{k+1}))=0\ \forall k\)
\begin{center}
    \(\{\zeta(k)\}\) are called eligibility vectors in RL. 
\end{center}
\marginnote{One can introduce them in at least one other way ...}

\(\zeta(k)\) does not depend on \(\theta\), \(\zeta(k)\neq \zeta^\theta(k)\),
\textit{maybe} \(\zeta(k)\approx \zeta^\theta(k),\theta\in\) \textit{region of interest}.
It can make sense to have \(d_\zeta=d,\) if \(\theta\in \R^d\).

\section{Projected Bellman equation}

Consider \(h^\star=T(h^\star)\). \marginnote{Motivated by the solution of the Bellmann equation}

Reminder \(Q^n(x,u)=c(x,u)+Q^n(x^+,u^+)\), where \(x^+=F(x,u),\ u^+=\phi(x^+)\). In our notation \(Q^\theta(x,u)\):

\begin{align*}
    T(h)\restrict{(x,u)}=c(x,u)+h(x^+,u^+),
\end{align*}

so \(Q^\theta=T\left(Q^\theta\right)\). Consider an approximation in a function class \(\mathcal{H}\).
\begin{equation}\label{eq:bellman_projection}
    \hat{h}=\hat{T}(\hat{h})=P_{\mathcal{H}}(T(\hat{h}))
\end{equation}

with \(P_\mathcal{H}(h)\in\mathcal{H}\) for \(h\in \mathcal{H}\).

Or, consider a second function class \(\mathcal{G}\) and solve 
for \(\hat{h}\in \mathcal{H}:\)
\begin{equation}\label{eq:bellman_projection2}
    0=P_\mathcal{G}(\hat{h}-T(\hat{h}))
\end{equation}

\begin{proposition}\label{prop:48}
    Suppose that the following hold
    \begin{enumerate}
        \item[(i)] \(\mathcal{H}=G\)
        \item[(ii)] \(\mathcal{H}\) is a linear function class, i.e. \(a_1h_1+a_2h_2\in\mathcal{H}\) for \(h_1,h_2\in\mathcal{H},\ a_1,a_2\in\R\) 
        \item[(iii)] The mapping \(P_\mathcal{H}\) is linear. For  \(h_1,h_2\in \cH,\ a_1,_2\in \R\): \[P_\cH(a_1h_1+a_2h_2)=a_1P_\cH(h_1)+a_2P_\cH(h_2)\]
    \end{enumerate}
    Then the solution to (\ref{eq:bellman_projection}) and (\ref{eq:bellman_projection2}) coincide.
\end{proposition}
\markeol{14}
\beginlecture{15}{05.06.2025}

\begin{proof}
    Trivial
\end{proof}

We assume for \(g\in G\): \(g:Z\to\R\), and \(G\) is a linear function class. We further assume 
there is a state-process \(\Phi\) on \(Z\), where \((x(k),u(k),\xi(k))=w(\Phi(k))\), where \(w\)
is Lipschitz. We define for a probability measure \(\omega\) with density \(\rho\)
\[\langle h_1,h_2\rangle_\omega=\bE_\omega(h_1(\Phi),h_2(\Phi))=\int_Z h_1(z)h_2(z)\rho(z)dz\] 
\[\Vert h\Vert_\omega=\sqrt{\langle h,h\rangle_\omega}.\]

\[L_2(\omega)=\{h\mid \Vert h\Vert_\omega<\infty\}.\] %TODO: FIX

For any \(h\in L_2(\omega)\), we define projection onto \(G\) as
\[\hat{h}=P_G(h)=\argmin_{g\in G}\{\Vert g-h\Vert_\omega\}.\]
For \(\hat{h}\in G\)
\[\langle h-\hat{h},g\rangle_\omega=0,\ g\in G\]
In particular, we assume that \(G\) has finite dimension. We choose \(d\) functions 
\[\{\zeta_i\mid 1\leq i\leq d\}\] \marginnote{We do not assume that \(d\) is the dimension of \(G\) in general}
stack them to get \(\zeta:Z\to\R^d\) and define \(G\{g=\theta^\intercal\zeta\mid \theta\in\R^d\}\).
\(\zeta(k)\coloneqq \zeta(\Phi(k))\) is the sequence of \dhighlight{eligibility vectors}.

\begin{proposition}\label{prop:49}
    Suppose that \(\zeta_i\in L_2(\omega)\) for each \(i\) and that the functions are linear independent in \(L_2^\omega\).
    That is \(\Vert \zeta^\intercal\zeta\Vert_\omega=0\). For each \(h\in L_2(\omega)\), the projection exists, is unique,
    and given by \[\hat{h}=(\omega^\star)^\intercal\zeta\]
    with  \(\theta^\star=[R^\zeta]^{-1}\bar{\psi}^h,\) \(\bar{\psi}^h\in \R^d,\bar{\psi}_i^h=\langle\zeta_i,h\rangle_\omega\)
    \(\R^zeta\in\R^{d\times d},R_{ij}^\zeta=\langle \zeta_i,\zeta_j\rangle_\omega\).
\end{proposition}

\begin{proof}[Sketch]
    The orthogonality principle gives 
    \begin{align*}
        \langle h-\hat{h},\zeta_i\rangle_\omega&=0
    \end{align*}
    we use this identity with \(\hat{h}=(\theta^\star)^\intercal \zeta\)
\end{proof}

\begin{proposition}\label{prop:50}
    \(0=P_G(\hat{h}-T(\hat{h}))\) holds if and only if 
    \[0=\langle \zeta_i,\hat{h}-T(\hat{h})\rangle_\omega 1\leq i\leq d.\]
    This is the \dhighlight{Galerkin relaxation} of \(h^\star=T(h^\star)\) in the \(L_2(\omega)\) setting.\marginnote{We saw this last time as well}
\end{proposition}

Consider \(\cH=\{h=\theta^\intercal \psi\mid \theta\in\R^d\},\) where \(\psi:X\times U\to \R\) \marginnote{\(\psi\) like a \(Q\)-function}.
Now, we use the above on the Bellman operator.
\begin{align*}
    0=\bE(\zeta_i(k)(\hat{h}(x(k),u(k))-[c(x(k),u(k))+\hat{h}(x(k+1),\phi(x(k+1)))]))
\end{align*}

Solutions of this root finding problem define \(Q^{\theta^\star}\in \cH\).

Recall \(D_{k+1}\), we can write equivalently
\begin{align*}
    0=\bE(\zeta(k)D_{k+1}(Q^{\theta}))\restrict{\theta=\theta^\star}.
\end{align*}

Given \(N\) observations, we approximate this by \marginnote{More concrete Galerkin estimation}
\[0=\frac{1}{N}\sum_{k=0}^{N-1}\zeta(k)D_{k+1}(Q^\theta)\restrict{\theta=\theta^\star}.\]

\subsection{Algorithm: TD\((\lambda)\)}\marginnote{This was introduced differently, maybe we will also see this later. In all there are three views}
% FIX phi and psi
Notation: \(\psi_{(k)}=\psi(x(k),u(k))\), \(c(k)=c(x(k),u(k))\), \(\zeta_k=\zeta(k)\)

For a given \(\lambda\in [0,1]\), nonnegative step size sequence \(\{\alpha_n\}\),
initial conditions \(\theta_0,\zeta_0\) and observed samples \(\{x(k),u(k)\mid 0\leq k\leq N\}\),
the sequence of estimates is defined by three coupled equations
\begin{align*}
    \theta_{n+1}&=\theta_n+\alpha_{n+1}D_{n+1}\zeta_n\\
    D_{n+1}     &= -Q^{\theta_n}(x(n),u(n))+c_n-Q^{\theta_n}(x(n+1),\phi(x(n+1)))\\
    \zeta_{n+1} &= \lambda \zeta_{n}+\psi_{(n+1)}
\end{align*}

This defines the approximation of the \(Q\)-function \(Q^{\theta_n}=\sum_{i=1}^d(\theta_N)_i\phi_i\).

We extend the state process 
\[\Phi(k)=(x(k),u(k),\xi(k),\zeta(k)).\]
This means that \(\zeta(k)\) is a linear function of the state process \(\Phi(k)\).

Denote \(\bar{f}_\lambda(\theta)=\bE_\omega\left[\zeta(k)D_{k+1}(Q^\theta)\right]\). TD\((\lambda)\)
is an approximation of the ODE 
\begin{eqnarray}\label{eq:tdlambda_ode}
    \frac{d}{dt}\vartheta=\bar{f}_\lambda(\vartheta)
\end{eqnarray}

\(\bar{f}_\lambda(\theta)=A(\theta-\theta^\star)\), where 
\(A=\bE_\omega\left[\zeta(k)\left[-\psi_{(k)}+\psi(x(k+1),\phi(k+1))\right]^\intercal\right]\).

For linear systems QSV-assumptions can be shown if all eigenvalues of the systemmatrix have 
strictly negative real parts,  i.e. \(A\) is Hurwitz.

This can be shown for the on-policy approach, so the algorithm converges. There is a counter example in the book
if we are off-policy., so convergence of TD\((\lambda)\) is not guaranteed in the off-policy setting.

\subsection{Algorithm TD\((\lambda)\)-learning with nonlinear function approximation}

In the setup as before in TD\((\lambda)\), \(\theta_{n+1},D_{n+1}\) are 
as for the linear case.
\begin{align*}
    \zeta_{n+1}&=\lambda\zeta_{n}+\zeta_{n+1}^0\\
    \zeta_{n+1}^0&=\nabla_\theta Q^\theta(x(n),u(n))\restrict{\theta=\theta_n}  
\end{align*}

Observe that \(\zeta_n^0=\Psi_{(n)}\) for a linear function class, so this is a consistent 
generalization.
\marginnote{\(\lambda=0\) means we don't have a history at all! TD\(\lambda\) is for a fixed policy}

To extend, we use instead of the so far fixed policy \(\phi\)
\[\phi_{(n)}^\theta=\argmin_{u} Q^\theta(x,u)\]
\subsection{Algorithm: \(Q\)-learning}
The change in comparison to TD\((\lambda)\) is  
\[D_{n+1}(Q^{\theta_n})=-Q^{\theta_n}(x(n),u(n))+c(k)-Q^{\theta_n}(x(n+1),\phi^{\theta_{n+1}})\]
A limit \(\theta^\star\) will save \(\bar{f}(\theta^\star)=0\) with 
\[\bar{f}(\theta) = \bE_\omega\left[\zeta(k),D_{k+1}(Q^\theta)\right].\]

At first glance this looks as for TD\((\lambda)\), but the last term of the update to \(D_{n+1}\)
is different! For \(\lambda=0\), we can apply proposition \ref{prop:48} to conclude that 
\(Q^{\theta^\star}\) solves 
\[Q^{\theta^\star}=P_\cH(T(Q^{\theta^\star})),\]
where \(T(Q)\restrict{(x,u)}=c(x,u)+\min_{u^+}Q(x^+,u^+)\) and \(x^+=F(x,u)\).

Theory for existence of a solution or stability (in the sense of global asymptotic stability)
is so far lacking in the context of ODE analysis.

\markeol{15}
