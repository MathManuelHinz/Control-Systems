\chapter{Control Problems}

\begin{enumerate}
    \item $u$ is the  \dhighlight{control (input / action)}
    \item $y$ observations (outputs)
    \item $\phi:Y\to U$ policy
    \item $\text{ff}$ feed forward control (plan we had)
\end{enumerate}

Interactions with the outside world might be hidden in the observations. Typically $\ff$ is 
in regard to some reference state. There might be some disturbances (holes in the road, \dots).

The overall aim is to find a policy \(\phi\) that sticks close to $r(k),k\geq 0$ \marginnote{$t$ is continous, $k$ is step by step / iterative}.
\[u(k)=u_\ff(k)+U_{fb}(k)\]
where $u_\ff$ is the planing to reach the overall goal and $u_{\fb}$ actual steering, updated "all the time".

Some examples from the book:

\begin{figure}[H]\label{fig:1.01}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 1.01}
\end{figure}

\begin{figure}[H]\label{fig:1.02}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 1.02: Mountain car}
\end{figure}

Difference: In Reinforcement learning, we don't start with a model / ode. 

Some part of reinforcement learning works model-free (i.e. assumes the model only implicitly)

\begin{figure}[H]\label{fig:1.03}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 1.03: cart pole / inverted pendulum}
\end{figure}

Next example: Acrobot (more then one equilibrium)

\section{State Space Models}

We have some

\begin{itemize}
    \item \dhighlight{state space} \(X,x\in X\)
    \item \dhighlight{action space} \(U, u\in U\)
    \item \dhighlight{action} at step \(k: u(k)\in U(k)\), i.e. we might have some constraints
    \item \dhighlight{observation space \(Y,y\in Y\)}
\end{itemize}

\begin{definition}\label{def:1.1} \marginnote{\(x(k)\) might include the past, might be useful for the stock trading problem}
    Given state, action and observation spaces \(X,U,Y\), a \dhighlight{state space model}
    is defined by
    \begin{align}\label{eq:1}
        x(k+1)&=\cF(x(k),u(k))\\
        y(k)&=\mathcal{C}(x(k),u(k))  
    \end{align}
\end{definition}

\begin{remark}
    Overcomplicating problems by loading lots of information into the state space, might make the problem harder!
\end{remark}

\subsection{Linear State Space Model}

\begin{align}\label{eq:lssm}
    x(k+1)&=Fx(k)+Gu(k)\\
    y(k)&=Cx(k)+Du(k)
\end{align}

\begin{remark}
    The representations (in terms of the matrices) might not be unique!
\end{remark}

Common scenario for (\ref{eq:lssm}) is to keep \(x(k)\) near the origin. You have to think about robustness of the system. 
Disturbances should be handled by the system. 

\begin{align*}
    u(k)&=-Kx(k).
\end{align*}

Consider a disturbance under the same control:
\begin{align*}
    u(k)=-Kx(k)+v(k)
\end{align*} 
inserting this into (\ref{eq:lssm}) yields
\begin{align*}
    x(k+1) &= (F-GK)x(k)-Gv(k)\\
    y(k) &= (C-DK)x(k)+Dv(k)
\end{align*}

Closed vs open loop: In closed loops we don't change our course based on observations, while in open loop systems we do.
% TODO: Check

\subsection{State Space Models in continuous Time}

\[\frac{d}{dt}x=f(x,u)\]
for \(x\in \R^n, u\in \R^m\). We often write \(u_t,x_t\) for \(u,x\) at time \(t\).
If \(f\) is linear we get
\begin{align*}
    \frac{d}{dt}x&=Ax+Bu\\
    y&=Cx+Du
\end{align*}

\begin{figure}[H]\label{fig:1.04}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 1.04}
\end{figure}

To discretize we use the \dhighlight{forward Euler method}. Given time interval \(\Delta\)
\[x(k+1)=x(k)+\Delta f(x(k),u(k))\]
so in (\ref{eq:1}) \(\cF(x,u)=x+\Delta f(x,u)\).
Using Taylor
\[x_{t+ \Delta}=x_t \Delta f(x,u)+O(\Delta^2)\]

For the linear model we get 
\(F=I+\Delta A\)
\begin{align*}
    x(k+1) &= x(k)+\Delta Ax(k)+\underbrace{\Delta B}_{\eqqcolon G}u(k)
\end{align*}

For now fix some policy \(\phi\), so \(u(k)=\phi(x(k))\):
\[x(k+1)=\cF(x(k))\]
\begin{assumption}\label{ass:1.2}
    The state space \(X\) is equal to \(\R^n\) or a closed subset of \(\R^n\).
\end{assumption}

\begin{definition}\label{def:1.3}
    An \dhighlight{equilibrium} \(x^e\) is a state at which is system is frozen:
    \[x^e=\cF(x^e).\]
\end{definition}


\begin{definition}\label{def:1.4}\marginnote{The same concept with a different sign comes up in RL under the term \dhighlight{reward}}
    Given a \dhighlight{cost function} \(C:X\to \R_+\) and a 
    policy \(\phi\) we define 
    \[J_\phi(x)=J(x)=\sum_{k=0}^\infty C(x(k)),\ x(0)=x\] 
    This is called \dhighlight{total cost} or \dhighlight{value function} of the policy \(\phi\).
\end{definition}
Given \(x^e\), we usually assume \(C(x^e)=0\). Generally, we consider a discount factor \(\gamma^k\) in front of \(C(x(k))\).

\begin{definition}\label{def:1.5}
    Denote by \(\cX(k;x_0)\) the state step \(k\) with initial condition \(x_0\) and following fixed policy \(\phi\).
    The equilibrium \(x^e\) is \dhighlight{stable in the sense of Lyapunov} if for all 
    \(\epsilon>0\exists \delta>0\) s.t. \(\Vert x_0-x^e\Vert < \delta\), then 
    \[\Vert \cX(k;x_0)-\cX(k;x^e)\Vert<\epsilon \forall k\geq 0\]
\end{definition}

\begin{figure}[H]\label{fig:1.05}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch about Lyapunov stability}
\end{figure}

\begin{definition}\label{def:1.6}
    An equilibrium is said to be \dhighlight{asymptotically stable} if 
    \(x^e\) is stable in the sense of Lyapunov and for some \(\delta_0>0\),
    whenever \(\Vert x_0-x^e\Vert<\delta_0\), it follows
    \[\lim_{k \to\infty} \cX(k,x_0)=x^e.\]
    The set of \(x_0\) for which this holds is the \dhighlight{region of attraction} for 
    \(x^e\), An equilibrium is \dhighlight{globally asymptotically stable} if the region of attraction 
    is \(X\). 
\end{definition}
\markeol{01}

\beginlecture{02}{15.04.2025}

\begin{definition}[Lyapunov function]\label{def:1.07}

    A function \(V:X\to \R_+\) is called \dhighlight{Lyapunov function}. We frequently
    assume \(V\) is \dhighlight{inf-compact}, i.e.: it holds 
    \[\forall x^0\in X:\ \{x\in X\mid V(x)\leq V(x^0)\}\text{ is a bounded set}.\]

\end{definition}

\begin{remark}
    There is some variability in the definition of Lyapunov functions! We often assume \(V(x)\) is large if \(x\) is large.
\end{remark}

\dhighlight{Sublevel sets:} \[S_V(r)=\{x\in X \mid V(x)\leq r\}.\]
On can see with \(V\) being inf-compact \(S_V(r)\) is either
\begin{itemize}
    \item empty
    \item the whole domain \(X\) \marginnote{We usually want to avoid this}
    \item a bounded subset of \(X\). 
\end{itemize}

Usually, \(S_V(r)=X\) is impossible, a common assumption is \dhighlight{coersiveness}:
\[\lim_{\Vert x\Vert \to\infty} V(x)=\infty.\]

\begin{example}
    \begin{itemize}
        \item \(V(x)=x^2\), coercive
        \item \(V(x)=\frac{x^2}{(1+x)^2}\), not coercive, but inf-compact \(r>1: S_V(r)=\R,\ r<1: S_V(r)=[-a,a]\) with \(a=\sqrt{\frac{r}{1+r}}\)
        \item \(V(x)=e^x\) is neither
    \end{itemize}
\end{example}

\begin{lemma}\label{lem:1.08}
    Suppose that the cost function \(C\) and the value function \(J\) from definition \ref{def:1.5} are non-negative and 
    finite valued\marginnote{this is a assumption on the value function}.
    \begin{enumerate}
        \item \(J(x(k))\) is non-increasing in \(k\) and \(\lim_{k\to\infty} J(x(k))=0\) for each initial condition.
        \item In addition let \(J\) be continuous, inf-compact and vanishing only at \(x^e\). Then for each initial condition 
            \[\lim_{k\to\infty} x(k)=x^e\]
    \end{enumerate} 
\end{lemma}

\begin{proof}
    Consider \(J(x)=\sum_{k=0}^\infty c(x(k))\), then 
    \begin{align*}
        J(x) &= c(x)+\sum_{k=1}^\infty c(x(k))\\
        &=c(x)+\sum_{k=0}^\infty c(x^+(k)); \ x^+(0)=\cF(x)\\
        &=c(x)+ J(\mathcal{F}(x))
    \end{align*}
    This is the \dhighlight{dynamic programming principle} for a \dhighlight{fixed policy}.\marginnote{We are separating one step!}
    It is also called \dhighlight{Bellmann equation}.\marginnote{\\ This is the same Bellman from the curse of dimensionality!}
    For 1. from this it follows \begin{align*}
        J(x(k+1))+c(x)-J(x(k)) &= 0
    \end{align*}
    summing up from \(k=0\) up to \(N-1\)
    \begin{align*}
        J(x)=J(x(N))+\sum_{k=0}^{N-1}c(x(k))\\ \implies \text{ non-increasing}
    \end{align*}
    Taking the limit
    \begin{align*}
        =\lim_{N\to\infty}\left[J(x(N)+\sum_{k=0}^{N-1}c(x(k)))\right]=\left[\lim_{N\to\infty} J(x(N))\right]+J(x)
    \end{align*}
    using \(J(x)\) is finite gives (i).


    For 2. with \(r=J(x)\), we get \(x(k)\in S_J(r)\forall k\). Now suppose \(\{x(k_i)\}\) is a convergent subsequence
    of the trajectory with limit \(x^\infty\). Then \(J(x^\infty)=\lim_{i\to\infty}J(x(k_i))=0\) by the continuity of \(J\).
    We assumed \(J(x)=0 \iff x^e=x\implies x^\infty=x^e.\) Finally, the assumption follows, since each convergent subsequence reach the same value \(x^e\).

\end{proof}

\begin{definition}[Poisson's inequality]\label{def:1.9}\marginnote{We often assume \(\eta=0\)}
    Let \(V,c:X\to\R_+\) and \(\eta\geq 0\). Then \dhighlight{Poisson's inequality} states that 
    \[V(\mathcal{F}(x))\leq V(x)-c(x)+\eta.\]
\end{definition}


\begin{proposition}\label{prop:1.10}
    Suppose the Poisson inequality holds with \(\eta=0\). Additionally
    \(V\) shall be continuous, inf-compact and it shall have a unique minima at $x^e$. Then \(x^e\)
    is stable in the sense of Lyapunov (\dhighlight{sitsoL}).    
\end{proposition}

\begin{proof}
    \[\bigcap \{S_V(r)\mid r>V(x^e)\}=\{S_V(r)\restrict{r=V(x^e)}\}\stackrel{\text{unique minimizer}}{=}\{x^e\}.\]
    Using compactness we get: For each \(\epsilon>0\), we can find some \(r>V(x^e)\)
    and some \(\delta<\epsilon\) s.t. 
    \[\{x\in X\mid \Vert x-x^e\Vert < \delta\}\subset S_V(r)\subset \{x\in X\mid \Vert x-x^e\Vert < \epsilon\}\]
    If \(\Vert x_0-x^e\Vert<\delta\), then \(x_0\in S_V(r)\) and hence \(x(k)\in S_V(r)\)
    since \(V(x(k))\) is non-increasing. With the second inclusion we see 
    \[\Vert x(k)-x^e\Vert<\epsilon\forall k\]
    This gives sitsoL.
\end{proof}

\begin{proposition}[Comparison theorem]\label{prop:1.11}\marginnote{We don't write that explictlly, but we don't start in \(x^e\)!}
    Poisson's inequality implies 
    \begin{enumerate}
        \item For each \(N\geq 1\) and \(x=x(0)\) \[V(x(N))+\sum_{k=0}^{N-1}c(x(k))\leq V(x)+ N\eta\]
        \item If \(\eta=0\), then \(J(x)\leq V(x)\forall x\)
        \item Assume \(\eta=0\) and \(V,c\) are continuous. Suppose that \(c\) is inf-compact and vanishes only at the equilibrium \(x^e\). Then 
              \(x^e\) is globally asymptotically stable.  
    \end{enumerate}
\end{proposition}

\begin{proof}
    1. \begin{align*}
        V(x(k+1))-V(x(k))+c(x(k))\leq \eta 
    \end{align*}
    summing up from \(0\) to \(N-1\):
    \begin{align*}
        V(x(N))-V(x(0))+\sum_{k=0}^{N-1}c(x(k))\leq N\eta
    \end{align*}
    2. for \(\eta=0\) the above is \(\leq 0\), so \(\sum_{k=0}^{N-1}c(x(k))\leq V(x(0))-V(x(k))\leq V(x(0))\)
    where the LHS converges to \(J(x(0))\) for \(N\to\infty\) 

    3. Show sitsoL,  with \(\eta=0\) it follows form definition \ref{def:1.9} that \(V(x)\geq c(x)\), which gives \(V\) is 
    also inf-compact. \marginnote{This is important!}
    \(c\) is vanishing only at \(x^e\), so \(V(x(k))\) is strictly decreasing. When \(x(k)\neq x^e\),
    implies \(V(x(k))\downarrow V(x^e)\) for each \(x(0)\). Further
    \[V(x^e)<V(x(0))\ \forall x(0)\in X\setminus\{x^e\}.\] 
    So it is a unique minimum. \(V\) has therefore the properties of proposition \ref{prop:1.10}, which gives sitsoL. 

    For global: with 1. we get \[\lim_{k\to\infty}c(x(k))=0\]
    and assumptions give us by lemma \ref{lem:1.08} that \(x(k)\to x^e\) as \(k\to\infty\).
    So, we converge from any initial condition, which gives global asymptotical stability.

\end{proof}


\begin{proposition}\label{prop:1.12}
    Suppose that \(V(\mathcal{F}(x))=V(x)-c(x)\). Further, we assume that 
    \begin{enumerate}
        \item \(J\) is continuous, inf-compact, vanishing only at \(x^e\)
        \item \(V\) is continuous
    \end{enumerate}
    Then \(J(x)=V(x)-V(x^e)\).
\end{proposition}

\begin{proof}
    As before we sum up:
    \begin{align*}
        V(x(N))+\overbrace{\sum_{k=0}^{N-1}c(x(k))}^{J(x(N-1))\stackrel{N\to\infty}{\to}J(x)}=V(x).
    \end{align*}
    Lemma \ref{lem:1.08} together with the continuity of \(V\) implies that
    \[V(x(N))\to V(x^e)\ \text{ as }N\to\infty.\]
    This gives \[V(x^e)+J(x)=V(x)\qedhere\]
\end{proof}
\markeol{02} % Oben solte alles \cF sein?
\beginlecture{03}{17.04.2025}

\begin{example}[Linear state space model]
    Setting \(x(k+1)=\cF(x(k)),\) now with linear dynamics:
    \[x(k+1)=F x(k)=F^{k+1}x(0)=F^{k+1}x.\]
    Assume quadratic cost \(c(x)=x^\intercal S x\), where \(S\) is symmetric and 
    positive definite.
    Observe
    \[c(x(k))=(F^kx)^\intercal S F^k x\]
    Summing up yields 
    \begin{align*}
        J(x)&=x^\intercal \underbrace{\left[\sum_{k=0}^{\infty} (F^k)^\intercal S F^k\right]}_{\eqqcolon M}x
    \end{align*}
    This satisfies a linear fixed point equation:\marginnote{This is also called \dhighlight{discrete time} \dhighlight{Lyapunov equation}}
    \begin{equation}\label{eq:dle}
        M=S+F^\intercal M F
    \end{equation}
    One can show for the linear state space model, that the following are equivalent:
    \begin{enumerate}
        \item the origin is asymptotically stable 
        \item the origin is globally asymptotically stable 
        \item Each eigenvalue \(\lambda\) of \(F\) satisfies \(|\lambda|<1\)
        \item (\ref{eq:dle}) admits a solution \(M\) positive semi-definite for any \(S\) positive semidefinite.
    \end{enumerate}

    Reference: \cite{basar2024lecturenotescontroltheory} % TODO

\end{example}

Consider \ref{def:1.1} without \(y\)
\[y(k+1)=\cF(x(k),u(k))\]
with \[c:X\times U\to \R_+.\]
The total cost \(J_\phi\) for a given \(\phi\) given 
\(u(k)=\phi(x(k))\) is 
\[J_\phi(x)=\sum_{k=0}^\infty c(x(k),u(k)).\]
The optimal value function is the minimum over all controls \marginnote{This describes the optimal control policy (OCP)}
\begin{equation}\label{eq:OCP}
    J^\star(x)=\min_{\underbar{U}=[u(0),u(1),\dots]} \sum_{k=0}^\infty c(x(k),u(k)),\ x(0)=x\in X
\end{equation}

\begin{remark}
    The minimizer might not be unique! In harder settings this might need to be an inf!
\end{remark}

\dhighlight{Goal:} Find a control sequence that achieves the minimum. \marginnote{and the corrensponding policy}

Computationally we can't expect to calculate \(J_\phi\) exactly, but we will approximate it. 
\begin{remark}
    We are in the infinite horizon setting (infinite time steps) to talk about the stability. For this it is important 
    that the equilibrium has cost \(0\). Without an equilibrium we can also think about discounted value functions \[J_\phi=\sum_{k=0}^\infty \gamma^k c(x(k),u(k))\]
\end{remark}

We will see later that it holds for the sequence \(x^\star\) achieving the minimum 
\[J^\star(x^\star(k))=c(x^\star(k),u^\star(k))+J(x^\star(k+1))\]
which is definition \ref{def:1.9} with \(\eta=0\) and equality.

Proposition \ref{prop:1.11} implies, under some conditions, that \(x^e\)
is globally asymptotically stable.

%To achieve finiteness
Under the following assumptions \(J^\star\) is finite:
\begin{enumerate} % Fix equilibria
    \item there is a (target) state \(x^e\) that is an equilibria for some control \(F(x^e,u^e)=x^e\)
    \item \(c\geq 0,c(x^e,u^e)=0\)
    \item for any initial condition \(x(0)=x\) there is a control sequence \(\underbar{u}\) and a time \(T\), such that \(x(T)=x^e\) for \(x(0)=x\) using control \(\underbar{u}\).\marginnote{This is sometimes called \dhighlight{controllability}}
\end{enumerate}

\begin{example}[Linear Quadratic Regulator]
    Consider linear dynamics \ref{eq:lssm} %TODO: check
     from the first lecture with 
    quadratic cost \(c(x,u)=x^\intercal Sx+u^\intercal Ru\) with \(S\) positive semi-definite and 
    \(R\) positive definite.
    Reminder: \(u=-Kx\).

    If there is a policy for which \(J^\star\) is finite, then 
    \begin{align*}
        J^\star(x)=x^\intercal M^\star x
    \end{align*}
    with \(M^\star\) positive semi-definite and 
    \[\phi^\star(x)=-K^\star(x)\]
    with \(K^\star\) depends on \(M^\star,R,F,G\). \marginnote{and implicitly on \(c\)}
\end{example}

\subsection*{Bellmann equation}
\begin{figure}[H]\label{fig:1.06}
    \centering
    \includegraphics[width=.7\textwidth]{example-image}
    \caption{Sketch 1.06; Principle of optimality}
\end{figure}

\dhighlight{Observation:}

\begin{align*}
    J^\star(x)&=\min_{\underbar{u}}\left[\sum_{k=0}^{k_m-1}c(x(k),u(k))+\sum_{k_m}^\infty c(x(k),u(k))\right]\\
    &=\min_{u[0,\dots,k_m-1]}\left[\sum_{k=0}^{k_m-1}c(x(k),u(k))+\underbrace{\min_{u[k_m,\dots,]}\sum_{k_m}^\infty c(x(k),u(k))}_{=J^*(x(k_m))}\right]
\end{align*}

This gives \marginnote{which can be seen as a kind of fix point equation}
\[J^*(x)=\min_{u[0,\dots,k_m-1]}\left[\sum_{k=0}^{k_m-1}c(x(k),u(k))\right]+J^\star(x(k_m)).\]

With \(k_m=1\) we have shown the following theorem 
\begin{theorem}[\dhighlight{Bellmann equation}, \dhighlight{Dynamic Programming} equation]\label{thm:1.13}
    Assume that \(J^\star\) is finite and optimal control \(u^\star\) solving (\ref{eq:OCP}) exists.
    Then the value function satisfies 
    \begin{equation}\label{eq:bellmann}
        J^\star(x)=\min_{u}\{c(x,u)+J^\star(\cF(x,u))\}
    \end{equation}
\end{theorem}

Suppose the minimum is unique for each \(x\) and let \(\phi^\star(x)\) denote 
the minimum feedback law at \(x\). Then the optimal control is expressed as 
\[u^\star(k)=\phi^\star(x^\star(k)).\]

\begin{definition}[Q-function]\label{def:1.14}\marginnote{Definition, which is not so useful for the analysis, but for the pratical application!}
    The function of two variables within the minimum in (\ref{eq:bellmann}) is called \dhighlight{Q}-function.
    \[Q^\star(x,u)=c(x,u)+J^\star(\cF(x,u))\]
    In the optimal case we write \(Q^\star\). Thus 
    \[J^\star(x)=\min_{u}Q^\star(x,u).\] 
    The optimal feedback law is then 
    \[\phi^\star(x)\in\argmin_{u}Q^\star(x,u).\]    
\end{definition} % FIX argmin

The \(Q\)-function solves the fixed point equation 
\[Q^\star(x,u)=c(x,u)+\min_{u}Q^\star(\cF(x,u),u).\]
This already gives a hint for an algorithm coming later next lecture.

\begin{remark}
    In RL the difference is that we don't know the model, we only observe state action pairs.
    This motivates the \(Q\)-function.        
\end{remark}

\subsection*{Some concepts from Reinforcement Learning}

\dhighlight{Actors and critic:}

Given is a parameterized family of policies \(\{\phi^\theta\mid \theta\in \R^d\}\).
the \dhighlight{actors}. For each \(\theta\), observe the trajectories by their states 
\(x\) and actions \(u\) determined by their policy.

The \dhighlight{critic} approximates the associated value function 
\(\tilde{J}_\theta\). Aim for the minimum 
\[\theta^\star=\argmin_{\theta}\langle v, \tilde{J}_\theta \rangle,\]\marginnote{scalar product in \(\R^n\) (all states?)}
where the weight vector \(v\geq 0\) reflects the weighting of the states. \(v(x)\) is large for \textit{important} states.

\dhighlight{Temporal differences:}

\[J_\theta(x(k))=c(x(k),u(k\mid\theta))+J_\theta(x(k+1))\]
Look for an approximation \(\hat{J}\) for which the error is small (w.r.t. the equality above).

\dhighlight{Temporal differences} are \marginnote{What changes, or what is the information gain}
\[D_{k+1}(\hat{J})\coloneqq -\hat{J}(x(k))+\hat{J}(x(k+1))+c(x(k),u(k)).\]

After \(N\) samples 
\[\Gamma(\hat{J})\coloneqq \frac{1}{N}\sum_{k=0}^{N-1} D_{k+1}(\hat{J})^2.\]
We can optimize / minimize this.

There is a whole class of TD algorithms and those fit into the actors critic approach!

\markeol{03}
\beginlecture{04}{22.04.2025}

\subsection{Value iteration}

We approximate \(J^\star\) by a sequence of \(V^k\) given an initial value function \(V^0\).\marginnote{For infinite state spaces we will have to fix this algorithm for memory related reasons}
\[V^{k+1}(x)=\min_{u}\{c(x,u)+V^k(\cF(x,u))\}, x\in X,\ k\geq 0\]
This is called \dhighlight{value iteration} often shortened to VI.

\begin{proposition}\label{prop:1.15}
    Let \(V^0\) be chosen with non-negative entries and \(V^0(x^e)=0\). Further,
    we assume
    \begin{enumerate}
        \item \(X,U\) are finite sets 
        \item \(c\) is non-negative and vanishes only at \((x^e,u^e)\), and \(J^\star\) is finite valued.
    \end{enumerate}
    Then there is \(n_0\geq 1\) such that 
    \[V^k(x)=J^\star(x), \ x\in X,k\geq n_0.\]
\end{proposition}

\begin{proof}\marginnote{We really exploit the finiteness!}
    Let \(\phi^\star(x)\) be an optimal policy, and let 
    \(n_0\geq 1\) denote the value such that 
    \[(x^\star(k),u^\star(k))=(x^e,u^e)\]
    for \(k\geq n_0\). This exists since \(J^\star\) is finite.

    Using the principle of optimality (\ref{eq:OCP}) % TODO: fix
     we can show
    \begin{align}\label{eq:povi}
        V^n(x)=\min_{u[0,\dots,n-1]} \left\{\sum_{k=0}^{n-1} c(x,u)+V^0(x(n)) \right\},\ x(0)\in X 
    \end{align}
    This gives 
    \begin{align*}
        V^n(x) & \leq \sum_{k=0}^{n-1}c(x(k),u(k))+V^0(x(n))\ \text{ for all } u \text{ including }u(k)=\phi^\star(k) \\
               &\stackrel{n\geq n_0}{=}J^\star(x)+V^0(x(0))=J^\star(x)
    \end{align*}
    For such \(n\), the inequality must be an equality, due to (\ref{eq:povi}) and the use 
    of the optimal policy.
\end{proof}

VI provides a sequence of policies \(\phi^n\)
\[\phi^n(x)\in\argmin_{u}\{c(x,u)+V^n(\cF(x,u))\}.\]
If we assume that \(V^0\) is non-negative and satisfies for some 
\(\eta\geq 0\) poisson's inequality(\ref{def:1.9})
\[V^0(\cF(x,u))\leq V^0(x)-c(x,\varphi^0(x))+\eta,\ x\in X\]
then we get the following statement 
\begin{proposition}\label{prop:1.16}
    Suppose that \(V^0\) is non-negative and it holds 
    \begin{align*}
        \min_u (c(x,u)+V^0(\cF(x,u)))&=\{c(x,u)+V^0(\cF(x,u))\}\mid_{u=\phi^0(x)}\\
        &\leq V^0(x)+\eta,\ x\in X
    \end{align*}
    Then a corresponding bound holds for each \(n\)
    \begin{align*}
        \{c(x,u)+V^n(\cF(x,u))\}\mid_{u=\phi^0(x)} & \leq V^n(x)+\eta_n,\ x\in X,
    \end{align*}
    where \(\eta_i\) is non-increasing:
    \[\eta\geq \eta_0\geq \eta_1\dots\]
\end{proposition}

\begin{proof}
    Write \(B^n(x)=V^{n+1}(x)-V^n(x)\)\marginnote{This is (connected to?) the Bellman error}
    \[\eta_n\coloneqq \sup_x B^n(x).\]
    Value iteration gives 
    \begin{align*}
        \{c(x,u)+V^n(\cF(x,u))\}\mid_{u=\phi^n(x)}&=\min_u\{c(x,u)+V^n(\cF(x,u))\}\\
        &=V^{n+1}(x)=V^n(x)+B^n(x)\\
        &\leq V^n(x)+\eta_n 
    \end{align*}
    For non-increasing,we consider 
    \begin{align*}
        V^1(x)=\{c(x,u)+V^0(\cF(x,u))\}\restrict{u=\phi^0(x)}\stackrel{\text{Assumption}}{\leq} V^0(x)+\eta
    \end{align*}
    which gives \(B^0(x)\leq \eta\forall x\implies\eta_0\leq \eta\).

    For \(n\geq 1\) 
    The trick is using the old control in the second line:
    \begin{align*}
        V^n(x)&=\{c(x,u)+V^{n-1}\cF((x,u))\}\restrict{u=\phi^{n-1}(x)}\\
        V^{n+1}(x) &\leq \{c(x,u)+V^n(\cF(x,u))\}\restrict{u=\phi^{n-1}(x)}
    \end{align*}
    So, 
    \[V^{n+1}(x)-V^{n}(x)\leq \{V^n(\cF(x,u))-V^{n-1}(\cF(x,u))\}\restrict{u=\phi^{n-1}(x)}\leq \eta_{n-1}.\]
    Hence, \(\eta_n=\sup_x B^n(x)\leq \eta_{n-1}\).
\end{proof}

%TODO: Part of te proof?
Now consider \(\eta=0\), so for each \(n\)
\begin{align*}
    \{c(x,u)+V^n(\cF(x,u))\}\restrict{u=\phi^n(x)}\leq V^n(x)
\end{align*}
with proposition \ref{prop:1.11} it follows 
\begin{align*}
    J^\star &\leq V^n(x),\ x\in X,   
\end{align*}
where \(J^\star\) is the total cost using policy \(\phi^n\).

One view of policy iteration is the focus on updating the policy function!

\subsection{Policy iteration}

Start with an initial policy \(\phi^0,n=0\)
\begin{itemize}
    \item Compute the total cost for the policy \(\phi^n\), this is called policy evaluation
    \[J^n(x)=\sum_{k=0}^\infty c(x(k),u(k)),\ u(k)=\phi^n(x(k))\forall x\in X\]
    \item perform \dhighlight{policy improvement} to obtain the next policy 
    \[\phi^{n-1}(x)\in \argmin_{u}\{c(x,u)+J^n(\cF(x,u))\},\ x\in X\]
    \item while \textit{not good enough}
\end{itemize}

This is sometimes also called Howard's algorithm.

\begin{remark}
    The first step is some linearization and the second is the update. Like a generalization of Newton's method    
\end{remark}

% TODO
\begin{algorithm}[H]
    \caption{This will be fixed soon}
    \textbf{Input:} $A\in\R^{m\times n},m\geq n$\\
    \textbf{Output:} $R$ von der $QR$-Zerlegung ($A$ wird zerstÃ¶rt ``in place'')
    \begin{algorithmic}
    \For{$j=1,\dots, n$}
        \For{$i=m,m-1,\dots, j+1$}
            \State Berechne $c,s$
            \State $A[i-1:i,j:n]=\begin{bmatrix}
                c & s \\
                -s & c 
            \end{bmatrix}^t A[i-1:i,j:n]$
        \EndFor
    \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{proposition}\label{prop:1.17}
    Suppose that \(J^0\) for \(\phi^0\) is finite valued. Then 
    for each \(n\geq 0\)
    \[\{c(x,u)+J^n(\cF(x,u))\}\restrict{u=\phi^{n+1}(x)}\leq J^n(x),\ x\in X\]
    and consequently, the value functions are non-increasing
    \[J^0(x)\geq J^1(x)\geq \dots\]
\end{proposition}

\begin{proof}
    Similar to the proof of proposition \ref{prop:1.16}, where the non-increasing sequence again follows  
    from proposition \ref{prop:1.11}. 
\end{proof}

Here we always assumed that we can compute everything, especially \(\cF\) and the infinite sum.

\subsection{Exploration}
In RL we learn from observations, each state-action pair,
new state and observed cost gives us information. We need 
\textit{good} and \textit{useful} information. 

Consider a policy that is not optimal, but has 
\(x(k)\to x^e\) reasonably rapidly, where we assume \(c(x^e,\cdot)=0\).
Typically we have continuity 
\begin{align*}
    \lim_{k\to \infty} D_{k+1}(\hat{J}) & = \lim_{k\to\infty} \left[
        -\hat{J}(x(k))+\hat{J}(x(k+1))+c(x(k),u(k))
    \right] \\
    &=-\hat{J}(x^e)+\hat{J}(x^e)+0=0. 
\end{align*}

This is not much information, one cannot further improve the policy! 
\begin{align*}
    \Gamma^\epsilon(\hat{J},x^i)=\frac{1}{N_\epsilon}\sum_{k=0}^{N_\epsilon-1}[D_{k+1}(\hat{J})]^2, \ x(0)=x^i
\end{align*}
To avoid getting \textit{small} information from long trajectories, one 
can take a couple of shorter ones.

\begin{align*}
    \hat{\Gamma}(\hat{J})=\frac{1}{M}\sum_{i=1}^M\Gamma^\epsilon(J;x^i)
\end{align*}

How to choose \(x^i\) is current research. Much of the theoretical research 
assume that ``every state is assumed regularly'', which is nice for results, but not so nice realistic 
in most applications. 
Another way to get more diverse information is to use \dhighlight{exploration}. Namely
one modifies the trajectories, not strictly follows \(\phi^n\). 

\(u(k)=\hat{\phi}(x(k),\zeta(k))\), where \(\zeta(k)\) is some form of noise.
Typically 
\begin{enumerate}
    \item \(\hat{\phi}(x(k),\zeta(k))=\phi^\theta(k)\) for \textit{most} \(k\)
    \item Choose action to explore the state-action space (e.g. randomly) the other times
\end{enumerate}

Generally, the trajectory to gather information \marginnote{this is also sometimes called off-polciy and on-policy}
stems from a different policy than the current estimate \(\phi^\theta\). 

This dilemma is called the \dhighlight{exploration-exploitation} dilemma.

\markeol{04}